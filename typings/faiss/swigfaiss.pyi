"""
This type stub file was generated by pyright.
"""

if __package__ or "." in __name__:
    ...
else:
    ...
class _SwigNonDynamicMeta(type):
    """Meta class to enforce nondynamic attributes (no new attributes) for a class"""
    __setattr__ = ...


class SwigPyIterator:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    __swig_destroy__ = ...
    def value(self):
        ...
    
    def incr(self, n=...):
        ...
    
    def decr(self, n=...):
        ...
    
    def distance(self, x):
        ...
    
    def equal(self, x):
        ...
    
    def copy(self):
        ...
    
    def next(self):
        ...
    
    def __next__(self):
        ...
    
    def previous(self):
        ...
    
    def advance(self, n):
        ...
    
    def __eq__(self, x) -> bool:
        ...
    
    def __ne__(self, x) -> bool:
        ...
    
    def __iadd__(self, n):
        ...
    
    def __isub__(self, n):
        ...
    
    def __add__(self, n):
        ...
    
    def __sub__(self, *args):
        ...
    
    def __iter__(self): # -> Self:
        ...
    


SHARED_PTR_DISOWN = ...
class Float32Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Float64Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int8Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int16Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int32Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int64Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class UInt8Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class UInt16Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class UInt32Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class UInt64Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Float32VectorVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class UInt8VectorVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int32VectorVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class Int64VectorVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class VectorTransformVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class OperatingPointVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class InvertedListsPtrVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class RepeatVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class ClusteringIterationStatsVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class ParameterRangeVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorUInt8Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorInt32Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorFloat32Vector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


class OnDiskOneListVector:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    def push_back(self, arg2):
        ...
    
    def clear(self):
        ...
    
    def data(self):
        ...
    
    def size(self):
        ...
    
    def at(self, n):
        ...
    
    def resize(self, n):
        ...
    
    def swap(self, other):
        ...
    
    __swig_destroy__ = ...


def simd_histogram_8(data, n, min, shift, hist):
    r"""
     low level SIMD histogramming functions  8-bin histogram of (x - min) >> shift
    values outside the range are ignored.
    the data table should be aligned on 32 bytes
    """
    ...

def simd_histogram_16(data, n, min, shift, hist):
    r"""same for 16-bin histogram"""
    ...

class PartitionStats:
    thisown = ...
    __repr__ = ...
    bissect_cycles = ...
    compress_cycles = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


def bitvec_print(b, d):
    ...

def fvecs2bitvecs(x, b, d, n):
    ...

def bitvecs2fvecs(b, x, d, n):
    ...

def fvec2bitvec(x, b, d):
    ...

def bitvec_shuffle(n, da, db, order, a, b):
    r"""Shuffle the bits from b(i, j) := a(i, order[j])"""
    ...

class BitstringWriter:
    thisown = ...
    __repr__ = ...
    code = ...
    code_size = ...
    i = ...
    def __init__(self, code, code_size) -> None:
        ...
    
    def write(self, x, nbit):
        ...
    
    __swig_destroy__ = ...


cvar = ...
class BitstringReader:
    thisown = ...
    __repr__ = ...
    code = ...
    code_size = ...
    i = ...
    def __init__(self, code, code_size) -> None:
        ...
    
    def read(self, nbit):
        ...
    
    __swig_destroy__ = ...


def hammings(a, b, na, nb, nbytespercode, dis):
    r"""
     Compute a set of Hamming distances between na and nb binary vectors

    :type a: uint8_t
    :param a:             size na * nbytespercode
    :type b: uint8_t
    :param b:             size nb * nbytespercode
    :type nbytespercode: int
    :param nbytespercode: should be multiple of 8
    :type dis: int
    :param dis:           output distances, size na * nb
    """
    ...

def hammings_knn_hc(*args):
    r"""
     Return the k smallest Hamming distances for a set of binary query vectors,
    using a max heap.
    :type a: uint8_t
    :param a:       queries, size ha->nh * ncodes
    :type b: uint8_t
    :param b:       database, size nb * ncodes
    :type nb: int
    :param nb:      number of database vectors
    :type ncodes: int
    :param ncodes:  size of the binary codes (bytes)
    :type ordered: int
    :param ordered: if != 0: order the results by decreasing distance
                       (may be bottleneck for k/n > 0.01)
    :type approx_topk_mode: int, optional
    :param approx_topk_mode: allows to use approximate top-k facilities
                                to speedup heap
    """
    ...

def hammings_knn(ha, a, b, nb, ncodes, ordered):
    ...

def hammings_knn_mc(a, b, na, nb, k, ncodes, distances, labels, sel=...):
    r"""
     Return the k smallest Hamming distances for a set of binary query vectors,
    using counting max.
    :type a: uint8_t
    :param a:       queries, size na * ncodes
    :type b: uint8_t
    :param b:       database, size nb * ncodes
    :type na: int
    :param na:      number of query vectors
    :type nb: int
    :param nb:      number of database vectors
    :type k: int
    :param k:       number of vectors/distances to return
    :type ncodes: int
    :param ncodes:  size of the binary codes (bytes)
    :type distances: int
    :param distances: output distances from each query vector to its k nearest
                       neighbors
    :type labels: int
    :param labels:  output ids of the k nearest neighbors to each query vector
    """
    ...

def hamming_range_search(a, b, na, nb, radius, ncodes, result, sel=...):
    r"""same as hammings_knn except we are doing a range search with radius"""
    ...

def hamming_count_thres(bs1, bs2, n1, n2, ht, ncodes, nptr):
    ...

def match_hamming_thres(bs1, bs2, n1, n2, ht, ncodes, idx, dis):
    ...

def crosshamming_count_thres(dbs, n, ht, ncodes, nptr):
    ...

def generalized_hammings_knn_hc(ha, a, b, nb, code_size, ordered=...):
    r"""
    generalized Hamming distances (= count number of code bytes that
       are the same)
    """
    ...

def pack_bitstrings(*args):
    r"""
    *Overload 1:*
     Pack a set of n codes of size M * nbit

    :type n: int
    :param n:           number of codes to pack
    :type M: int
    :param M:           number of elementary codes per code
    :type nbit: int
    :param nbit:        number of bits per elementary code
    :type unpacked: int
    :param unpacked:    input unpacked codes, size (n, M)
    :type packed: uint8_t
    :param packed:      output packed codes, size (n, code_size)
    :type code_size: int
    :param code_size:   should be >= ceil(M * nbit / 8)

    |

    *Overload 2:*
     Pack a set of n codes of variable sizes

    :param nbit:       number of bits per entry (size M)
    """
    ...

def unpack_bitstrings(*args):
    r"""
    *Overload 1:*
     Unpack a set of n codes of size M * nbit

    :type n: int
    :param n:           number of codes to pack
    :type M: int
    :param M:           number of elementary codes per code
    :type nbit: int
    :param nbit:        number of bits per elementary code
    :type unpacked: int
    :param unpacked:    input unpacked codes, size (n, M)
    :type packed: uint8_t
    :param packed:      output packed codes, size (n, code_size)
    :type code_size: int
    :param code_size:   should be >= ceil(M * nbit / 8)

    |

    *Overload 2:*
     Unpack a set of n codes of variable sizes

    :param nbit:       number of bits per entry (size M)
    """
    ...

def popcount32(x):
    ...

def popcount64(x):
    ...

def get_num_gpus():
    ...

def gpu_profiler_start():
    ...

def gpu_profiler_stop():
    ...

def gpu_sync_all_devices():
    ...

def get_compile_options():
    r"""get compile options"""
    ...

def get_version():
    ...

def getmillisecs():
    r"""ms elapsed since some arbitrary epoch"""
    ...

def get_mem_usage_kb():
    r"""get current RSS usage in kB"""
    ...

def get_cycles():
    ...

def reflection(u, x, n, d, nu):
    ...

def matrix_qr(m, n, a):
    r"""
     compute the Q of the QR decomposition for m > n
    :type a: float
    :param a:   size n * m: input matrix and output Q
    """
    ...

def ranklist_handle_ties(k, idx, dis):
    r"""distances are supposed to be sorted. Sorts indices with same distance"""
    ...

def ranklist_intersection_size(k1, v1, k2, v2):
    r"""
     count the number of common elements between v1 and v2
    algorithm = sorting + bissection to avoid double-counting duplicates
    """
    ...

def merge_result_table_with(n, k, I0, D0, I1, D1, keep_min=..., translation=...):
    r"""
     merge a result table into another one

    :type I0: int
    :param I0:, D0       first result table, size (n, k)
    :type I1: int
    :param I1:, D1       second result table, size (n, k)
    :type keep_min: boolean, optional
    :param keep_min:     if true, keep min values, otherwise keep max
    :type translation: int, optional
    :param translation:  add this value to all I1's indexes
    :rtype: int
    :return: nb of values that were taken from the second table
    """
    ...

def imbalance_factor(*args):
    r"""
    *Overload 1:*
    a balanced assignment has a IF of 1, a completely unbalanced assignment has
    an IF = k.

    |

    *Overload 2:*
    same, takes a histogram as input
    """
    ...

def ivec_hist(n, v, vmax, hist):
    r"""compute histogram on v"""
    ...

def bincode_hist(n, nbits, codes, hist):
    r"""
     Compute histogram of bits on a code array

    :type codes: uint8_t
    :param codes:   size(n, nbits / 8)
    :type hist: int
    :param hist:    size(nbits): nb of 1s in the array of codes
    """
    ...

def ivec_checksum(n, a):
    r"""compute a checksum on a table."""
    ...

def bvec_checksum(n, a):
    r"""compute a checksum on a table."""
    ...

def bvecs_checksum(n, d, a, cs):
    r"""
     compute checksums for the rows of a matrix

    :type n: int
    :param n:   number of rows
    :type d: int
    :param d:   size per row
    :type a: uint8_t
    :param a:   matrix to handle, size n * d
    :type cs: int
    :param cs:  output checksums, size n
    """
    ...

def fvecs_maybe_subsample(d, n, nmax, x, verbose=..., seed=...):
    r"""
     random subsamples a set of vectors if there are too many of them

    :type d: int
    :param d:      dimension of the vectors
    :type n: int
    :param n:      on input: nb of input vectors, output: nb of output vectors
    :type nmax: int
    :param nmax:   max nb of vectors to keep
    :type x: float
    :param x:      input array, size *n-by-d
    :type seed: int, optional
    :param seed:   random seed to use for sampling
    :rtype: float
    :return: x or an array allocated with new [] with *n vectors
    """
    ...

def binary_to_real(d, x_in, x_out):
    r"""
     Convert binary vector to +1/-1 valued float vector.

    :type d: int
    :param d:      dimension of the vector (multiple of 8)
    :type x_in: uint8_t
    :param x_in:   input binary vector (uint8_t table of size d / 8)
    :type x_out: float
    :param x_out:  output float vector (float table of size d)
    """
    ...

def real_to_binary(d, x_in, x_out):
    r"""
     Convert float vector to binary vector. Components > 0 are converted to 1,
    others to 0.

    :type d: int
    :param d:      dimension of the vector (multiple of 8)
    :type x_in: float
    :param x_in:   input float vector (float table of size d)
    :type x_out: uint8_t
    :param x_out:  output binary vector (uint8_t table of size d / 8)
    """
    ...

def hash_bytes(bytes, n):
    r"""A reasonable hashing function"""
    ...

def check_openmp():
    r"""Whether OpenMP annotations were respected."""
    ...

class CodeSet:
    thisown = ...
    __repr__ = ...
    d = ...
    s = ...
    def __init__(self, d) -> None:
        ...
    
    def insert(self, n, codes, inserted):
        ...
    
    __swig_destroy__ = ...


hamdis_tab_ham_bytes = ...
class CombinerRangeKNNfloat:
    r"""
     This class is used to combine range and knn search results
    in contrib.exhaustive_search.range_search_gpu
    """
    thisown = ...
    __repr__ = ...
    nq = ...
    k = ...
    r2 = ...
    keep_max = ...
    def __init__(self, nq, k, r2, keep_max) -> None:
        r"""whether to keep max values instead of min."""
        ...
    
    I = ...
    D = ...
    mask = ...
    lim_remain = ...
    D_remain = ...
    I_remain = ...
    L_res = ...
    def compute_sizes(self, L_res):
        r"""size nq + 1"""
        ...
    
    def write_result(self, D_res, I_res):
        r"""
        Phase 2: caller allocates D_res and I_res (size L_res[nq])
        Phase 3: fill in D_res and I_res
        """
        ...
    
    __swig_destroy__ = ...


class CombinerRangeKNNint16:
    r"""
     This class is used to combine range and knn search results
    in contrib.exhaustive_search.range_search_gpu
    """
    thisown = ...
    __repr__ = ...
    nq = ...
    k = ...
    r2 = ...
    keep_max = ...
    def __init__(self, nq, k, r2, keep_max) -> None:
        r"""whether to keep max values instead of min."""
        ...
    
    I = ...
    D = ...
    mask = ...
    lim_remain = ...
    D_remain = ...
    I_remain = ...
    L_res = ...
    def compute_sizes(self, L_res):
        r"""size nq + 1"""
        ...
    
    def write_result(self, D_res, I_res):
        r"""
        Phase 2: caller allocates D_res and I_res (size L_res[nq])
        Phase 3: fill in D_res and I_res
        """
        ...
    
    __swig_destroy__ = ...


def fvec_L2sqr(x, y, d):
    r"""Squared L2 distance between two vectors"""
    ...

def fvec_inner_product(x, y, d):
    r"""inner product"""
    ...

def fvec_L1(x, y, d):
    r"""L1 distance"""
    ...

def fvec_Linf(x, y, d):
    r"""infinity distance"""
    ...

def fvec_inner_product_batch_4(x, y0, y1, y2, y3, d, dis0, dis1, dis2, dis3):
    r"""
    Special version of inner product that computes 4 distances
    between x and yi, which is performance oriented.
    """
    ...

def fvec_L2sqr_batch_4(x, y0, y1, y2, y3, d, dis0, dis1, dis2, dis3):
    r"""
    Special version of L2sqr that computes 4 distances
    between x and yi, which is performance oriented.
    """
    ...

def pairwise_L2sqr(d, nq, xq, nb, xb, dis, ldq=..., ldb=..., ldd=...):
    r"""
     Compute pairwise distances between sets of vectors

    :type d: int
    :param d:     dimension of the vectors
    :type nq: int
    :param nq:    nb of query vectors
    :type nb: int
    :param nb:    nb of database vectors
    :type xq: float
    :param xq:    query vectors (size nq * d)
    :type xb: float
    :param xb:    database vectors (size nb * d)
    :type dis: float
    :param dis:   output distances (size nq * nb)
    :param ldq,ldb:, ldd strides for the matrices
    """
    ...

def fvec_inner_products_ny(ip, x, y, d, ny):
    ...

def fvec_L2sqr_ny(dis, x, y, d, ny):
    ...

def fvec_L2sqr_ny_transposed(dis, x, y, y_sqlen, d, d_offset, ny):
    ...

def fvec_L2sqr_ny_nearest(distances_tmp_buffer, x, y, d, ny):
    ...

def fvec_L2sqr_ny_nearest_y_transposed(distances_tmp_buffer, x, y, y_sqlen, d, d_offset, ny):
    ...

def fvec_norm_L2sqr(x, d):
    r"""squared norm of a vector"""
    ...

def fvec_norms_L2(norms, x, d, nx):
    r"""
     compute the L2 norms for a set of vectors

    :type norms: float
    :param norms:    output norms, size nx
    :type x: float
    :param x:        set of vectors, size nx * d
    """
    ...

def fvec_norms_L2sqr(norms, x, d, nx):
    r"""same as fvec_norms_L2, but computes squared norms"""
    ...

def fvec_renorm_L2(d, nx, x):
    ...

def inner_product_to_L2sqr(dis, nr1, nr2, n1, n2):
    ...

def fvec_add(*args):
    r"""
    *Overload 1:*
     compute c := a + b for vectors

    c and a can overlap, c and b can overlap

    :type a: float
    :param a: size d
    :type b: float
    :param b: size d
    :type c: float
    :param c: size d

    |

    *Overload 2:*
     compute c := a + b for a, c vectors and b a scalar

    c and a can overlap

    :type a: float
    :param a: size d
    :type c: float
    :param c: size d
    """
    ...

def fvec_sub(d, a, b, c):
    r"""
     compute c := a - b for vectors

    c and a can overlap, c and b can overlap

    :type a: float
    :param a: size d
    :type b: float
    :param b: size d
    :type c: float
    :param c: size d
    """
    ...

def fvec_inner_products_by_idx(ip, x, y, ids, d, nx, ny):
    r"""
     compute the inner product between x and a subset y of ny vectors defined by
    ids

    ip(i, j) = inner_product(x(i, :), y(ids(i, j), :))

    :type ip: float
    :param ip:    output array, size nx * ny
    :type x: float
    :param x:     first-term vector, size nx * d
    :type y: float
    :param y:     second-term vector, size (max(ids) + 1) * d
    :type ids: int
    :param ids:   ids to sample from y, size nx * ny
    """
    ...

def fvec_L2sqr_by_idx(dis, x, y, ids, d, nx, ny):
    r"""
     compute the squared L2 distances between x and a subset y of ny vectors
    defined by ids

    dis(i, j) = inner_product(x(i, :), y(ids(i, j), :))

    :type dis: float
    :param dis:   output array, size nx * ny
    :type x: float
    :param x:     first-term vector, size nx * d
    :type y: float
    :param y:     second-term vector, size (max(ids) + 1) * d
    :type ids: int
    :param ids:   ids to sample from y, size nx * ny
    """
    ...

def pairwise_indexed_L2sqr(d, n, x, ix, y, iy, dis):
    r"""
     compute dis[j] = L2sqr(x[ix[j]], y[iy[j]]) forall j=0..n-1

    :type x: float
    :param x:  size (max(ix) + 1, d)
    :type y: float
    :param y:  size (max(iy) + 1, d)
    :type ix: int
    :param ix: size n
    :type iy: int
    :param iy: size n
    :type dis: float
    :param dis: size n
    """
    ...

def pairwise_indexed_inner_product(d, n, x, ix, y, iy, dis):
    r"""
     compute dis[j] = inner_product(x[ix[j]], y[iy[j]]) forall j=0..n-1

    :type x: float
    :param x:  size (max(ix) + 1, d)
    :type y: float
    :param y:  size (max(iy) + 1, d)
    :type ix: int
    :param ix: size n
    :type iy: int
    :param iy: size n
    :type dis: float
    :param dis: size n
    """
    ...

def knn_inner_product(*args):
    r"""
    *Overload 1:*
     Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, w.r.t to max inner product.

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type res: :py:class:`float_minheap_array_t`
    :param res:  result heap structure, which also provides k. Sorted on output

    |

    *Overload 2:*
      Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the inner product metric.

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type distances: float
    :param distances:  output distances, size nq * k
    :type indexes: int
    :param indexes:    output vector ids, size nq * k

    |

    *Overload 3:*
      Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the inner product metric.

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type distances: float
    :param distances:  output distances, size nq * k
    :type indexes: int
    :param indexes:    output vector ids, size nq * k
    """
    ...

def knn_L2sqr(*args):
    r"""
    *Overload 1:*
     Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the L2 distance
    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type res: :py:class:`float_maxheap_array_t`
    :param res:  result heap strcture, which also provides k. Sorted on output
    :type y_norm2: float, optional
    :param y_norm2:    (optional) norms for the y vectors (nullptr or size ny)
    :type sel: :py:class:`IDSelector`, optional
    :param sel:  search in this subset of vectors

    |

    *Overload 2:*
      Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the L2 distance

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type distances: float
    :param distances:  output distances, size nq * k
    :type indexes: int
    :param indexes:    output vector ids, size nq * k
    :type y_norm2: float, optional
    :param y_norm2:    (optional) norms for the y vectors (nullptr or size ny)
    :type sel: :py:class:`IDSelector`, optional
    :param sel:  search in this subset of vectors

    |

    *Overload 3:*
      Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the L2 distance

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type distances: float
    :param distances:  output distances, size nq * k
    :type indexes: int
    :param indexes:    output vector ids, size nq * k
    :type y_norm2: float, optional
    :param y_norm2:    (optional) norms for the y vectors (nullptr or size ny)
    :param sel:  search in this subset of vectors

    |

    *Overload 4:*
      Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, for the L2 distance

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size ny * d
    :type distances: float
    :param distances:  output distances, size nq * k
    :type indexes: int
    :param indexes:    output vector ids, size nq * k
    :param y_norm2:    (optional) norms for the y vectors (nullptr or size ny)
    :param sel:  search in this subset of vectors
    """
    ...

def knn_inner_products_by_idx(x, y, subset, d, nx, ny, nsubset, k, vals, ids, ld_ids=...):
    r"""
     Find the max inner product neighbors for nx queries in a set of ny vectors
    indexed by ids. May be useful for re-ranking a pre-selected vector list

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size (max(ids) + 1) * d
    :type ids: int
    :param ids:  subset of database vectors to consider, size (nx, nsubset)
    :param res:  result structure
    :type ld_ids: int, optional
    :param ld_ids: stride for the ids array. -1: use nsubset, 0: all queries
        process the same subset
    """
    ...

def knn_L2sqr_by_idx(x, y, subset, d, nx, ny, nsubset, k, vals, ids, ld_subset=...):
    r"""
     Find the nearest neighbors for nx queries in a set of ny vectors
    indexed by ids. May be useful for re-ranking a pre-selected vector list

    :type x: float
    :param x:    query vectors, size nx * d
    :type y: float
    :param y:    database vectors, size (max(ids) + 1) * d
    :type subset: int
    :param subset: subset of database vectors to consider, size (nx, nsubset)
    :param res:  rIDesult structure
    :type ld_subset: int, optional
    :param ld_subset: stride for the subset array. -1: use nsubset, 0: all queries
        process the same subset
    """
    ...

def range_search_L2sqr(x, y, d, nx, ny, radius, result, sel=...):
    r"""
     Return the k nearest neighbors of each of the nx vectors x among the ny
     vector y, w.r.t to max inner product

    :type x: float
    :param x:      query vectors, size nx * d
    :type y: float
    :param y:      database vectors, size ny * d
    :type radius: float
    :param radius: search radius around the x vectors
    :type result: :py:class:`RangeSearchResult`
    :param result: result structure
    """
    ...

def range_search_inner_product(x, y, d, nx, ny, radius, result, sel=...):
    r"""same as range_search_L2sqr for the inner product similarity"""
    ...

def compute_PQ_dis_tables_dsub2(d, ksub, centroids, nx, x, is_inner_product, dis_tables):
    r"""specialized function for PQ2"""
    ...

def fvec_madd(n, a, bf, b, c):
    r"""
     compute c := a + bf * b for a, b and c tables

    :type n: int
    :param n:   size of the tables
    :type a: float
    :param a:   size n
    :type b: float
    :param b:   size n
    :type c: float
    :param c:   result table, size n
    """
    ...

def fvec_madd_and_argmin(n, a, bf, b, c):
    r"""
     same as fvec_madd, also return index of the min of the result table
    :rtype: int
    :return: index of the min of table c
    """
    ...

class RandomGenerator:
    r"""random generator that can be used in multithreaded contexts"""
    thisown = ...
    __repr__ = ...
    mt = ...
    def rand_int64(self):
        r"""random int64_t"""
        ...
    
    def rand_int(self, *args):
        r"""
        *Overload 1:*
        random positive integer

        |

        *Overload 2:*
        generate random integer between 0 and max-1
        """
        ...
    
    def rand_float(self):
        r"""between 0 and 1"""
        ...
    
    def rand_double(self):
        ...
    
    def __init__(self, seed=...) -> None:
        ...
    
    __swig_destroy__ = ...


class SplitMix64RandomGenerator:
    r"""
    fast random generator that cannot be used in multithreaded contexts.
    based on https://prng.di.unimi.it/
    """
    thisown = ...
    __repr__ = ...
    state = ...
    def rand_int64(self):
        r"""random int64_t"""
        ...
    
    def rand_int(self, *args):
        r"""
        *Overload 1:*
        random positive integer

        |

        *Overload 2:*
        generate random integer between 0 and max-1
        """
        ...
    
    def rand_float(self):
        r"""between 0 and 1"""
        ...
    
    def rand_double(self):
        ...
    
    def __init__(self, seed=...) -> None:
        ...
    
    def next(self):
        ...
    
    __swig_destroy__ = ...


def float_rand(x, n, seed):
    ...

def float_randn(x, n, seed):
    ...

def int64_rand(x, n, seed):
    ...

def byte_rand(x, n, seed):
    ...

def int64_rand_max(x, n, max, seed):
    ...

def rand_perm(perm, n, seed):
    ...

def rand_perm_splitmix64(perm, n, seed):
    ...

def rand_smooth_vectors(n, d, x, seed):
    ...

def fvec_argsort(n, vals, perm):
    r"""
     Indirect sort of a floating-point array

    :type n: int
    :param n:     size of the array
    :type vals: float
    :param vals:  array to sort, size n
    :type perm: int
    :param perm:  output: permutation of [0..n-1], st.
                     vals[perm[i + 1]] >= vals[perm[i]]
    """
    ...

def fvec_argsort_parallel(n, vals, perm):
    r"""Same as fvec_argsort, parallelized"""
    ...

def bucket_sort(nval, vals, nbucket, lims, perm, nt=...):
    r"""
     Bucket sort of a list of values

    :type vals: int
    :param vals:     values to sort, size nval, max value nbucket - 1
    :type lims: int
    :param lims:     output limits of buckets, size nbucket + 1
    :type perm: int
    :param perm:     output buckets, the elements of bucket
                        i are in perm[lims[i]:lims[i + 1]]
    :type nt: int, optional
    :param nt:       number of threads (0 = pure sequential code)
    """
    ...

def matrix_bucket_sort_inplace(*args):
    r"""
    *Overload 1:*
     in-place bucket sort (with attention to memory=>int32)
    on input the values are in a nrow * col matrix
    we want to store the row numbers in the output.

    :type vals: int
    :param vals:     positive values to sort, size nrow * ncol,
                        max value nbucket - 1
    :type lims: int
    :param lims:     output limits of buckets, size nbucket + 1
    :type nt: int, optional
    :param nt:       number of threads (0 = pure sequential code)

    |

    *Overload 2:*
     same with int64 elements

    |

    *Overload 3:*
     same with int64 elements
    """
    ...

def hashtable_int64_to_int64_init(log2_capacity, tab):
    r"""
     Hashtable implementation for int64 -> int64 with external storage
    implemented for fast batch add and lookup.

    tab is of size  2 * (1 << log2_capacity)
    n is the number of elements to add or search

    adding several values in a same batch: an arbitrary one gets added
    in different batches: the newer batch overwrites.
    raises an exception if capacity is exhausted.
    """
    ...

def hashtable_int64_to_int64_add(log2_capacity, tab, n, keys, vals):
    ...

def hashtable_int64_to_int64_lookup(log2_capacity, tab, n, keys, vals):
    ...

METRIC_INNER_PRODUCT = ...
METRIC_L2 = ...
METRIC_L1 = ...
METRIC_Linf = ...
METRIC_Lp = ...
METRIC_Canberra = ...
METRIC_BrayCurtis = ...
METRIC_JensenShannon = ...
METRIC_Jaccard = ...
METRIC_NaNEuclidean = ...
METRIC_ABS_INNER_PRODUCT = ...
def is_similarity_metric(metric_type):
    r"""
    this function is used to distinguish between min and max indexes since
    we need to support similarity and dis-similarity metrics in a flexible way
    """
    ...

FAISS_VERSION_MAJOR = ...
FAISS_VERSION_MINOR = ...
FAISS_VERSION_PATCH = ...
VERSION_STRING = ...
class SearchParameters:
    r"""
     Parent class for the optional search paramenters.

    Sub-classes with additional search parameters should inherit this class.
    Ownership of the object fields is always to the caller.
    """
    thisown = ...
    __repr__ = ...
    sel = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class Index:
    r"""
     Abstract structure for an index, supports adding vectors and searching
    them.

    All vectors provided at add or search time are 32-bit float arrays,
    although the internal representation may vary.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    d = ...
    ntotal = ...
    verbose = ...
    is_trained = ...
    metric_type = ...
    metric_arg = ...
    __swig_destroy__ = ...
    def train(self, n, x):
        r"""
         Perform training on a representative set of vectors

        :type n: int
        :param n:      nb of training vectors
        :type x: float
        :param x:      training vecors, size n * d
        """
        ...
    
    def add(self, n, x):
        r"""
         Add n vectors of dimension d to the index.

        Vectors are implicitly assigned labels ntotal .. ntotal + n - 1
        This function slices the input vectors in chunks smaller than
        blocksize_add and calls add_core.
        :type n: int
        :param n:      number of vectors
        :type x: float
        :param x:      input matrix, size n * d
        """
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""
         Same as add, but stores xids instead of sequential ids.

        The default implementation fails with an assertion, as it is
        not supported by all indexes.

        :type n: int
        :param n:         number of vectors
        :type x: float
        :param x:         input vectors, size n * d
        :type xids: int
        :param xids:      if non-null, ids to store for the vectors (size n)
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""
         query n vectors of dimension d to the index.

        return at most k vectors. If there are not enough results for a
        query, the result array is padded with -1s.

        :type n: int
        :param n:           number of vectors
        :type x: float
        :param x:           input vectors to search, size n * d
        :type k: int
        :param k:           number of extracted vectors
        :type distances: float
        :param distances:   output pairwise distances, size n*k
        :type labels: int
        :param labels:      output labels of the NNs, size n*k
        """
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        r"""
         query n vectors of dimension d to the index.

        return all vectors with distance < radius. Note that many
        indexes do not implement the range_search (only the k-NN search
        is mandatory).

        :type n: int
        :param n:           number of vectors
        :type x: float
        :param x:           input vectors to search, size n * d
        :type radius: float
        :param radius:      search radius
        :type result: :py:class:`RangeSearchResult`
        :param result:      result table
        """
        ...
    
    def assign(self, n, x, labels, k=...):
        r"""
         return the indexes of the k vectors closest to the query x.

        This function is identical as search but only return labels of
        neighbors.
        :type n: int
        :param n:           number of vectors
        :type x: float
        :param x:           input vectors to search, size n * d
        :type labels: int
        :param labels:      output labels of the NNs, size n*k
        :type k: int, optional
        :param k:           number of nearest neighbours
        """
        ...
    
    def reset(self):
        r"""removes all elements from the database."""
        ...
    
    def remove_ids(self, sel):
        r"""
         removes IDs from the index. Not supported by all
        indexes. Returns the number of elements removed.
        """
        ...
    
    def reconstruct(self, key, recons):
        r"""
         Reconstruct a stored vector (or an approximation if lossy coding)

        this function may not be defined for some indexes
        :type key: int
        :param key:         id of the vector to reconstruct
        :type recons: float
        :param recons:      reconstucted vector (size d)
        """
        ...
    
    def reconstruct_batch(self, n, keys, recons):
        r"""
         Reconstruct several stored vectors (or an approximation if lossy
        coding)

        this function may not be defined for some indexes
        :type n: int
        :param n:           number of vectors to reconstruct
        :type keys: int
        :param keys:        ids of the vectors to reconstruct (size n)
        :type recons: float
        :param recons:      reconstucted vector (size n * d)
        """
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        r"""
         Reconstruct vectors i0 to i0 + ni - 1

        this function may not be defined for some indexes
        :type i0: int
        :param i0:          index of the first vector in the sequence
        :type ni: int
        :param ni:          number of vectors in the sequence
        :type recons: float
        :param recons:      reconstucted vector (size ni * d)
        """
        ...
    
    def search_and_reconstruct(self, n, x, k, distances, labels, recons, params=...):
        r"""
         Similar to search, but also reconstructs the stored vectors (or an
        approximation in the case of lossy coding) for the search results.

        If there are not enough results for a query, the resulting arrays
        is padded with -1s.

        :type n: int
        :param n:           number of vectors
        :type x: float
        :param x:           input vectors to search, size n * d
        :type k: int
        :param k:           number of extracted vectors
        :type distances: float
        :param distances:   output pairwise distances, size n*k
        :type labels: int
        :param labels:      output labels of the NNs, size n*k
        :type recons: float
        :param recons:      reconstructed vectors size (n, k, d)
        """
        ...
    
    def compute_residual(self, x, residual, key):
        r"""
         Computes a residual vector after indexing encoding.

        The residual vector is the difference between a vector and the
        reconstruction that can be decoded from its representation in
        the index. The residual can be used for multiple-stage indexing
        methods, like IndexIVF's methods.

        :type x: float
        :param x:           input vector, size d
        :type residual: float
        :param residual:    output residual vector, size d
        :type key: int
        :param key:         encoded index, as returned by search and assign
        """
        ...
    
    def compute_residual_n(self, n, xs, residuals, keys):
        r"""
         Computes a residual vector after indexing encoding (batch form).
        Equivalent to calling compute_residual for each vector.

        The residual vector is the difference between a vector and the
        reconstruction that can be decoded from its representation in
        the index. The residual can be used for multiple-stage indexing
        methods, like IndexIVF's methods.

        :type n: int
        :param n:           number of vectors
        :type xs: float
        :param xs:          input vectors, size (n x d)
        :type residuals: float
        :param residuals:   output residual vectors, size (n x d)
        :type keys: int
        :param keys:        encoded index, as returned by search and assign
        """
        ...
    
    def get_distance_computer(self):
        r"""
         Get a DistanceComputer (defined in AuxIndexStructures) object
        for this kind of index.

        DistanceComputer is implemented for indexes that support random
        access of their vectors.
        """
        ...
    
    def sa_code_size(self):
        r"""size of the produced codes in bytes"""
        ...
    
    def sa_encode(self, n, x, bytes):
        r"""
         encode a set of vectors

        :type n: int
        :param n:       number of vectors
        :type x: float
        :param x:       input vectors, size n * d
        :type bytes: uint8_t
        :param bytes:   output encoded vectors, size n * sa_code_size()
        """
        ...
    
    def sa_decode(self, n, bytes, x):
        r"""
         decode a set of vectors

        :type n: int
        :param n:       number of vectors
        :type bytes: uint8_t
        :param bytes:   input encoded vectors, size n * sa_code_size()
        :type x: float
        :param x:       output vectors, size n * d
        """
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        r"""
         moves the entries from another dataset to self.
        On output, other is empty.
        add_id is added to all moved ids
        (for sequential ids, this would be this->ntotal)
        """
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        r"""
         check that the two indexes are compatible (ie, they are
        trained in the same way and have the same
        parameters). Otherwise throw.
        """
        ...
    
    def add_sa_codes(self, n, codes, xids):
        r"""
         Add vectors that are computed with the standalone codec

        :type codes: uint8_t
        :param codes:  codes to add size n * sa_code_size()
        :type xids: int
        :param xids:   corresponding ids, size n
        """
        ...
    


class DistanceComputer:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def set_query(self, x):
        r"""
        called before computing distances. Pointer x should remain valid
        while operator () is called
        """
        ...
    
    def __call__(self, i):
        r"""compute distance of vector i to current query"""
        ...
    
    def distances_batch_4(self, idx0, idx1, idx2, idx3, dis0, dis1, dis2, dis3):
        r"""
        compute distances of current query to 4 stored vectors.
        certain DistanceComputer implementations may benefit
        heavily from this.
        """
        ...
    
    def symmetric_dis(self, i, j):
        r"""compute distance between two stored vectors"""
        ...
    
    __swig_destroy__ = ...


class NegativeDistanceComputer(DistanceComputer):
    thisown = ...
    __repr__ = ...
    basedis = ...
    def __init__(self, basedis) -> None:
        ...
    
    def set_query(self, x):
        ...
    
    def __call__(self, i):
        r"""compute distance of vector i to current query"""
        ...
    
    def distances_batch_4(self, idx0, idx1, idx2, idx3, dis0, dis1, dis2, dis3):
        ...
    
    def symmetric_dis(self, i, j):
        r"""compute distance between two stored vectors"""
        ...
    
    __swig_destroy__ = ...


class FlatCodesDistanceComputer(DistanceComputer):
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    codes = ...
    code_size = ...
    def __call__(self, i):
        ...
    
    def distance_to_code(self, code):
        r"""compute distance of current query to an encoded vector"""
        ...
    
    __swig_destroy__ = ...


class IOReader:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    name = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    
    __swig_destroy__ = ...


class IOWriter:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    name = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    
    __swig_destroy__ = ...


class VectorIOReader(IOReader):
    thisown = ...
    __repr__ = ...
    data = ...
    rp = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class VectorIOWriter(IOWriter):
    thisown = ...
    __repr__ = ...
    data = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class FileIOReader(IOReader):
    thisown = ...
    __repr__ = ...
    f = ...
    need_close = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    


class FileIOWriter(IOWriter):
    thisown = ...
    __repr__ = ...
    f = ...
    need_close = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def __call__(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    


class BufferedIOReader(IOReader):
    r"""wraps an ioreader to make buffered reads to avoid too small reads"""
    thisown = ...
    __repr__ = ...
    reader = ...
    bsz = ...
    ofs = ...
    ofs2 = ...
    b0 = ...
    b1 = ...
    buffer = ...
    def __init__(self, *args) -> None:
        r"""
        :type bsz: int, optional
        :param bsz:    buffer size (bytes). Reads will be done by batched of
                          this size
        """
        ...
    
    def __call__(self, ptr, size, nitems):
        ...
    
    __swig_destroy__ = ...


class BufferedIOWriter(IOWriter):
    thisown = ...
    __repr__ = ...
    writer = ...
    bsz = ...
    ofs = ...
    ofs2 = ...
    b0 = ...
    buffer = ...
    def __init__(self, *args) -> None:
        ...
    
    def __call__(self, ptr, size, nitems):
        ...
    
    __swig_destroy__ = ...


def fourcc(*args):
    r"""cast a 4-character string to a uint32_t that can be written and read easily"""
    ...

def fourcc_inv(*args):
    ...

def fourcc_inv_printable(x):
    ...

class MaybeOwnedVectorOwner:
    thisown = ...
    __repr__ = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class MmappedFileMappingOwner(MaybeOwnedVectorOwner):
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def data(self):
        ...
    
    def size(self):
        ...
    


class MappedFileIOReader(IOReader):
    thisown = ...
    __repr__ = ...
    mmap_owner = ...
    pos = ...
    def __init__(self, owner) -> None:
        ...
    
    def __call__(self, ptr, size, nitems):
        ...
    
    def mmap(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    
    __swig_destroy__ = ...


class ZeroCopyIOReader(IOReader):
    thisown = ...
    __repr__ = ...
    data_ = ...
    rp_ = ...
    total_ = ...
    def __init__(self, data, size) -> None:
        ...
    
    __swig_destroy__ = ...
    def reset(self):
        ...
    
    def get_data_view(self, ptr, size, nitems):
        ...
    
    def __call__(self, ptr, size, nitems):
        ...
    
    def filedescriptor(self):
        ...
    


class IndexFlatCodes(Index):
    r"""
     Index that encodes all vectors as fixed-size codes (size code_size). Storage
    is in the codes vector
    """
    thisown = ...
    __repr__ = ...
    code_size = ...
    codes = ...
    def __init__(self, *args) -> None:
        ...
    
    def add(self, n, x):
        r"""default add uses sa_encode"""
        ...
    
    def reset(self):
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def sa_code_size(self):
        ...
    
    def remove_ids(self, sel):
        r"""
         remove some ids. NB that because of the structure of the
        index, the semantics of this operation are
        different from the usual ones: the new ids are shifted
        """
        ...
    
    def get_FlatCodesDistanceComputer(self):
        r"""
         a FlatCodesDistanceComputer offers a distance_to_code method

        The default implementation explicitly decodes the vector with sa_decode.
        """
        ...
    
    def get_distance_computer(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""Search implemented by decoding"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def get_CodePacker(self):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    def add_sa_codes(self, n, x, xids):
        ...
    
    def permute_entries(self, perm):
        ...
    
    __swig_destroy__ = ...


class IndexFlat(IndexFlatCodes):
    r"""Index that stores the full vectors and performs exhaustive search"""
    thisown = ...
    __repr__ = ...
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def compute_distance_subset(self, n, x, k, distances, labels):
        r"""
         compute distance with a subset of vectors

        :type x: float
        :param x:       query vectors, size n * d
        :type labels: int
        :param labels:  indices of the vectors that should be compared
                           for each query vector, size n * k
        :type distances: float
        :param distances:
                           corresponding output distances, size n * k
        """
        ...
    
    def get_xb(self, *args):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class IndexFlatIP(IndexFlat):
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexFlatL2(IndexFlat):
    thisown = ...
    __repr__ = ...
    cached_l2norms = ...
    def __init__(self, *args) -> None:
        r"""
        :type d: int
        :param d: dimensionality of the input vectors
        """
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    def sync_l2norms(self):
        ...
    
    def clear_l2norms(self):
        ...
    
    __swig_destroy__ = ...


class IndexFlat1D(IndexFlatL2):
    r"""optimized version for 1D "vectors"."""
    thisown = ...
    __repr__ = ...
    continuous_update = ...
    perm = ...
    def __init__(self, continuous_update=...) -> None:
        ...
    
    def update_permutation(self):
        r"""
        if not continuous_update, call this between the last add and
        the first search
        """
        ...
    
    def add(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""Warn: the distances returned are L1 not L2"""
        ...
    
    __swig_destroy__ = ...


class ClusteringParameters:
    r"""
     Class for the clustering parameters. Can be passed to the
    constructor of the Clustering object.
    """
    thisown = ...
    __repr__ = ...
    niter = ...
    nredo = ...
    verbose = ...
    spherical = ...
    int_centroids = ...
    update_index = ...
    frozen_centroids = ...
    min_points_per_centroid = ...
    max_points_per_centroid = ...
    seed = ...
    decode_block_size = ...
    check_input_data_for_NaNs = ...
    use_faster_subsampling = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class ClusteringIterationStats:
    thisown = ...
    __repr__ = ...
    obj = ...
    time = ...
    time_search = ...
    imbalance_factor = ...
    nsplit = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class Clustering(ClusteringParameters):
    r"""
     K-means clustering based on assignment - centroid update iterations

    The clustering is based on an Index object that assigns training
    points to the centroids. Therefore, at each iteration the centroids
    are added to the index.

    On output, the centoids table is set to the latest version
    of the centroids and they are also added to the index. If the
    centroids table it is not empty on input, it is also used for
    initialization.
    """
    thisown = ...
    __repr__ = ...
    d = ...
    k = ...
    centroids = ...
    iteration_stats = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x, index, x_weights=...):
        r"""
         run k-means training

        :type x: float
        :param x:          training vectors, size n * d
        :type index: :py:class:`Index`
        :param index:      index used for assignment
        :type x_weights: float, optional
        :param x_weights:  weight associated to each vector: NULL or size n
        """
        ...
    
    def train_encoded(self, nx, x_in, codec, index, weights=...):
        r"""
         run with encoded vectors

        win addition to train()'s parameters takes a codec as parameter
        to decode the input vectors.

        :type codec: :py:class:`Index`
        :param codec:      codec used to decode the vectors (nullptr =
                              vectors are in fact floats)
        """
        ...
    
    def post_process_centroids(self):
        r"""
        Post-process the centroids after each centroid update.
        includes optional L2 normalization and nearest integer rounding
        """
        ...
    
    __swig_destroy__ = ...


class Clustering1D(Clustering):
    r"""
     Exact 1D clustering algorithm

    Since it does not use an index, it does not overload the train() function
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_exact(self, n, x):
        ...
    
    __swig_destroy__ = ...


class ProgressiveDimClusteringParameters(ClusteringParameters):
    thisown = ...
    __repr__ = ...
    progressive_dim_steps = ...
    apply_pca = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class ProgressiveDimIndexFactory:
    r"""generates an index suitable for clustering when called"""
    thisown = ...
    __repr__ = ...
    def __call__(self, dim):
        r"""ownership transferred to caller"""
        ...
    
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class ProgressiveDimClustering(ProgressiveDimClusteringParameters):
    r"""
     K-means clustering with progressive dimensions used

    The clustering first happens in dim 1, then with exponentially increasing
    dimension until d (I steps). This is typically applied after a PCA
    transformation (optional). Reference:

    "Improved Residual Vector Quantization for High-dimensional Approximate
    Nearest Neighbor Search"

    Shicong Liu, Hongtao Lu, Junru Shao, AAAI'15

    https://arxiv.org/abs/1509.05195
    """
    thisown = ...
    __repr__ = ...
    d = ...
    k = ...
    centroids = ...
    iteration_stats = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x, factory):
        ...
    
    __swig_destroy__ = ...


def kmeans_clustering(d, n, k, x, centroids):
    r"""
     simplified interface

    :type d: int
    :param d: dimension of the data
    :type n: int
    :param n: nb of training vectors
    :type k: int
    :param k: nb of output centroids
    :type x: float
    :param x: training set (size n * d)
    :type centroids: float
    :param centroids: output centroids (size k * d)
    :rtype: float
    :return: final quantization error
    """
    ...

def pairwise_extra_distances(d, nq, xq, nb, xb, mt, metric_arg, dis, ldq=..., ldb=..., ldd=...):
    ...

def knn_extra_metrics(x, y, d, nx, ny, mt, metric_arg, k, distances, indexes):
    ...

def get_extra_distance_computer(d, mt, metric_arg, nb, xb):
    r"""
    get a DistanceComputer that refers to this type of distance and
    indexes a flat array of size nb
    """
    ...

class Quantizer:
    r"""General interface for quantizer objects"""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    d = ...
    code_size = ...
    def train(self, n, x):
        r"""
         Train the quantizer

        :type x: float
        :param x:       training vectors, size n * d
        """
        ...
    
    def compute_codes(self, x, codes, n):
        r"""
         Quantize a set of vectors

        :type x: float
        :param x:        input vectors, size n * d
        :type codes: uint8_t
        :param codes:    output codes, size n * code_size
        """
        ...
    
    def decode(self, code, x, n):
        r"""
         Decode a set of vectors

        :param codes:    input codes, size n * code_size
        :type x: float
        :param x:        output vectors, size n * d
        """
        ...
    
    __swig_destroy__ = ...


class ProductQuantizer(Quantizer):
    r"""
     Product Quantizer.
    PQ is trained using k-means, minimizing the L2 distance to centroids.
    PQ supports L2 and Inner Product search, however the quantization error is
    biased towards L2 distance.
    """
    thisown = ...
    __repr__ = ...
    M = ...
    nbits = ...
    dsub = ...
    ksub = ...
    verbose = ...
    Train_default = ...
    Train_hot_start = ...
    Train_shared = ...
    Train_hypercube = ...
    Train_hypercube_pca = ...
    train_type = ...
    cp = ...
    assign_index = ...
    centroids = ...
    transposed_centroids = ...
    centroids_sq_lengths = ...
    def get_centroids(self, m, i):
        r"""return the centroids associated with subvector m"""
        ...
    
    def train(self, n, x):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    def set_derived_values(self):
        r"""compute derived values when d, M and nbits have been set"""
        ...
    
    def set_params(self, centroids, m):
        r"""Define the centroids for subquantizer m"""
        ...
    
    def compute_code(self, x, code):
        r"""Quantize one vector with the product quantizer"""
        ...
    
    def compute_codes(self, x, codes, n):
        r"""same as compute_code for several vectors"""
        ...
    
    def compute_codes_with_assign_index(self, x, codes, n):
        r"""
        speed up code assignment using assign_index
        (non-const because the index is changed)
        """
        ...
    
    def decode(self, *args):
        r"""decode a vector from a given code (or n vectors if third argument)"""
        ...
    
    def compute_code_from_distance_table(self, tab, code):
        r"""
        If we happen to have the distance tables precomputed, this is
        more efficient to compute the codes.
        """
        ...
    
    def compute_distance_table(self, x, dis_table):
        r"""
         Compute distance table for one vector.

        The distance table for x = [x_0 x_1 .. x_(M-1)] is a M * ksub
        matrix that contains

          dis_table (m, j) = || x_m - c_(m, j)||^2
          for m = 0..M-1 and j = 0 .. ksub - 1

        where c_(m, j) is the centroid no j of sub-quantizer m.

        :type x: float
        :param x:         input vector size d
        :type dis_table: float
        :param dis_table: output table, size M * ksub
        """
        ...
    
    def compute_inner_prod_table(self, x, dis_table):
        ...
    
    def compute_distance_tables(self, nx, x, dis_tables):
        r"""
         compute distance table for several vectors
        :type nx: int
        :param nx:        nb of input vectors
        :type x: float
        :param x:         input vector size nx * d
        :param dis_table: output table, size nx * M * ksub
        """
        ...
    
    def compute_inner_prod_tables(self, nx, x, dis_tables):
        ...
    
    def search(self, x, nx, codes, ncodes, res, init_finalize_heap=...):
        r"""
         perform a search (L2 distance)
        :type x: float
        :param x:        query vectors, size nx * d
        :type nx: int
        :param nx:       nb of queries
        :type codes: uint8_t
        :param codes:    database codes, size ncodes * code_size
        :type ncodes: int
        :param ncodes:   nb of nb vectors
        :type res: :py:class:`float_maxheap_array_t`
        :param res:      heap array to store results (nh == nx)
        :type init_finalize_heap: boolean, optional
        :param init_finalize_heap:  initialize heap (input) and sort (output)?
        """
        ...
    
    def search_ip(self, x, nx, codes, ncodes, res, init_finalize_heap=...):
        r"""same as search, but with inner product similarity"""
        ...
    
    sdc_table = ...
    def compute_sdc_table(self):
        ...
    
    def search_sdc(self, qcodes, nq, bcodes, ncodes, res, init_finalize_heap=...):
        ...
    
    def sync_transposed_centroids(self):
        r"""
        Sync transposed centroids with regular centroids. This call
        is needed if centroids were edited directly.
        """
        ...
    
    def clear_transposed_centroids(self):
        r"""Clear transposed centroids table so ones are no longer used."""
        ...
    
    __swig_destroy__ = ...


class PQEncoderGeneric:
    thisown = ...
    __repr__ = ...
    code = ...
    offset = ...
    nbits = ...
    reg = ...
    def __init__(self, code, nbits, offset=...) -> None:
        ...
    
    def encode(self, x):
        ...
    
    __swig_destroy__ = ...


class PQEncoder8:
    thisown = ...
    __repr__ = ...
    code = ...
    def __init__(self, code, nbits) -> None:
        ...
    
    def encode(self, x):
        ...
    
    __swig_destroy__ = ...


class PQEncoder16:
    thisown = ...
    __repr__ = ...
    code = ...
    def __init__(self, code, nbits) -> None:
        ...
    
    def encode(self, x):
        ...
    
    __swig_destroy__ = ...


class PQDecoderGeneric:
    thisown = ...
    __repr__ = ...
    code = ...
    offset = ...
    nbits = ...
    mask = ...
    reg = ...
    def __init__(self, code, nbits) -> None:
        ...
    
    def decode(self):
        ...
    
    __swig_destroy__ = ...


class PQDecoder8:
    thisown = ...
    __repr__ = ...
    nbits = ...
    code = ...
    def __init__(self, code, nbits) -> None:
        ...
    
    def decode(self):
        ...
    
    __swig_destroy__ = ...


class PQDecoder16:
    thisown = ...
    __repr__ = ...
    nbits = ...
    code = ...
    def __init__(self, code, nbits) -> None:
        ...
    
    def decode(self):
        ...
    
    __swig_destroy__ = ...


class AdditiveQuantizer(Quantizer):
    r"""
     Abstract structure for additive quantizers

    Different from the product quantizer in which the decoded vector is the
    concatenation of M sub-vectors, additive quantizers sum M sub-vectors
    to get the decoded vector.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    M = ...
    nbits = ...
    codebooks = ...
    codebook_offsets = ...
    tot_bits = ...
    norm_bits = ...
    total_codebook_size = ...
    only_8bit = ...
    verbose = ...
    is_trained = ...
    norm_tabs = ...
    qnorm = ...
    def compute_codebook_tables(self):
        ...
    
    centroid_norms = ...
    codebook_cross_products = ...
    max_mem_distances = ...
    def encode_norm(self, norm):
        r"""encode a norm into norm_bits bits"""
        ...
    
    def encode_qcint(self, x):
        r"""encode norm by non-uniform scalar quantization"""
        ...
    
    def decode_qcint(self, c):
        r"""decode norm by non-uniform scalar quantization"""
        ...
    
    ST_decompress = ...
    ST_LUT_nonorm = ...
    ST_norm_from_LUT = ...
    ST_norm_float = ...
    ST_norm_qint8 = ...
    ST_norm_qint4 = ...
    ST_norm_cqint8 = ...
    ST_norm_cqint4 = ...
    ST_norm_lsq2x4 = ...
    ST_norm_rq2x4 = ...
    def set_derived_values(self):
        r"""Train the norm quantizer"""
        ...
    
    def train_norm(self, n, norms):
        ...
    
    def compute_codes(self, x, codes, n):
        ...
    
    def compute_codes_add_centroids(self, x, codes, n, centroids=...):
        r"""
         Encode a set of vectors

        :type x: float
        :param x:      vectors to encode, size n * d
        :type codes: uint8_t
        :param codes:  output codes, size n * code_size
        :type centroids: float, optional
        :param centroids:  centroids to be added to x, size n * d
        """
        ...
    
    def pack_codes(self, n, codes, packed_codes, ld_codes=..., norms=..., centroids=...):
        r"""
         pack a series of code to bit-compact format

        :type codes: int
        :param codes:        codes to be packed, size n * code_size
        :type packed_codes: uint8_t
        :param packed_codes: output bit-compact codes
        :type ld_codes: int, optional
        :param ld_codes:     leading dimension of codes
        :type norms: float, optional
        :param norms:        norms of the vectors (size n). Will be computed if
                                needed but not provided
        :type centroids: float, optional
        :param centroids:    centroids to be added to x, size n * d
        """
        ...
    
    def decode(self, codes, x, n):
        r"""
         Decode a set of vectors

        :type codes: uint8_t
        :param codes:  codes to decode, size n * code_size
        :type x: float
        :param x:      output vectors, size n * d
        """
        ...
    
    def decode_unpacked(self, codes, x, n, ld_codes=...):
        r"""
         Decode a set of vectors in non-packed format

        :type codes: int
        :param codes:  codes to decode, size n * ld_codes
        :type x: float
        :param x:      output vectors, size n * d
        """
        ...
    
    search_type = ...
    norm_min = ...
    norm_max = ...
    def decode_64bit(self, n, x):
        r"""decoding function for a code in a 64-bit word"""
        ...
    
    def compute_LUT(self, n, xq, LUT, alpha=..., ld_lut=...):
        r"""
         Compute inner-product look-up tables. Used in the centroid search
        functions.

        :type xq: float
        :param xq:     query vector, size (n, d)
        :type LUT: float
        :param LUT:    look-up table, size (n, total_codebook_size)
        :type alpha: float, optional
        :param alpha:  compute alpha * inner-product
        :type ld_lut: int, optional
        :param ld_lut:  leading dimension of LUT
        """
        ...
    
    def knn_centroids_inner_product(self, n, xq, k, distances, labels):
        r"""exact IP search"""
        ...
    
    def compute_centroid_norms(self, norms):
        r"""
         For L2 search we need the L2 norms of the centroids

        :type norms: float
        :param norms:    output norms table, size total_codebook_size
        """
        ...
    
    def knn_centroids_L2(self, n, xq, k, distances, labels, centroid_norms):
        r"""Exact L2 search, with precomputed norms"""
        ...
    
    __swig_destroy__ = ...


def beam_search_encode_step(*args):
    r"""
     Encode a residual by sampling from a centroid table.

    This is a single encoding step the residual quantizer.
    It allows low-level access to the encoding function, exposed mainly for unit
    tests.

    :type n: int
    :param n:              number of vectors to handle
    :type residuals: float
    :param residuals:      vectors to encode, size (n, beam_size, d)
    :type cent: float
    :param cent:           centroids, size (K, d)
    :type beam_size: int
    :param beam_size:      input beam size
    :type m: int
    :param m:              size of the codes for the previous encoding steps
    :type codes: int
    :param codes:          code array for the previous steps of the beam (n,
        beam_size, m)
    :type new_beam_size: int
    :param new_beam_size:  output beam size (should be <= K * beam_size)
    :type new_codes: int
    :param new_codes:      output codes, size (n, new_beam_size, m + 1)
    :type new_residuals: float
    :param new_residuals:  output residuals, size (n, new_beam_size, d)
    :type new_distances: float
    :param new_distances:  output distances, size (n, new_beam_size)
    :type assign_index: :py:class:`Index`, optional
    :param assign_index:   if non-NULL, will be used to perform assignment
    """
    ...

def beam_search_encode_step_tab(*args):
    r"""
     Encode a set of vectors using their dot products with the codebooks

    :type K: int
    :param K:           number of vectors in the codebook
    :type n: int
    :param n:           nb of vectors to encode
    :type beam_size: int
    :param beam_size:   input beam size
    :type codebook_cross_norms: float
    :param codebook_cross_norms: inner product of this codebook with the m
                                    previously encoded codebooks
    :type codebook_offsets: int
    :param codebook_offsets:     offsets into codebook_cross_norms for each
                                    previous codebook
    :type query_cp: float
    :param query_cp:    dot products of query vectors with ???
    :type cent_norms_i: float
    :param cent_norms_i:  norms of centroids
    """
    ...

class RefineBeamMemoryPool:
    thisown = ...
    __repr__ = ...
    new_codes = ...
    new_residuals = ...
    residuals = ...
    codes = ...
    distances = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def refine_beam_mp(rq, n, beam_size, x, out_beam_size, out_codes, out_residuals, out_distances, pool):
    ...

class RefineBeamLUTMemoryPool:
    thisown = ...
    __repr__ = ...
    new_codes = ...
    new_distances = ...
    codes = ...
    distances = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def refine_beam_LUT_mp(rq, n, query_norms, query_cp, out_beam_size, out_codes, out_distances, pool):
    ...

class ComputeCodesAddCentroidsLUT0MemoryPool:
    thisown = ...
    __repr__ = ...
    codes = ...
    norms = ...
    distances = ...
    residuals = ...
    refine_beam_pool = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def compute_codes_add_centroids_mp_lut0(rq, x, codes_out, n, centroids, pool):
    ...

class ComputeCodesAddCentroidsLUT1MemoryPool:
    thisown = ...
    __repr__ = ...
    codes = ...
    distances = ...
    query_norms = ...
    query_cp = ...
    residuals = ...
    refine_beam_lut_pool = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def compute_codes_add_centroids_mp_lut1(rq, x, codes_out, n, centroids, pool):
    ...

class ResidualQuantizer(AdditiveQuantizer):
    r"""
     Residual quantizer with variable number of bits per sub-quantizer

    The residual centroids are stored in a big cumulative centroid table.
    The codes are represented either as a non-compact table of size (n, M) or
    as the compact output (n, code_size).
    """
    thisown = ...
    __repr__ = ...
    train_type = ...
    Train_default = ...
    Train_progressive_dim = ...
    Train_refine_codebook = ...
    niter_codebook_refine = ...
    Train_top_beam = ...
    Skip_codebook_tables = ...
    max_beam_size = ...
    use_beam_LUT = ...
    approx_topk_mode = ...
    cp = ...
    assign_index_factory = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        r"""Train the residual quantizer"""
        ...
    
    def initialize_from(self, other, skip_M=...):
        r"""Copy the M codebook levels from other, starting from skip_M"""
        ...
    
    def retrain_AQ_codebook(self, n, x):
        r"""
         Encode the vectors and compute codebook that minimizes the quantization
        error on these codes

        :type x: float
        :param x:      training vectors, size n * d
        :type n: int
        :param n:      nb of training vectors, n >= total_codebook_size
        :rtype: float
        :return: returns quantization error for the new codebook with old
            codes
        """
        ...
    
    def compute_codes_add_centroids(self, x, codes, n, centroids=...):
        r"""
         Encode a set of vectors

        :type x: float
        :param x:      vectors to encode, size n * d
        :type codes: uint8_t
        :param codes:  output codes, size n * code_size
        :type centroids: float, optional
        :param centroids:  centroids to be added to x, size n * d
        """
        ...
    
    def refine_beam(self, n, beam_size, residuals, new_beam_size, new_codes, new_residuals=..., new_distances=...):
        r"""
         lower-level encode function

        :type n: int
        :param n:              number of vectors to handle
        :type residuals: float
        :param residuals:      vectors to encode, size (n, beam_size, d)
        :type beam_size: int
        :param beam_size:      input beam size
        :type new_beam_size: int
        :param new_beam_size:  output beam size (should be <= K * beam_size)
        :type new_codes: int
        :param new_codes:      output codes, size (n, new_beam_size, m + 1)
        :type new_residuals: float, optional
        :param new_residuals:  output residuals, size (n, new_beam_size, d)
        :type new_distances: float, optional
        :param new_distances:  output distances, size (n, new_beam_size)
        """
        ...
    
    def refine_beam_LUT(self, n, query_norms, query_cp, new_beam_size, new_codes, new_distances=...):
        ...
    
    def memory_per_point(self, beam_size=...):
        r"""
         Beam search can consume a lot of memory. This function estimates the
        amount of mem used by refine_beam to adjust the batch size

        :type beam_size: int, optional
        :param beam_size:  if != -1, override the beam size
        """
        ...
    
    __swig_destroy__ = ...


class LocalSearchQuantizer(AdditiveQuantizer):
    r"""
     Implementation of LSQ/LSQ++ described in the following two papers:

    Revisiting additive quantization
    Julieta Martinez, et al. ECCV 2016

    LSQ++: Lower running time and higher recall in multi-codebook quantization
    Julieta Martinez, et al. ECCV 2018

    This implementation is mostly translated from the Julia implementations
    by Julieta Martinez:
    (https://github.com/una-dinosauria/local-search-quantization,
     https://github.com/una-dinosauria/Rayuela.jl)

    The trained codes are stored in `codebooks` which is called
    `centroids` in PQ and RQ.
    """
    thisown = ...
    __repr__ = ...
    K = ...
    train_iters = ...
    encode_ils_iters = ...
    train_ils_iters = ...
    icm_iters = ...
    p = ...
    lambd = ...
    chunk_size = ...
    random_seed = ...
    nperts = ...
    icm_encoder_factory = ...
    update_codebooks_with_double = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def train(self, n, x):
        ...
    
    def compute_codes_add_centroids(self, x, codes, n, centroids=...):
        r"""
         Encode a set of vectors

        :type x: float
        :param x:      vectors to encode, size n * d
        :type codes: uint8_t
        :param codes:  output codes, size n * code_size
        :type n: int
        :param n:      number of vectors
        :type centroids: float, optional
        :param centroids:  centroids to be added to x, size n * d
        """
        ...
    
    def update_codebooks(self, x, codes, n):
        r"""
         Update codebooks given encodings

        :type x: float
        :param x:      training vectors, size n * d
        :type codes: int
        :param codes:  encoded training vectors, size n * M
        :type n: int
        :param n:      number of vectors
        """
        ...
    
    def icm_encode(self, codes, x, n, ils_iters, gen):
        r"""
         Encode vectors given codebooks using iterative conditional mode (icm).

        :type codes: int
        :param codes:     output codes, size n * M
        :type x: float
        :param x:         vectors to encode, size n * d
        :type n: int
        :param n:         number of vectors
        :type ils_iters: int
        :param ils_iters: number of iterations of iterative local search
        """
        ...
    
    def icm_encode_impl(self, codes, x, unaries, gen, n, ils_iters, verbose):
        ...
    
    def icm_encode_step(self, codes, unaries, binaries, n, n_iters):
        ...
    
    def perturb_codes(self, codes, n, gen):
        r"""
         Add some perturbation to codes

        :type codes: int
        :param codes: codes to be perturbed, size n * M
        :type n: int
        :param n:     number of vectors
        """
        ...
    
    def perturb_codebooks(self, T, stddev, gen):
        r"""
         Add some perturbation to codebooks

        :type T: float
        :param T:         temperature of simulated annealing
        :type stddev: std::vector< float >
        :param stddev:    standard derivations of each dimension in training data
        """
        ...
    
    def compute_binary_terms(self, binaries):
        r"""
         Compute binary terms

        :type binaries: float
        :param binaries: binary terms, size M * M * K * K
        """
        ...
    
    def compute_unary_terms(self, x, unaries, n):
        r"""
         Compute unary terms

        :type n: int
        :param n:       number of vectors
        :type x: float
        :param x:       vectors to encode, size n * d
        :type unaries: float
        :param unaries: unary terms, size n * M * K
        """
        ...
    
    def evaluate(self, codes, x, n, objs=...):
        r"""
         Helper function to compute reconstruction error

        :type codes: int
        :param codes: encoded codes, size n * M
        :type x: float
        :param x:     vectors to encode, size n * d
        :type n: int
        :param n:     number of vectors
        :type objs: float, optional
        :param objs:  if it is not null, store reconstruction
                                error of each vector into it, size n
        """
        ...
    


class IcmEncoder:
    thisown = ...
    __repr__ = ...
    binaries = ...
    verbose = ...
    lsq = ...
    def __init__(self, lsq) -> None:
        ...
    
    __swig_destroy__ = ...
    def set_binary_term(self):
        ...
    
    def encode(self, codes, x, gen, n, ils_iters):
        r"""
         Encode vectors given codebooks

        :type codes: int
        :param codes:     output codes, size n * M
        :type x: float
        :param x:         vectors to encode, size n * d
        :type gen: std::mt19937
        :param gen:       random generator
        :type n: int
        :param n:         number of vectors
        :type ils_iters: int
        :param ils_iters: number of iterations of iterative local search
        """
        ...
    


class IcmEncoderFactory:
    thisown = ...
    __repr__ = ...
    def get(self, lsq):
        ...
    
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class LSQTimer:
    r"""
    A helper struct to count consuming time during training.
    It is NOT thread-safe.
    """
    thisown = ...
    __repr__ = ...
    t = ...
    def __init__(self) -> None:
        ...
    
    def get(self, name):
        ...
    
    def add(self, name, delta):
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class LSQTimerScope:
    thisown = ...
    __repr__ = ...
    t0 = ...
    timer = ...
    name = ...
    finished = ...
    def __init__(self, timer, name) -> None:
        ...
    
    def finish(self):
        ...
    
    __swig_destroy__ = ...


class ProductAdditiveQuantizer(AdditiveQuantizer):
    r"""
     Product Additive Quantizers

    The product additive quantizer is a variant of AQ and PQ.
    It first splits the vector space into multiple orthogonal sub-spaces
    just like PQ does. And then it quantizes each sub-space by an independent
    additive quantizer.
    """
    thisown = ...
    __repr__ = ...
    nsplits = ...
    quantizers = ...
    def __init__(self, *args) -> None:
        r"""
         Construct a product additive quantizer.

        The additive quantizers passed in will be cloned into the
        ProductAdditiveQuantizer object.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type aqs: std::vector< faiss::AdditiveQuantizer * >
        :param aqs:    sub-additive quantizers
        :type search_type: int, optional
        :param search_type:  AQ search type
        """
        ...
    
    __swig_destroy__ = ...
    def init(self, d, aqs, search_type):
        ...
    
    def subquantizer(self, m):
        r"""Train the product additive quantizer"""
        ...
    
    def train(self, n, x):
        ...
    
    def compute_codes_add_centroids(self, x, codes, n, centroids=...):
        r"""
         Encode a set of vectors

        :type x: float
        :param x:      vectors to encode, size n * d
        :type codes: uint8_t
        :param codes:  output codes, size n * code_size
        :type centroids: float, optional
        :param centroids:  centroids to be added to x, size n * d
        """
        ...
    
    def compute_unpacked_codes(self, x, codes, n, centroids=...):
        ...
    
    def decode_unpacked(self, codes, x, n, ld_codes=...):
        r"""
         Decode a set of vectors in non-packed format

        :type codes: int
        :param codes:  codes to decode, size n * ld_codes
        :type x: float
        :param x:      output vectors, size n * d
        """
        ...
    
    def decode(self, codes, x, n):
        r"""
         Decode a set of vectors

        :type codes: uint8_t
        :param codes:  codes to decode, size n * code_size
        :type x: float
        :param x:      output vectors, size n * d
        """
        ...
    
    def compute_LUT(self, n, xq, LUT, alpha=..., ld_lut=...):
        r"""
         Compute inner-product look-up tables. Used in the search functions.

        :type xq: float
        :param xq:     query vector, size (n, d)
        :type LUT: float
        :param LUT:    look-up table, size (n, total_codebook_size)
        :type alpha: float, optional
        :param alpha:  compute alpha * inner-product
        :type ld_lut: int, optional
        :param ld_lut:  leading dimension of LUT
        """
        ...
    


class ProductLocalSearchQuantizer(ProductAdditiveQuantizer):
    r"""Product Local Search Quantizer"""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
         Construct a product LSQ object.

        :type d: int
        :param d:   dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of sub-vectors we split a vector into
        :type Msub: int
        :param Msub:     number of codebooks of each LSQ
        :type nbits: int
        :param nbits:    bits for each step
        :type search_type: int, optional
        :param search_type:  AQ search type
        """
        ...
    
    __swig_destroy__ = ...


class ProductResidualQuantizer(ProductAdditiveQuantizer):
    r"""Product Residual Quantizer"""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
         Construct a product RQ object.

        :type d: int
        :param d:   dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of sub-vectors we split a vector into
        :type Msub: int
        :param Msub:     number of codebooks of each RQ
        :type nbits: int
        :param nbits:    bits for each step
        :type search_type: int, optional
        :param search_type:  AQ search type
        """
        ...
    
    __swig_destroy__ = ...


class CodePacker:
    r"""
    Packing consists in combining a fixed number of codes of constant size
    (code_size) into a block of data where they may (or may not) be interleaved
    for efficient consumption by distance computation kernels. This exists for
    the "fast_scan" indexes on CPU and for some GPU kernels.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    code_size = ...
    nvec = ...
    block_size = ...
    def pack_1(self, flat_code, offset, block):
        ...
    
    def unpack_1(self, block, offset, flat_code):
        ...
    
    def pack_all(self, flat_codes, block):
        ...
    
    def unpack_all(self, block, flat_codes):
        ...
    
    __swig_destroy__ = ...


class CodePackerFlat(CodePacker):
    r"""Trivial code packer where codes are stored one by one"""
    thisown = ...
    __repr__ = ...
    def __init__(self, code_size) -> None:
        ...
    
    def pack_1(self, flat_code, offset, block):
        ...
    
    def unpack_1(self, block, offset, flat_code):
        ...
    
    def pack_all(self, flat_codes, block):
        ...
    
    def unpack_all(self, block, flat_codes):
        ...
    
    __swig_destroy__ = ...


class VectorTransform:
    r"""Any transformation applied on a set of vectors"""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    d_in = ...
    d_out = ...
    is_trained = ...
    def train(self, n, x):
        r"""
         Perform training on a representative set of vectors. Does
        nothing by default.

        :type n: int
        :param n:      nb of training vectors
        :type x: float
        :param x:      training vecors, size n * d
        """
        ...
    
    def apply(self, n, x):
        r"""
         apply the transformation and return the result in an allocated pointer
        :type n: int
        :param n: number of vectors to transform
        :type x: float
        :param x: input vectors, size n * d_in
        :rtype: float
        :return: output vectors, size n * d_out
        """
        ...
    
    def apply_noalloc(self, n, x, xt):
        r"""
         apply the transformation and return the result in a provided matrix
        :type n: int
        :param n: number of vectors to transform
        :type x: float
        :param x: input vectors, size n * d_in
        :type xt: float
        :param xt: output vectors, size n * d_out
        """
        ...
    
    def reverse_transform(self, n, xt, x):
        r"""
        reverse transformation. May not be implemented or may return
        approximate result
        """
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class LinearTransform(VectorTransform):
    r"""
     Generic linear transformation, with bias term applied on output
    y = A * x + b
    """
    thisown = ...
    __repr__ = ...
    have_bias = ...
    is_orthonormal = ...
    A = ...
    b = ...
    def __init__(self, d_in=..., d_out=..., have_bias=...) -> None:
        r"""both d_in > d_out and d_out < d_in are supported"""
        ...
    
    def apply_noalloc(self, n, x, xt):
        r"""same as apply, but result is pre-allocated"""
        ...
    
    def transform_transpose(self, n, y, x):
        r"""
        compute x = A^T * (x - b)
        is reverse transform if A has orthonormal lines
        """
        ...
    
    def reverse_transform(self, n, xt, x):
        r"""works only if is_orthonormal"""
        ...
    
    def set_is_orthonormal(self):
        r"""compute A^T * A to set the is_orthonormal flag"""
        ...
    
    verbose = ...
    def print_if_verbose(self, name, mat, n, d):
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class RandomRotationMatrix(LinearTransform):
    r"""Randomly rotate a set of vectors"""
    thisown = ...
    __repr__ = ...
    def init(self, seed):
        r"""must be called before the transform is used"""
        ...
    
    def train(self, n, x):
        ...
    
    def __init__(self, *args) -> None:
        r"""both d_in > d_out and d_out < d_in are supported"""
        ...
    
    __swig_destroy__ = ...


class PCAMatrix(LinearTransform):
    r"""
    Applies a principal component analysis on a set of vectors,
    with optionally whitening and random rotation.
    """
    thisown = ...
    __repr__ = ...
    eigen_power = ...
    epsilon = ...
    random_rotation = ...
    max_points_per_d = ...
    balanced_bins = ...
    mean = ...
    eigenvalues = ...
    PCAMat = ...
    def __init__(self, d_in=..., d_out=..., eigen_power=..., random_rotation=...) -> None:
        ...
    
    def train(self, n, x):
        r"""
        train on n vectors. If n < d_in then the eigenvector matrix
        will be completed with 0s
        """
        ...
    
    def copy_from(self, other):
        r"""copy pre-trained PCA matrix"""
        ...
    
    def prepare_Ab(self):
        r"""called after mean, PCAMat and eigenvalues are computed"""
        ...
    
    __swig_destroy__ = ...


class ITQMatrix(LinearTransform):
    r"""
     ITQ implementation from

        Iterative quantization: A procrustean approach to learning binary codes
        for large-scale image retrieval,

    Yunchao Gong, Svetlana Lazebnik, Albert Gordo, Florent Perronnin,
    PAMI'12.
    """
    thisown = ...
    __repr__ = ...
    max_iter = ...
    seed = ...
    init_rotation = ...
    def __init__(self, d=...) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class ITQTransform(VectorTransform):
    r"""The full ITQ transform, including normalizations and PCA transformation"""
    thisown = ...
    __repr__ = ...
    mean = ...
    do_pca = ...
    itq = ...
    max_train_per_dim = ...
    pca_then_itq = ...
    def __init__(self, d_in=..., d_out=..., do_pca=...) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def apply_noalloc(self, n, x, xt):
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class OPQMatrix(LinearTransform):
    r"""
     Applies a rotation to align the dimensions with a PQ to minimize
     the reconstruction error. Can be used before an IndexPQ or an
     IndexIVFPQ. The method is the non-parametric version described in:

    "Optimized Product Quantization for Approximate Nearest Neighbor Search"
    Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun, CVPR'13
    """
    thisown = ...
    __repr__ = ...
    M = ...
    niter = ...
    niter_pq = ...
    niter_pq_0 = ...
    max_train_points = ...
    verbose = ...
    pq = ...
    def __init__(self, d=..., M=..., d2=...) -> None:
        r"""if d2 != -1, output vectors of this dimension"""
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class RemapDimensionsTransform(VectorTransform):
    r"""
     remap dimensions for intput vectors, possibly inserting 0s
    strictly speaking this is also a linear transform but we don't want
    to compute it with matrix multiplies
    """
    thisown = ...
    __repr__ = ...
    map = ...
    def apply_noalloc(self, n, x, xt):
        ...
    
    def reverse_transform(self, n, xt, x):
        r"""reverse transform correct only when the mapping is a permutation"""
        ...
    
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        remap input to output, skipping or inserting dimensions as needed
        if uniform: distribute dimensions uniformly
        otherwise just take the d_out first ones.

        |

        *Overload 2:*
        remap input to output, skipping or inserting dimensions as needed
        if uniform: distribute dimensions uniformly
        otherwise just take the d_out first ones.
        """
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class NormalizationTransform(VectorTransform):
    r"""per-vector normalization"""
    thisown = ...
    __repr__ = ...
    norm = ...
    def __init__(self, *args) -> None:
        ...
    
    def apply_noalloc(self, n, x, xt):
        ...
    
    def reverse_transform(self, n, xt, x):
        r"""Identity transform since norm is not revertible"""
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class CenteringTransform(VectorTransform):
    r"""Subtract the mean of each component from the vectors."""
    thisown = ...
    __repr__ = ...
    mean = ...
    def __init__(self, d=...) -> None:
        ...
    
    def train(self, n, x):
        r"""train on n vectors."""
        ...
    
    def apply_noalloc(self, n, x, xt):
        r"""subtract the mean"""
        ...
    
    def reverse_transform(self, n, xt, x):
        r"""add the mean"""
        ...
    
    def check_identical(self, other):
        ...
    
    __swig_destroy__ = ...


class SearchParametersPreTransform(SearchParameters):
    thisown = ...
    __repr__ = ...
    index_params = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexPreTransform(Index):
    r"""
    Index that applies a LinearTransform transform on vectors before
    handing them over to a sub-index
    """
    thisown = ...
    __repr__ = ...
    chain = ...
    index = ...
    own_fields = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        whether pointers are deleted in destructor

        |

        *Overload 2:*
         ltrans is the last transform before the index
        """
        ...
    
    def prepend_transform(self, ltrans):
        ...
    
    def train(self, n, x):
        ...
    
    def add(self, n, x):
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def reset(self):
        ...
    
    def remove_ids(self, sel):
        r"""removes IDs from the index. Not supported by all indexes."""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        ...
    
    def search_and_reconstruct(self, n, x, k, distances, labels, recons, params=...):
        ...
    
    def apply_chain(self, n, x):
        r"""
        apply the transforms in the chain. The returned float * may be
        equal to x, otherwise it should be deallocated.
        """
        ...
    
    def reverse_chain(self, n, xt, x):
        r"""
        Reverse the transforms in the chain. May not be implemented for
        all transforms in the chain or may return approximate results.
        """
        ...
    
    def get_distance_computer(self):
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    __swig_destroy__ = ...


class IndexRefineSearchParameters(SearchParameters):
    thisown = ...
    __repr__ = ...
    k_factor = ...
    base_index_params = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class IndexRefine(Index):
    r"""
    Index that queries in a base_index (a fast one) and refines the
    results with an exact search, hopefully improving the results.
    """
    thisown = ...
    __repr__ = ...
    base_index = ...
    refine_index = ...
    own_fields = ...
    own_refine_index = ...
    k_factor = ...
    def __init__(self, *args) -> None:
        r"""initialize from empty index"""
        ...
    
    def train(self, n, x):
        ...
    
    def add(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        r"""
        The sa_decode decodes from the index_refine, which is assumed to be more
        accurate
        """
        ...
    
    __swig_destroy__ = ...


class IndexRefineFlat(IndexRefine):
    r"""
     Version where the refinement index is an IndexFlat. It has one additional
    constructor that takes a table of elements to add to the flat refinement
    index
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    __swig_destroy__ = ...


class IndexLSH(IndexFlatCodes):
    r"""The sign of each vector component is put in a binary signature"""
    thisown = ...
    __repr__ = ...
    nbits = ...
    rotate_data = ...
    train_thresholds = ...
    rrot = ...
    thresholds = ...
    def apply_preprocess(self, n, x):
        r"""
         Preprocesses and resizes the input to the size required to
        binarize the data

        :type x: float
        :param x: input vectors, size n * d
        :rtype: float
        :return: output vectors, size n * bits. May be the same pointer
                    as x, otherwise it should be deleted by the caller
        """
        ...
    
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def transfer_thresholds(self, vt):
        r"""
        transfer the thresholds to a pre-processing stage (and unset
        train_thresholds)
        """
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    


class SimulatedAnnealingParameters:
    r"""parameters used for the simulated annealing method"""
    thisown = ...
    __repr__ = ...
    init_temperature = ...
    temperature_decay = ...
    n_iter = ...
    n_redo = ...
    seed = ...
    verbose = ...
    only_bit_flips = ...
    init_random = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class PermutationObjective:
    r"""abstract class for the loss function"""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    n = ...
    def compute_cost(self, perm):
        ...
    
    def cost_update(self, perm, iw, jw):
        ...
    
    __swig_destroy__ = ...


class ReproduceDistancesObjective(PermutationObjective):
    thisown = ...
    __repr__ = ...
    dis_weight_factor = ...
    @staticmethod
    def sqr(x):
        ...
    
    def dis_weight(self, x):
        ...
    
    source_dis = ...
    target_dis = ...
    weights = ...
    def get_source_dis(self, i, j):
        ...
    
    def compute_cost(self, perm):
        ...
    
    def cost_update(self, perm, iw, jw):
        ...
    
    def __init__(self, n, source_dis_in, target_dis_in, dis_weight_factor) -> None:
        ...
    
    @staticmethod
    def compute_mean_stdev(tab, n2, mean_out, stddev_out):
        ...
    
    def set_affine_target_dis(self, source_dis_in):
        ...
    
    __swig_destroy__ = ...


class SimulatedAnnealingOptimizer(SimulatedAnnealingParameters):
    r"""Simulated annealing optimization algorithm for permutations."""
    thisown = ...
    __repr__ = ...
    obj = ...
    n = ...
    logfile = ...
    def __init__(self, obj, p) -> None:
        r"""logs values of the cost function"""
        ...
    
    rnd = ...
    init_cost = ...
    def optimize(self, perm):
        ...
    
    def run_optimization(self, best_perm):
        ...
    
    __swig_destroy__ = ...


class PolysemousTraining(SimulatedAnnealingParameters):
    r"""optimizes the order of indices in a ProductQuantizer"""
    thisown = ...
    __repr__ = ...
    OT_None = ...
    OT_ReproduceDistances_affine = ...
    OT_Ranking_weighted_diff = ...
    optimization_type = ...
    ntrain_permutation = ...
    dis_weight_factor = ...
    max_memory = ...
    log_pattern = ...
    def __init__(self) -> None:
        ...
    
    def optimize_pq_for_hamming(self, pq, n, x):
        r"""
        reorder the centroids so that the Hamming distance becomes a
        good approximation of the SDC distance (called by train)
        """
        ...
    
    def optimize_ranking(self, pq, n, x):
        r"""called by optimize_pq_for_hamming"""
        ...
    
    def optimize_reproduce_distances(self, pq):
        r"""called by optimize_pq_for_hamming"""
        ...
    
    def memory_usage_per_thread(self, pq):
        r"""make sure we don't blow up the memory"""
        ...
    
    __swig_destroy__ = ...


class IndexPQ(IndexFlatCodes):
    r"""
     Index based on a product quantizer. Stored vectors are
    approximated by PQ codes.
    """
    thisown = ...
    __repr__ = ...
    pq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    do_polysemous_training = ...
    polysemous_training = ...
    ST_PQ = ...
    ST_HE = ...
    ST_generalized_HE = ...
    ST_SDC = ...
    ST_polysemous = ...
    ST_polysemous_generalize = ...
    search_type = ...
    encode_signs = ...
    polysemous_ht = ...
    def search_core_polysemous(self, n, x, k, distances, labels, polysemous_ht, generalized_hamming):
        ...
    
    def hamming_distance_histogram(self, n, x, nb, xb, dist_histogram):
        r"""
        prepare query for a polysemous search, but instead of
        computing the result, just get the histogram of Hamming
        distances. May be computed on a provided dataset if xb != NULL
        :type dist_histogram: int
        :param dist_histogram: (M * nbits + 1)
        """
        ...
    
    def hamming_distance_table(self, n, x, dis):
        r"""
         compute pairwise distances between queries and database

        :type n: int
        :param n:    nb of query vectors
        :type x: float
        :param x:    query vector, size n * d
        :type dis: int
        :param dis:  output distances, size n * ntotal
        """
        ...
    
    __swig_destroy__ = ...


class SearchParametersPQ(SearchParameters):
    r"""override search parameters from the class"""
    thisown = ...
    __repr__ = ...
    search_type = ...
    polysemous_ht = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexPQStats:
    r"""
    statistics are robust to internal threading, but not if
    IndexPQ::search is called by multiple threads
    """
    thisown = ...
    __repr__ = ...
    nq = ...
    ncode = ...
    n_hamming_pass = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class MultiIndexQuantizer(Index):
    r"""
    Quantizer where centroids are virtual: they are the Cartesian
    product of sub-centroids.
    """
    thisown = ...
    __repr__ = ...
    pq = ...
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def add(self, n, x):
        r"""add and reset will crash at runtime"""
        ...
    
    def reset(self):
        ...
    
    def __init__(self, *args) -> None:
        r"""
         number of bit per subvector index
        :type d: int
        :param d: dimension of the input vectors
        :type M: int
        :param M: number of subquantizers
        """
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    __swig_destroy__ = ...


class MultiIndexQuantizer2(MultiIndexQuantizer):
    r"""MultiIndexQuantizer where the PQ assignmnet is performed by sub-indexes"""
    thisown = ...
    __repr__ = ...
    assign_indexes = ...
    own_fields = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    __swig_destroy__ = ...


class IndexAdditiveQuantizer(IndexFlatCodes):
    r"""Abstract class for additive quantizers. The search functions are in common."""
    thisown = ...
    __repr__ = ...
    aq = ...
    def __init__(self, *args) -> None:
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    __swig_destroy__ = ...


class IndexResidualQuantizer(IndexAdditiveQuantizer):
    r"""
     Index based on a residual quantizer. Stored vectors are
    approximated by residual quantization codes.
    Can also be used as a codec
    """
    thisown = ...
    __repr__ = ...
    rq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class IndexLocalSearchQuantizer(IndexAdditiveQuantizer):
    thisown = ...
    __repr__ = ...
    lsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class IndexProductResidualQuantizer(IndexAdditiveQuantizer):
    r"""Index based on a product residual quantizer."""
    thisown = ...
    __repr__ = ...
    prq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of residual quantizers
        :type Msub: int
        :param Msub:      number of subquantizers per RQ
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type nsplits: int
        :param nsplits: number of residual quantizers
        :type Msub: int
        :param Msub: number of subquantizers per RQ
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class IndexProductLocalSearchQuantizer(IndexAdditiveQuantizer):
    r"""Index based on a product local search quantizer."""
    thisown = ...
    __repr__ = ...
    plsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of local search quantizers
        :type Msub: int
        :param Msub:     number of subquantizers per LSQ
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type nsplits: int
        :param nsplits: number of local search quantizers
        :type Msub: int
        :param Msub: number of subquantizers per LSQ
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class AdditiveCoarseQuantizer(Index):
    r"""
     A "virtual" index where the elements are the residual quantizer centroids.

    Intended for use as a coarse quantizer in an IndexIVF.
    """
    thisown = ...
    __repr__ = ...
    aq = ...
    def __init__(self, *args) -> None:
        ...
    
    centroid_norms = ...
    def add(self, n, x):
        r"""N/A"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def train(self, n, x):
        ...
    
    def reset(self):
        r"""N/A"""
        ...
    
    __swig_destroy__ = ...


class SearchParametersResidualCoarseQuantizer(SearchParameters):
    thisown = ...
    __repr__ = ...
    beam_factor = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class ResidualCoarseQuantizer(AdditiveCoarseQuantizer):
    r"""
     The ResidualCoarseQuantizer is a bit specialized compared to the
    default AdditiveCoarseQuantizer because it can use a beam search
    at search time (slow but may be useful for very large vocabularies)
    """
    thisown = ...
    __repr__ = ...
    rq = ...
    beam_factor = ...
    def set_beam_factor(self, new_beam_factor):
        r"""computes centroid norms if required"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def initialize_from(self, other):
        r"""
         Copy the M first codebook levels from other. Useful to crop a
        ResidualQuantizer to its first M quantizers.
        """
        ...
    
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class LocalSearchCoarseQuantizer(AdditiveCoarseQuantizer):
    thisown = ...
    __repr__ = ...
    lsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class InvertedListsIterator:
    r"""
    Definition of inverted lists + a few common classes that implement
    the interface.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    __swig_destroy__ = ...
    def is_available(self):
        ...
    
    def next(self):
        ...
    
    def get_id_and_codes(self):
        ...
    


class InvertedLists:
    r"""
     Table of inverted lists
    multithreading rules:
    - concurrent read accesses are allowed
    - concurrent update accesses are allowed
    - for resize and add_entries, only concurrent access to different lists
      are allowed
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    nlist = ...
    code_size = ...
    use_iterator = ...
    __swig_destroy__ = ...
    INVALID_CODE_SIZE = ...
    def list_size(self, list_no):
        r"""get the size of a list"""
        ...
    
    def get_codes(self, list_no):
        r"""
         get the codes for an inverted list
        must be released by release_codes

        :rtype: uint8_t
        :return: codes    size list_size * code_size
        """
        ...
    
    def get_ids(self, list_no):
        r"""
         get the ids for an inverted list
        must be released by release_ids

        :rtype: int
        :return: ids      size list_size
        """
        ...
    
    def release_codes(self, list_no, codes):
        r"""release codes returned by get_codes (default implementation is nop"""
        ...
    
    def release_ids(self, list_no, ids):
        r"""release ids returned by get_ids"""
        ...
    
    def get_single_id(self, list_no, offset):
        r"""
        :rtype: int
        :return: a single id in an inverted list
        """
        ...
    
    def get_single_code(self, list_no, offset):
        r"""
        :rtype: uint8_t
        :return: a single code in an inverted list
            (should be deallocated with release_codes)
        """
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        r"""
        prepare the following lists (default does nothing)
        a list can be -1 hence the signed long
        """
        ...
    
    def is_empty(self, list_no, inverted_list_context=...):
        r"""check if the list is empty"""
        ...
    
    def get_iterator(self, list_no, inverted_list_context=...):
        r"""get iterable for lists that use_iterator"""
        ...
    
    def add_entry(self, list_no, theid, code, inverted_list_context=...):
        r"""add one entry to an inverted list"""
        ...
    
    def add_entries(self, list_no, n_entry, ids, code):
        ...
    
    def update_entry(self, list_no, offset, id, code):
        ...
    
    def update_entries(self, list_no, offset, n_entry, ids, code):
        ...
    
    def resize(self, list_no, new_size):
        ...
    
    def reset(self):
        ...
    
    def merge_from(self, oivf, add_id):
        r"""move all entries from oivf (empty on output)"""
        ...
    
    SUBSET_TYPE_ID_RANGE = ...
    SUBSET_TYPE_ID_MOD = ...
    SUBSET_TYPE_ELEMENT_RANGE = ...
    SUBSET_TYPE_INVLIST_FRACTION = ...
    SUBSET_TYPE_INVLIST = ...
    def copy_subset_to(self, other, subset_type, a1, a2):
        r"""
         copy a subset of the entries index to the other index
        :rtype: int
        :return: number of entries copied
        """
        ...
    
    def imbalance_factor(self):
        r"""1= perfectly balanced, >1: imbalanced"""
        ...
    
    def print_stats(self):
        r"""display some stats about the inverted lists"""
        ...
    
    def compute_ntotal(self):
        r"""sum up list sizes"""
        ...
    


class ArrayInvertedLists(InvertedLists):
    r"""simple (default) implementation as an array of inverted lists"""
    thisown = ...
    __repr__ = ...
    codes = ...
    ids = ...
    def __init__(self, nlist, code_size) -> None:
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def add_entries(self, list_no, n_entry, ids, code):
        ...
    
    def update_entries(self, list_no, offset, n_entry, ids, code):
        ...
    
    def resize(self, list_no, new_size):
        ...
    
    def permute_invlists(self, map):
        r"""permute the inverted lists, map maps new_id to old_id"""
        ...
    
    def is_empty(self, list_no, inverted_list_context=...):
        ...
    
    __swig_destroy__ = ...


class ReadOnlyInvertedLists(InvertedLists):
    r"""invlists that fail for all write functions"""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def add_entries(self, list_no, n_entry, ids, code):
        ...
    
    def update_entries(self, list_no, offset, n_entry, ids, code):
        ...
    
    def resize(self, list_no, new_size):
        ...
    
    __swig_destroy__ = ...


class HStackInvertedLists(ReadOnlyInvertedLists):
    r"""Horizontal stack of inverted lists"""
    thisown = ...
    __repr__ = ...
    ils = ...
    def __init__(self, nil, ils) -> None:
        r"""build InvertedLists by concatenating nil of them"""
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    def release_codes(self, list_no, codes):
        ...
    
    def release_ids(self, list_no, ids):
        ...
    
    def get_single_id(self, list_no, offset):
        ...
    
    def get_single_code(self, list_no, offset):
        ...
    
    __swig_destroy__ = ...


class SliceInvertedLists(ReadOnlyInvertedLists):
    r"""vertical slice of indexes in another InvertedLists"""
    thisown = ...
    __repr__ = ...
    il = ...
    i0 = ...
    i1 = ...
    def __init__(self, il, i0, i1) -> None:
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def release_codes(self, list_no, codes):
        ...
    
    def release_ids(self, list_no, ids):
        ...
    
    def get_single_id(self, list_no, offset):
        ...
    
    def get_single_code(self, list_no, offset):
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    __swig_destroy__ = ...


class VStackInvertedLists(ReadOnlyInvertedLists):
    thisown = ...
    __repr__ = ...
    ils = ...
    cumsz = ...
    def __init__(self, nil, ils) -> None:
        r"""build InvertedLists by concatenating nil of them"""
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def release_codes(self, list_no, codes):
        ...
    
    def release_ids(self, list_no, ids):
        ...
    
    def get_single_id(self, list_no, offset):
        ...
    
    def get_single_code(self, list_no, offset):
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    __swig_destroy__ = ...


class MaskedInvertedLists(ReadOnlyInvertedLists):
    r"""
     use the first inverted lists if they are non-empty otherwise use the second

    This is useful if il1 has a few inverted lists that are too long,
    and that il0 has replacement lists for those, with empty lists for
    the others.
    """
    thisown = ...
    __repr__ = ...
    il0 = ...
    il1 = ...
    def __init__(self, il0, il1) -> None:
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def release_codes(self, list_no, codes):
        ...
    
    def release_ids(self, list_no, ids):
        ...
    
    def get_single_id(self, list_no, offset):
        ...
    
    def get_single_code(self, list_no, offset):
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    __swig_destroy__ = ...


class StopWordsInvertedLists(ReadOnlyInvertedLists):
    r"""
    if the inverted list in il is smaller than maxsize then return it,
    otherwise return an empty invlist
    """
    thisown = ...
    __repr__ = ...
    il0 = ...
    maxsize = ...
    def __init__(self, il, maxsize) -> None:
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def release_codes(self, list_no, codes):
        ...
    
    def release_ids(self, list_no, ids):
        ...
    
    def get_single_id(self, list_no, offset):
        ...
    
    def get_single_code(self, list_no, offset):
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    __swig_destroy__ = ...


class InvertedListsIOHook:
    r"""
     Callbacks to handle other types of InvertedList objects.

    The callbacks should be registered with add_callback before calling
    read_index or read_InvertedLists. The callbacks for
    OnDiskInvertedLists are registrered by default. The invlist type is
    identified by:

    - the key (a fourcc) at read time
    - the class name (as given by typeid.name) at write time
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    key = ...
    classname = ...
    def write(self, ils, f):
        r"""write the index to the IOWriter (including the fourcc)"""
        ...
    
    def read(self, f, io_flags):
        r"""called when the fourcc matches this class's fourcc"""
        ...
    
    def read_ArrayInvertedLists(self, f, io_flags, nlist, code_size, sizes):
        r"""
         read from a ArrayInvertedLists into this invertedlist type.
        For this to work, the callback has to be enabled and the io_flag has to
        be set to IO_FLAG_SKIP_IVF_DATA | (16 upper bits of the fourcc)

        (default implementation fails)
        """
        ...
    
    __swig_destroy__ = ...
    @staticmethod
    def add_callback(arg1):
        ...
    
    @staticmethod
    def print_callbacks():
        ...
    
    @staticmethod
    def lookup(h):
        ...
    
    @staticmethod
    def lookup_classname(classname):
        ...
    


class BlockInvertedLists(InvertedLists):
    r"""
     Inverted Lists that are organized by blocks.

    Different from the regular inverted lists, the codes are organized by blocks
    of size block_size bytes that reprsent a set of n_per_block. Therefore, code
    allocations are always rounded up to block_size bytes. The codes are also
    aligned on 32-byte boundaries for use with SIMD.

    To avoid misinterpretations, the code_size is set to (size_t)(-1), even if
    arguably the amount of memory consumed by code is block_size / n_per_block.

    The writing functions add_entries and update_entries operate on block-aligned
    data.
    """
    thisown = ...
    __repr__ = ...
    n_per_block = ...
    block_size = ...
    packer = ...
    codes = ...
    ids = ...
    def __init__(self, *args) -> None:
        ...
    
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def remove_ids(self, sel):
        r"""remove ids from the InvertedLists"""
        ...
    
    def add_entries(self, list_no, n_entry, ids, code):
        ...
    
    def update_entries(self, list_no, offset, n_entry, ids, code):
        r"""not implemented"""
        ...
    
    def resize(self, list_no, new_size):
        ...
    
    __swig_destroy__ = ...


def lo_build(list_id, offset):
    ...

def lo_listno(lo):
    ...

def lo_offset(lo):
    ...

class DirectMap:
    r"""Direct map: a way to map back from ids to inverted lists"""
    thisown = ...
    __repr__ = ...
    NoMap = ...
    Array = ...
    Hashtable = ...
    type = ...
    array = ...
    hashtable = ...
    def __init__(self) -> None:
        ...
    
    def set_type(self, new_type, invlists, ntotal):
        r"""set type and initialize"""
        ...
    
    def get(self, id):
        r"""get an entry"""
        ...
    
    def no(self):
        r"""for quick checks"""
        ...
    
    def check_can_add(self, ids):
        r"""
        update the direct_map

         throw if Array and ids is not NULL
        """
        ...
    
    def add_single_id(self, id, list_no, offset):
        r"""non thread-safe version"""
        ...
    
    def clear(self):
        r"""remove all entries"""
        ...
    
    def remove_ids(self, sel, invlists):
        r"""
        operations on inverted lists that require translation with a DirectMap

         remove ids from the InvertedLists, possibly using the direct map
        """
        ...
    
    def update_codes(self, invlists, n, ids, list_nos, codes):
        r"""update entries, using the direct map"""
        ...
    
    __swig_destroy__ = ...


class DirectMapAdd:
    r"""Thread-safe way of updating the direct_map"""
    thisown = ...
    __repr__ = ...
    direct_map = ...
    type = ...
    ntotal = ...
    n = ...
    xids = ...
    all_ofs = ...
    def __init__(self, direct_map, n, xids) -> None:
        ...
    
    def add(self, i, list_no, offset):
        r"""add vector i (with id xids[i]) at list_no and offset"""
        ...
    
    __swig_destroy__ = ...


class Level1Quantizer:
    r"""
     Encapsulates a quantizer object for the IndexIVF

    The class isolates the fields that are independent of the storage
    of the lists (especially training)
    """
    thisown = ...
    __repr__ = ...
    quantizer = ...
    nlist = ...
    quantizer_trains_alone = ...
    own_fields = ...
    cp = ...
    clustering_index = ...
    def train_q1(self, n, x, verbose, metric_type):
        r"""Trains the quantizer and calls train_residual to train sub-quantizers"""
        ...
    
    def coarse_code_size(self):
        r"""compute the number of bytes required to store list ids"""
        ...
    
    def encode_listno(self, list_no, code):
        ...
    
    def decode_listno(self, code):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class SearchParametersIVF(SearchParameters):
    thisown = ...
    __repr__ = ...
    nprobe = ...
    max_codes = ...
    quantizer_params = ...
    inverted_list_context = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class IndexIVFInterface(Level1Quantizer):
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    nprobe = ...
    max_codes = ...
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=..., stats=...):
        r"""
         search a set of vectors, that are pre-quantized by the IVF
         quantizer. Fill in the corresponding heaps with the query
         results. The default implementation uses InvertedListScanners
         to do the search.

        :type n: int
        :param n:      nb of vectors to query
        :type x: float
        :param x:      query vectors, size nx * d
        :type assign: int
        :param assign: coarse quantization indices, size nx * nprobe
        :type centroid_dis: float
        :param centroid_dis:
                          distances to coarse centroids, size nx * nprobe
        :param distance:
                          output distances, size n * k
        :type labels: int
        :param labels: output labels, size n * k
        :type store_pairs: boolean
        :param store_pairs: store inv list index + inv list offset
                                instead in upper/lower 32 bit of result,
                                instead of ids (used for reranking).
        :type params: :py:class:`IVFSearchParameters`, optional
        :param params: used to override the object's search parameters
        :type stats: :py:class:`IndexIVFStats`, optional
        :param stats:  search stats to be updated (can be null)
        """
        ...
    
    def range_search_preassigned(self, nx, x, radius, keys, coarse_dis, result, store_pairs=..., params=..., stats=...):
        r"""
         Range search a set of vectors, that are pre-quantized by the IVF
         quantizer. Fill in the RangeSearchResults results. The default
        implementation uses InvertedListScanners to do the search.

        :param n:      nb of vectors to query
        :type x: float
        :param x:      query vectors, size nx * d
        :param assign: coarse quantization indices, size nx * nprobe
        :param centroid_dis:
                          distances to coarse centroids, size nx * nprobe
        :type result: :py:class:`RangeSearchResult`
        :param result: Output results
        :type store_pairs: boolean, optional
        :param store_pairs: store inv list index + inv list offset
                                instead in upper/lower 32 bit of result,
                                instead of ids (used for reranking).
        :type params: :py:class:`IVFSearchParameters`, optional
        :param params: used to override the object's search parameters
        :type stats: :py:class:`IndexIVFStats`, optional
        :param stats:  search stats to be updated (can be null)
        """
        ...
    
    __swig_destroy__ = ...


class IndexIVF(Index, IndexIVFInterface):
    r"""
     Index based on a inverted file (IVF)

    In the inverted file, the quantizer (an Index instance) provides a
    quantization index for each vector to be added. The quantization
    index maps to a list (aka inverted list or posting list), where the
    id of the vector is stored.

    The inverted list object is required only after trainng. If none is
    set externally, an ArrayInvertedLists is used automatically.

    At search time, the vector to be searched is also quantized, and
    only the list corresponding to the quantization index is
    searched. This speeds up the search by making it
    non-exhaustive. This can be relaxed using multi-probe search: a few
    (nprobe) quantization indices are selected and several inverted
    lists are visited.

    Sub-classes implement a post-filtering of the index that refines
    the distance estimation from the query to databse vectors.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    invlists = ...
    own_invlists = ...
    code_size = ...
    parallel_mode = ...
    PARALLEL_MODE_NO_HEAP_INIT = ...
    direct_map = ...
    by_residual = ...
    def reset(self):
        ...
    
    def train(self, n, x):
        r"""Trains the quantizer and calls train_encoder to train sub-quantizers"""
        ...
    
    def add(self, n, x):
        r"""Calls add_with_ids with NULL ids"""
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""default implementation that calls encode_vectors"""
        ...
    
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        r"""
         Implementation of vector addition where the vector assignments are
        predefined. The default implementation hands over the code extraction to
        encode_vectors.

        :type precomputed_idx: int
        :param precomputed_idx:    quantization indices for the input vectors
            (size n)
        """
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listno=...):
        r"""
         Encodes a set of vectors as they would appear in the inverted lists

        :type list_nos: int
        :param list_nos:   inverted list ids as returned by the
                              quantizer (size n). -1s are ignored.
        :type codes: uint8_t
        :param codes:      output codes, size n * code_size
        :type include_listno: boolean, optional
        :param include_listno:
                              include the list ids in the code (in this case add
                              ceil(log8(nlist)) to the code size)
        """
        ...
    
    def add_sa_codes(self, n, codes, xids):
        r"""
         Add vectors that are computed with the standalone codec

        :type codes: uint8_t
        :param codes:  codes to add size n * sa_code_size()
        :type xids: int
        :param xids:   corresponding ids, size n
        """
        ...
    
    def train_encoder(self, n, x, assign):
        r"""
         Train the encoder for the vectors.

        If by_residual then it is called with residuals and corresponding assign
        array, otherwise x is the raw training vectors and assign=nullptr
        """
        ...
    
    def train_encoder_num_vectors(self):
        r"""
        can be redefined by subclasses to indicate how many training vectors
        they need
        """
        ...
    
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=..., stats=...):
        ...
    
    def range_search_preassigned(self, nx, x, radius, keys, coarse_dis, result, store_pairs=..., params=..., stats=...):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""assign the vectors, then call search_preassign"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs=..., sel=..., params=...):
        r"""
         Get a scanner for this index (store_pairs means ignore labels)

        The default search implementation uses this to compute the distances.
        Use sel instead of params->sel, because sel is initialized with
        params->sel, but may get overridden by IndexIVF's internal logic.
        """
        ...
    
    def reconstruct(self, key, recons):
        r"""reconstruct a vector. Works only if maintain_direct_map is set to 1 or 2"""
        ...
    
    def update_vectors(self, nv, idx, v):
        r"""
         Update a subset of vectors.

        The index must have a direct_map

        :type nv: int
        :param nv:     nb of vectors to update
        :type idx: int
        :param idx:    vector indices to update, size nv
        :type v: float
        :param v:      vectors of new values, size nv*d
        """
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        r"""
         Reconstruct a subset of the indexed vectors.

        Overrides default implementation to bypass reconstruct() which requires
        direct_map to be maintained.

        :type i0: int
        :param i0:     first vector to reconstruct
        :type ni: int
        :param ni:     nb of vectors to reconstruct
        :type recons: float
        :param recons: output array of reconstructed vectors, size ni * d
        """
        ...
    
    def search_and_reconstruct(self, n, x, k, distances, labels, recons, params=...):
        r"""
         Similar to search, but also reconstructs the stored vectors (or an
        approximation in the case of lossy coding) for the search results.

        Overrides default implementation to avoid having to maintain direct_map
        and instead fetch the code offsets through the `store_pairs` flag in
        search_preassigned().

        :type recons: float
        :param recons:      reconstructed vectors size (n, k, d)
        """
        ...
    
    def search_and_return_codes(self, n, x, k, distances, labels, recons, include_listno=..., params=...):
        r"""
         Similar to search, but also returns the codes corresponding to the
        stored vectors for the search results.

        :param codes:      codes (n, k, code_size)
        :type include_listno: boolean, optional
        :param include_listno:
                              include the list ids in the code (in this case add
                              ceil(log8(nlist)) to the code size)
        """
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        r"""
         Reconstruct a vector given the location in terms of (inv list index +
        inv list offset) instead of the id.

        Useful for reconstructing when the direct_map is not maintained and
        the inv list offset is computed by search_preassigned() with
        `store_pairs` set.
        """
        ...
    
    def remove_ids(self, sel):
        r"""Dataset manipulation functions"""
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def merge_from(self, otherIndex, add_id):
        ...
    
    def get_CodePacker(self):
        ...
    
    def copy_subset_to(self, other, subset_type, a1, a2):
        r"""
         copy a subset of the entries index to the other index
        see Invlists::copy_subset_to for the meaning of subset_type
        """
        ...
    
    __swig_destroy__ = ...
    def get_list_size(self, list_no):
        ...
    
    def check_ids_sorted(self):
        r"""are the ids sorted?"""
        ...
    
    def make_direct_map(self, new_maintain_direct_map=...):
        r"""
         initialize a direct map

        :type new_maintain_direct_map: boolean, optional
        :param new_maintain_direct_map:    if true, create a direct map,
                                              else clear it
        """
        ...
    
    def set_direct_map_type(self, type):
        ...
    
    def replace_invlists(self, il, own=...):
        r"""replace the inverted lists, old one is deallocated if own_invlists"""
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        r"""
         encode a set of vectors
        sa_encode will call encode_vectors with include_listno=true
        :type n: int
        :param n:      nb of vectors to encode
        :type x: float
        :param x:      the vectors to encode
        :type bytes: uint8_t
        :param bytes:  output array for the codes
        :rtype: void
        :return: nb of bytes written to codes
        """
        ...
    


class InvertedListScanner:
    r"""
     Object that handles a query. The inverted lists to scan are
    provided externally. The object has a lot of state, but
    distance_to_code and scan_codes can be called in multiple
    threads
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    list_no = ...
    keep_max = ...
    store_pairs = ...
    sel = ...
    code_size = ...
    def set_query(self, query_vector):
        r"""from now on we handle this query."""
        ...
    
    def set_list(self, list_no, coarse_dis):
        r"""following codes come from this inverted list"""
        ...
    
    def distance_to_code(self, code):
        r"""compute a single query-to-code distance"""
        ...
    
    def scan_codes(self, n, codes, ids, distances, labels, k):
        r"""
         scan a set of codes, compute distances to current query and
        update heap of results if necessary. Default implementation
        calls distance_to_code.

        :type n: int
        :param n:      number of codes to scan
        :type codes: uint8_t
        :param codes:  codes to scan (n * code_size)
        :type ids: int
        :param ids:        corresponding ids (ignored if store_pairs)
        :type distances: float
        :param distances:  heap distances (size k)
        :type labels: int
        :param labels:     heap labels (size k)
        :type k: int
        :param k:          heap size
        :rtype: int
        :return: number of heap updates performed
        """
        ...
    
    def iterate_codes(self, iterator, distances, labels, k, list_size):
        ...
    
    def scan_codes_range(self, n, codes, ids, radius, result):
        r"""
         scan a set of codes, compute distances to current query and
        update results if distances are below radius

        (default implementation fails)
        """
        ...
    
    def iterate_codes_range(self, iterator, radius, result, list_size):
        ...
    
    __swig_destroy__ = ...


class IndexIVFStats:
    thisown = ...
    __repr__ = ...
    nq = ...
    nlist = ...
    ndis = ...
    nheap_updates = ...
    quantization_time = ...
    search_time = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    def add(self, other):
        ...
    
    __swig_destroy__ = ...


def check_compatible_for_merge(index1, index2):
    r"""
     check if two indexes have the same parameters and are trained in
    the same way, otherwise throw.
    """
    ...

def extract_index_ivf(*args):
    r"""
     get an IndexIVF from an index. The index may be an IndexIVF or
    some wrapper class that encloses an IndexIVF

    throws an exception if this is not the case.
    """
    ...

def try_extract_index_ivf(*args):
    r"""same as above but returns nullptr instead of throwing on failure"""
    ...

def merge_into(index0, index1, shift_ids):
    r"""
     Merge index1 into index0. Works on IndexIVF's and IndexIVF's
     embedded in a IndexPreTransform. On output, the index1 is empty.

    :type shift_ids: boolean
    :param shift_ids:: translate the ids from index1 to index0->prev_ntotal
    """
    ...

def search_centroid(index, x, n, centroid_ids):
    ...

def search_and_return_centroids(index, n, xin, k, distances, labels, query_centroid_ids, result_centroid_ids):
    ...

class SlidingIndexWindow:
    r"""
     A set of IndexIVFs concatenated together in a FIFO fashion.
    at each "step", the oldest index slice is removed and a new index is added.
    """
    thisown = ...
    __repr__ = ...
    index = ...
    ils = ...
    n_slice = ...
    nlist = ...
    sizes = ...
    def __init__(self, index) -> None:
        r"""index should be initially empty and trained"""
        ...
    
    def step(self, sub_index, remove_oldest):
        r"""
         Add one index to the current index and remove the oldest one.

        :type sub_index: :py:class:`Index`
        :param sub_index:        slice to swap in (can be NULL)
        :type remove_oldest: boolean
        :param remove_oldest:    if true, remove the oldest slices
        """
        ...
    
    __swig_destroy__ = ...


def get_invlist_range(index, i0, i1):
    r"""Get a subset of inverted lists [i0, i1)"""
    ...

def set_invlist_range(index, i0, i1, src):
    r"""Set a subset of inverted lists"""
    ...

def search_with_parameters(index, n, x, k, distances, labels, params, nb_dis=..., ms_per_stage=...):
    r"""
     search an IndexIVF, possibly embedded in an IndexPreTransform with
    given parameters. This is a way to set the nprobe and get
    statdistics in a thread-safe way.

    Optionally returns (if non-nullptr):
    - nb_dis: number of distances computed
    - ms_per_stage: [0]: preprocessing time
                    [1]: coarse quantization,
                    [2]: list scanning
    """
    ...

def range_search_with_parameters(index, n, x, radius, result, params, nb_dis=..., ms_per_stage=...):
    r"""same as search_with_parameters but for range search"""
    ...

def ivf_residual_from_quantizer(arg1, nlevel):
    r"""
     Build an IndexIVFResidualQuantizer from an ResidualQuantizer, using the
    nlevel first components as coarse quantizer and the rest as codes in invlists
    """
    ...

def ivf_residual_add_from_flat_codes(ivfrq, ncode, codes, code_size=...):
    r"""
     add from codes. NB that the norm component is not used, so the code_size can
    be provided.

    :type ivfrq: :py:class:`IndexIVFResidualQuantizer`
    :param ivfrq:      index to populate with the codes
    :type codes: uint8_t
    :param codes:      codes to add, size (ncode, code_size)
    :type code_size: int, optional
    :param code_size:  override the ivfrq's code_size, useful if the norm encoding
                          is different
    """
    ...

class ShardingFunction:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def __call__(self, i, shard_count):
        ...
    
    __swig_destroy__ = ...


class DefaultShardingFunction(ShardingFunction):
    thisown = ...
    __repr__ = ...
    def __call__(self, i, shard_count):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def shard_ivf_index_centroids(*args):
    r"""
    Shards an IVF index centroids by the given sharding function, and writes
    the index to the path given by filename_generator. The centroids must already
    be added to the index quantizer.

    :type index: :py:class:`IndexIVF`
    :param index:             The IVF index containing centroids to shard.
    :type shard_count: int, optional
    :param shard_count:       Number of shards.
    :type filename_template: string, optional
    :param filename_template: Template for shard filenames.
    :type sharding_function: :py:class:`ShardingFunction`, optional
    :param sharding_function: The function to shard by. The default is ith vector
                                 mod shard_count.
    :type generate_ids: boolean, optional
    :param generate_ids:      Generates ids using IndexIDMap2. If true, ids will
                                 match the default ids in the unsharded index.
    :rtype: void
    :return: The number of shards written.
    """
    ...

def shard_binary_ivf_index_centroids(*args):
    ...

class ScalarQuantizer(Quantizer):
    r"""
    The uniform quantizer has a range [vmin, vmax]. The range can be
    the same for all dimensions (uniform) or specific per dimension
    (default).
    """
    thisown = ...
    __repr__ = ...
    QT_8bit = ...
    QT_4bit = ...
    QT_8bit_uniform = ...
    QT_4bit_uniform = ...
    QT_fp16 = ...
    QT_8bit_direct = ...
    QT_6bit = ...
    QT_bf16 = ...
    QT_8bit_direct_signed = ...
    qtype = ...
    RS_minmax = ...
    RS_meanstd = ...
    RS_quantiles = ...
    RS_optim = ...
    rangestat = ...
    rangestat_arg = ...
    bits = ...
    trained = ...
    def __init__(self, *args) -> None:
        ...
    
    def set_derived_sizes(self):
        r"""updates internal values based on qtype and d"""
        ...
    
    def train(self, n, x):
        ...
    
    def compute_codes(self, x, codes, n):
        r"""
         Encode a set of vectors

        :type x: float
        :param x:      vectors to encode, size n * d
        :type codes: uint8_t
        :param codes:  output codes, size n * code_size
        """
        ...
    
    def decode(self, code, x, n):
        r"""
         Decode a set of vectors

        :param codes:  codes to decode, size n * code_size
        :type x: float
        :param x:      output vectors, size n * d
        """
        ...
    
    def select_quantizer(self):
        ...
    
    def get_distance_computer(self, *args):
        ...
    
    def select_InvertedListScanner(self, mt, quantizer, store_pairs, sel, by_residual=...):
        ...
    
    __swig_destroy__ = ...


class IndexScalarQuantizer(IndexFlatCodes):
    r"""Flat index built on a scalar quantizer."""
    thisown = ...
    __repr__ = ...
    sq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :param M:      number of subquantizers
        :param nbits:  number of bit per subvector index
        """
        ...
    
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class IndexIVFScalarQuantizer(IndexIVF):
    r"""
     An IVF implementation where the components of the residuals are
    encoded with a scalar quantizer. All distance computations
    are asymmetric, so the encoded vectors are decoded and approximate
    distances are computed.
    """
    thisown = ...
    __repr__ = ...
    sq = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class IndexIVFSpectralHash(IndexIVF):
    r"""
     Inverted list that stores binary codes of size nbit. Before the
    binary conversion, the dimension of the vectors is transformed from
    dim d into dim nbit by vt (a random rotation by default).

    Each coordinate is subtracted from a value determined by
    threshold_type, and split into intervals of size period. Half of
    the interval is a 0 bit, the other half a 1.
    """
    thisown = ...
    __repr__ = ...
    vt = ...
    own_fields = ...
    nbit = ...
    period = ...
    Thresh_global = ...
    Thresh_centroid = ...
    Thresh_centroid_half = ...
    Thresh_median = ...
    threshold_type = ...
    trained = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def replace_vt(self, *args):
        r"""
        *Overload 1:*
         replace the vector transform for an empty (and possibly untrained) index

        |

        *Overload 2:*
         convenience function to get the VT from an index constucted by an
        index_factory (should end in "LSH")

        |

        *Overload 3:*
         convenience function to get the VT from an index constucted by an
        index_factory (should end in "LSH")
        """
        ...
    
    __swig_destroy__ = ...


class IndexIVFAdditiveQuantizer(IndexIVF):
    r"""
    Abstract class for IVF additive quantizers.
    The search functions are in common.
    """
    thisown = ...
    __repr__ = ...
    aq = ...
    use_precomputed_table = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def sa_decode(self, n, codes, x):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    __swig_destroy__ = ...


class IndexIVFResidualQuantizer(IndexIVFAdditiveQuantizer):
    r"""
     IndexIVF based on a residual quantizer. Stored vectors are
    approximated by residual quantization codes.
    """
    thisown = ...
    __repr__ = ...
    rq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :param M:      number of subquantizers
        :type nbits: std::vector< size_t >
        :param nbits:  number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexIVFLocalSearchQuantizer(IndexIVFAdditiveQuantizer):
    r"""
     IndexIVF based on a residual quantizer. Stored vectors are
    approximated by residual quantization codes.
    """
    thisown = ...
    __repr__ = ...
    lsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexIVFProductResidualQuantizer(IndexIVFAdditiveQuantizer):
    r"""
     IndexIVF based on a product residual quantizer. Stored vectors are
    approximated by product residual quantization codes.
    """
    thisown = ...
    __repr__ = ...
    prq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of residual quantizers
        :type Msub: int
        :param Msub:   number of subquantizers per RQ
        :type nbits: int
        :param nbits:  number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexIVFProductLocalSearchQuantizer(IndexIVFAdditiveQuantizer):
    r"""
     IndexIVF based on a product local search quantizer. Stored vectors are
    approximated by product local search quantization codes.
    """
    thisown = ...
    __repr__ = ...
    plsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of local search quantizers
        :type Msub: int
        :param Msub:   number of subquantizers per LSQ
        :type nbits: int
        :param nbits:  number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class SearchParametersHNSW(SearchParameters):
    thisown = ...
    __repr__ = ...
    efSearch = ...
    check_relative_distance = ...
    bounded_queue = ...
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class HNSW:
    thisown = ...
    __repr__ = ...
    assign_probas = ...
    cum_nneighbor_per_level = ...
    levels = ...
    offsets = ...
    neighbors = ...
    entry_point = ...
    rng = ...
    max_level = ...
    efConstruction = ...
    efSearch = ...
    check_relative_distance = ...
    search_bounded_queue = ...
    def set_default_probas(self, M, levelMult):
        r"""
        initialize the assign_probas and cum_nneighbor_per_level to
        have 2*M links on level 0 and M links on levels > 0
        """
        ...
    
    def set_nb_neighbors(self, level_no, n):
        r"""set nb of neighbors for this level (before adding anything)"""
        ...
    
    def nb_neighbors(self, layer_no):
        r"""nb of neighbors for this level"""
        ...
    
    def cum_nb_neighbors(self, layer_no):
        r"""cumumlative nb up to (and excluding) this level"""
        ...
    
    def neighbor_range(self, no, layer_no, begin, end):
        r"""range of entries in the neighbors table of vertex no at layer_no"""
        ...
    
    def __init__(self, M=...) -> None:
        r"""only mandatory parameter: nb of neighbors"""
        ...
    
    def random_level(self):
        r"""pick a random level for a new point"""
        ...
    
    def fill_with_random_links(self, n):
        r"""add n random levels to table (for debugging...)"""
        ...
    
    def add_links_starting_from(self, ptdis, pt_id, nearest, d_nearest, level, locks, vt, keep_max_size_level0=...):
        ...
    
    def add_with_locks(self, ptdis, pt_level, pt_id, locks, vt, keep_max_size_level0=...):
        r"""
         add point pt_id on all levels <= pt_level and build the link
        structure for them.
        """
        ...
    
    def search(self, qdis, res, vt, params=...):
        r"""search interface for 1 point, single thread"""
        ...
    
    def search_level_0(self, qdis, res, nprobe, nearest_i, nearest_d, search_type, search_stats, vt, params=...):
        r"""search only in level 0 from a given vertex"""
        ...
    
    def reset(self):
        ...
    
    def clear_neighbor_tables(self, level):
        ...
    
    def print_neighbor_stats(self, level):
        ...
    
    def prepare_level_tab(self, n, preset_levels=...):
        ...
    
    @staticmethod
    def shrink_neighbor_list(qdis, input, output, max_size, keep_max_size_level0=...):
        ...
    
    def permute_entries(self, map):
        ...
    
    __swig_destroy__ = ...


class HNSWStats:
    thisown = ...
    __repr__ = ...
    n1 = ...
    n2 = ...
    ndis = ...
    nhops = ...
    def reset(self):
        r"""number of hops aka number of edges traversed"""
        ...
    
    def combine(self, other):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def search_from_candidates(hnsw, qdis, res, candidates, vt, stats, level, nres_in=..., params=...):
    ...

def greedy_update_nearest(hnsw, qdis, level, nearest, d_nearest):
    ...

def search_from_candidate_unbounded(hnsw, node, qdis, ef, vt, stats):
    ...

def search_neighbors_to_add(hnsw, qdis, results, entry_point, d_entry_point, level, vt, reference_version=...):
    ...

class IndexHNSW(Index):
    r"""
     The HNSW index is a normal random-access index with a HNSW
    link structure built on top
    """
    thisown = ...
    __repr__ = ...
    hnsw = ...
    own_fields = ...
    storage = ...
    init_level0 = ...
    keep_max_size_level0 = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def add(self, n, x):
        ...
    
    def train(self, n, x):
        r"""Trains the storage if needed"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reset(self):
        ...
    
    def shrink_level_0_neighbors(self, size):
        ...
    
    def search_level_0(self, n, x, k, nearest, nearest_d, distances, labels, nprobe=..., search_type=..., params=...):
        r"""
         Perform search only on level 0, given the starting points for
        each vertex.

        :type search_type: int, optional
        :param search_type: 1:perform one search per nprobe, 2: enqueue
                               all entry points
        """
        ...
    
    def init_level_0_from_knngraph(self, k, D, I):
        r"""alternative graph building"""
        ...
    
    def init_level_0_from_entry_points(self, npt, points, nearests):
        r"""alternative graph building"""
        ...
    
    def reorder_links(self):
        ...
    
    def link_singletons(self):
        ...
    
    def permute_entries(self, perm):
        ...
    
    def get_distance_computer(self):
        ...
    


class IndexHNSWFlat(IndexHNSW):
    r"""
    Flat index topped with with a HNSW structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexHNSWPQ(IndexHNSW):
    r"""
    PQ index topped with with a HNSW structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class IndexHNSWSQ(IndexHNSW):
    r"""
    SQ index topped with a HNSW structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexHNSW2Level(IndexHNSW):
    r"""2-level code structure with fast random access"""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def flip_to_ivf(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    __swig_destroy__ = ...


class IndexHNSWCagra(IndexHNSW):
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    base_level_only = ...
    num_base_level_search_entrypoints = ...
    def add(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    __swig_destroy__ = ...


def smawk(nrows, ncols, x, argmins):
    r"""
     SMAWK algorithm. Find the row minima of a monotone matrix.

    Expose this for testing.

    :type nrows: int
    :param nrows:    number of rows
    :type ncols: int
    :param ncols:    number of columns
    :type x: float
    :param x:        input matrix, size (nrows, ncols)
    :type argmins: int
    :param argmins:  argmin of each row
    """
    ...

def kmeans1d(x, n, nclusters, centroids):
    r"""
     Exact 1D K-Means by dynamic programming

    From  "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D"
    Allan Grnlund, Kasper Green Larsen, Alexander Mathiasen, Jesper Sindahl
    Nielsen, Stefan Schneider, Mingzhou Song, ArXiV'17

    Section 2.2

    https://arxiv.org/abs/1701.07204

    :type x: float
    :param x:          input 1D array
    :type n: int
    :param n:          input array length
    :type nclusters: int
    :param nclusters:  number of clusters
    :type centroids: float
    :param centroids:  output centroids, size nclusters
    :rtype: float
    :return: imbalancce factor
    """
    ...

class Neighbor:
    thisown = ...
    __repr__ = ...
    id = ...
    distance = ...
    flag = ...
    def __init__(self, *args) -> None:
        ...
    
    def __lt__(self, other) -> bool:
        ...
    
    __swig_destroy__ = ...


class Nhood:
    thisown = ...
    __repr__ = ...
    pool = ...
    M = ...
    nn_old = ...
    nn_new = ...
    rnn_old = ...
    rnn_new = ...
    def __init__(self, *args) -> None:
        ...
    
    def insert(self, id, dist):
        ...
    
    __swig_destroy__ = ...


class NNDescent:
    thisown = ...
    __repr__ = ...
    def __init__(self, d, K) -> None:
        ...
    
    __swig_destroy__ = ...
    def build(self, qdis, n, verbose):
        ...
    
    def search(self, qdis, topk, indices, dists, vt):
        ...
    
    def reset(self):
        ...
    
    def init_graph(self, qdis):
        r"""Initialize the KNN graph randomly"""
        ...
    
    def nndescent(self, qdis, verbose):
        r"""Perform NNDescent algorithm"""
        ...
    
    def join(self, qdis):
        r"""Perform local join on each node"""
        ...
    
    def update(self):
        r"""Sample new neighbors for each node to peform local join later"""
        ...
    
    def generate_eval_set(self, qdis, c, v, N):
        r"""Sample a small number of points to evaluate the quality of KNNG built"""
        ...
    
    def eval_recall(self, ctrl_points, acc_eval_set):
        r"""Evaluate the quality of KNNG built"""
        ...
    
    has_built = ...
    S = ...
    R = ...
    iter = ...
    search_L = ...
    random_seed = ...
    K = ...
    d = ...
    L = ...
    ntotal = ...
    graph = ...
    final_graph = ...


class IndexNNDescent(Index):
    r"""
     The NNDescent index is a normal random-access index with an NNDescent
    link structure built on top
    """
    thisown = ...
    __repr__ = ...
    nndescent = ...
    own_fields = ...
    storage = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def add(self, n, x):
        ...
    
    def train(self, n, x):
        r"""Trains the storage if needed"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reset(self):
        ...
    


class IndexNNDescentFlat(IndexNNDescent):
    r"""
    Flat index topped with with a NNDescent structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFFlat(IndexIVF):
    r"""
     Inverted file with stored vectors. Here the inverted file
    pre-selects the vectors to be searched, but they are not otherwise
    encoded, the code array just contains the raw float entries.
    """
    thisown = ...
    __repr__ = ...
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFFlatDedup(IndexIVFFlat):
    thisown = ...
    __repr__ = ...
    instances = ...
    def train(self, n, x):
        r"""also dedups the training set"""
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""implemented for all IndexIVF* classes"""
        ...
    
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=..., stats=...):
        ...
    
    def remove_ids(self, sel):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        r"""not implemented"""
        ...
    
    def update_vectors(self, nv, idx, v):
        r"""not implemented"""
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        r"""not implemented"""
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


def storage_distance_computer(storage):
    ...

class NSG:
    thisown = ...
    __repr__ = ...
    ntotal = ...
    R = ...
    L = ...
    C = ...
    search_L = ...
    enterpoint = ...
    final_graph = ...
    is_built = ...
    rng = ...
    def __init__(self, R=...) -> None:
        ...
    
    def build(self, storage, n, knn_graph, verbose):
        ...
    
    def reset(self):
        ...
    
    def search(self, dis, k, I, D, vt):
        ...
    
    def init_graph(self, storage, knn_graph):
        ...
    
    def add_reverse_links(self, q, locks, dis, graph):
        ...
    
    def sync_prune(self, q, pool, dis, vt, knn_graph, graph):
        ...
    
    def link(self, storage, knn_graph, graph, verbose):
        ...
    
    def tree_grow(self, storage, degrees):
        ...
    
    def dfs(self, vt, root, cnt):
        ...
    
    def attach_unlinked(self, storage, vt, vt2, degrees):
        ...
    
    def check_graph(self):
        ...
    
    def get_final_graph(self):
        ...
    
    __swig_destroy__ = ...


class NSG_Graph_int:
    thisown = ...
    __repr__ = ...
    data = ...
    K = ...
    N = ...
    own_fields = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def at(self, *args):
        ...
    
    def get_neighbors(self, i, neighbors):
        ...
    


class IndexNSG(Index):
    r"""
     The NSG index is a normal random-access index with a NSG
    link structure built on top
    """
    thisown = ...
    __repr__ = ...
    nsg = ...
    own_fields = ...
    storage = ...
    is_built = ...
    GK = ...
    build_type = ...
    nndescent_S = ...
    nndescent_R = ...
    nndescent_L = ...
    nndescent_iter = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def build(self, n, x, knn_graph, GK):
        ...
    
    def add(self, n, x):
        ...
    
    def train(self, n, x):
        r"""Trains the storage if needed"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reset(self):
        ...
    
    def check_knn_graph(self, knn_graph, n, K):
        ...
    


class IndexNSGFlat(IndexNSG):
    r"""
    Flat index topped with with a NSG structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexNSGPQ(IndexNSG):
    r"""
    PQ index topped with with a NSG structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    __swig_destroy__ = ...


class IndexNSGSQ(IndexNSG):
    r"""
    SQ index topped with with a NSG structure to access elements
    more efficiently.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class OnDiskOneList:
    thisown = ...
    __repr__ = ...
    size = ...
    capacity = ...
    offset = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class OnDiskInvertedLists(InvertedLists):
    r"""
     On-disk storage of inverted lists.

    The data is stored in a mmapped chunk of memory (base pointer ptr,
    size totsize). Each list is a range of memory that contains (object
    List) that contains:

    - uint8_t codes[capacity * code_size]
    - followed by idx_t ids[capacity]

    in each of the arrays, the size <= capacity first elements are
    used, the rest is not initialized.

    Addition and resize are supported by:
    - roundind up the capacity of the lists to a power of two
    - maintaining a list of empty slots, sorted by size.
    - resizing the mmapped block is adjusted as needed.

    An OnDiskInvertedLists is compact if the size == capacity for all
    lists and there are no available slots.

    Addition to the invlists is slow. For incremental add it is better
    to use a default ArrayInvertedLists object and convert it to an
    OnDisk with merge_from.

    When it is known that a set of lists will be accessed, it is useful
    to call prefetch_lists, that launches a set of threads to read the
    lists in parallel.
    """
    thisown = ...
    __repr__ = ...
    lists = ...
    slots = ...
    filename = ...
    totsize = ...
    ptr = ...
    read_only = ...
    def list_size(self, list_no):
        ...
    
    def get_codes(self, list_no):
        ...
    
    def get_ids(self, list_no):
        ...
    
    def add_entries(self, list_no, n_entry, ids, code):
        ...
    
    def update_entries(self, list_no, offset, n_entry, ids, code):
        ...
    
    def resize(self, list_no, new_size):
        ...
    
    def merge_from_multiple(self, ils, n_il, shift_ids=..., verbose=...):
        ...
    
    def merge_from_1(self, il, verbose=...):
        r"""same as merge_from for a single invlist"""
        ...
    
    def crop_invlists(self, l0, l1):
        r"""restrict the inverted lists to l0:l1 without touching the mmapped region"""
        ...
    
    def prefetch_lists(self, list_nos, nlist):
        ...
    
    __swig_destroy__ = ...
    locks = ...
    pf = ...
    prefetch_nthread = ...
    def do_mmap(self):
        ...
    
    def update_totsize(self, new_totsize):
        ...
    
    def resize_locked(self, list_no, new_size):
        ...
    
    def allocate_slot(self, capacity):
        ...
    
    def free_slot(self, offset, capacity):
        ...
    
    def set_all_lists_sizes(self, sizes):
        r"""override all list sizes and make a packed storage"""
        ...
    
    def __init__(self, *args) -> None:
        r"""are inverted lists mapped read-only"""
        ...
    


class ZnSphereSearch:
    r"""
     returns the nearest vertex in the sphere to a query. Returns only
    the coordinates, not an id.

    Algorithm: all points are derived from a one atom vector up to a
    permutation and sign changes. The search function finds the most
    appropriate atom and transformation.
    """
    thisown = ...
    __repr__ = ...
    dimS = ...
    r2 = ...
    natom = ...
    voc = ...
    def __init__(self, dim, r2) -> None:
        ...
    
    def search(self, *args):
        r"""
        *Overload 1:*
        find nearest centroid. x does not need to be normalized

        |

        *Overload 2:*
        full call. Requires externally-allocated temp space

        |

        *Overload 3:*
        full call. Requires externally-allocated temp space
        """
        ...
    
    def search_multi(self, n, x, c_out, dp_out):
        ...
    
    __swig_destroy__ = ...


class EnumeratedVectors:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    nv = ...
    dim = ...
    def encode(self, x):
        r"""encode a vector from a collection"""
        ...
    
    def decode(self, code, c):
        r"""decode it"""
        ...
    
    def encode_multi(self, nc, c, codes):
        ...
    
    def decode_multi(self, nc, codes, c):
        ...
    
    def find_nn(self, n, codes, nq, xq, idx, dis):
        ...
    
    __swig_destroy__ = ...


class Repeat:
    thisown = ...
    __repr__ = ...
    val = ...
    n = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class Repeats:
    r"""
    Repeats: used to encode a vector that has n occurrences of
    val. Encodes the signs and permutation of the vector. Useful for
    atoms.
    """
    thisown = ...
    __repr__ = ...
    dim = ...
    repeats = ...
    def __init__(self, dim=..., c=...) -> None:
        ...
    
    def count(self):
        ...
    
    def encode(self, c):
        ...
    
    def decode(self, code, c):
        ...
    
    __swig_destroy__ = ...


class ZnSphereCodec(ZnSphereSearch, EnumeratedVectors):
    r"""
     codec that can return ids for the encoded vectors

    uses the ZnSphereSearch to encode the vector by encoding the
    permutation and signs. Depends on ZnSphereSearch because it uses
    the atom numbers
    """
    thisown = ...
    __repr__ = ...
    code_segments = ...
    nv = ...
    code_size = ...
    def __init__(self, dim, r2) -> None:
        ...
    
    def search_and_encode(self, x):
        ...
    
    def decode(self, code, c):
        ...
    
    def encode(self, x):
        r"""takes vectors that do not need to be centroids"""
        ...
    
    __swig_destroy__ = ...


class ZnSphereCodecRec(EnumeratedVectors):
    r"""
     recursive sphere codec

    Uses a recursive decomposition on the dimensions to encode
    centroids found by the ZnSphereSearch. The codes are *not*
    compatible with the ones of ZnSpehreCodec
    """
    thisown = ...
    __repr__ = ...
    r2 = ...
    log2_dim = ...
    code_size = ...
    def __init__(self, dim, r2) -> None:
        ...
    
    def encode_centroid(self, c):
        ...
    
    def decode(self, code, c):
        ...
    
    def encode(self, x):
        r"""
        vectors need to be centroids (does not work on arbitrary
        vectors)
        """
        ...
    
    all_nv = ...
    all_nv_cum = ...
    decode_cache_ld = ...
    decode_cache = ...
    def get_nv(self, ld, r2a):
        ...
    
    def get_nv_cum(self, ld, r2t, r2a):
        ...
    
    def set_nv_cum(self, ld, r2t, r2a, v):
        ...
    
    __swig_destroy__ = ...


class ZnSphereCodecAlt(ZnSphereCodec):
    r"""
     Codec that uses the recursive codec if dim is a power of 2 and
    the regular one otherwise
    """
    thisown = ...
    __repr__ = ...
    use_rec = ...
    znc_rec = ...
    def __init__(self, dim, r2) -> None:
        ...
    
    def encode(self, x):
        ...
    
    def decode(self, code, c):
        ...
    
    __swig_destroy__ = ...


class IndexLattice(IndexFlatCodes):
    r"""Index that encodes a vector with a series of Zn lattice quantizers"""
    thisown = ...
    __repr__ = ...
    nsq = ...
    dsq = ...
    zn_sphere_codec = ...
    scale_nbit = ...
    lattice_nbit = ...
    trained = ...
    def __init__(self, d, nsq, scale_nbit, r2) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class IVFPQSearchParameters(SearchParametersIVF):
    thisown = ...
    __repr__ = ...
    scan_table_threshold = ...
    polysemous_ht = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFPQ(IndexIVF):
    r"""
     Inverted file with Product Quantizer encoding. Each residual
    vector is encoded as a product quantizer code.
    """
    thisown = ...
    __repr__ = ...
    pq = ...
    do_polysemous_training = ...
    polysemous_training = ...
    scan_table_threshold = ...
    polysemous_ht = ...
    use_precomputed_table = ...
    precomputed_table = ...
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        ...
    
    def add_core_o(self, n, x, xids, residuals_2, precomputed_idx=..., inverted_list_context=...):
        r"""
        same as add_core, also:
        - output 2nd level residuals if residuals_2 != NULL
        - accepts precomputed_idx = nullptr
        """
        ...
    
    def train_encoder(self, n, x, assign):
        r"""trains the product quantizer"""
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def find_duplicates(self, ids, lims):
        r"""
         Find exact duplicates in the dataset.

        the duplicates are returned in pre-allocated arrays (see the
        max sizes).

        :type lims: int
        :param lims:   limits between groups of duplicates
                           (max size ntotal / 2 + 1)
        :type ids: int
        :param ids:    ids[lims[i]] : ids[lims[i+1]-1] is a group of
                           duplicates (max size ntotal)
        :rtype: int
        :return: n      number of groups found
        """
        ...
    
    def encode(self, key, x, code):
        ...
    
    def encode_multiple(self, n, keys, x, codes, compute_keys=...):
        r"""
         Encode multiple vectors

        :type n: int
        :param n:       nb vectors to encode
        :type keys: int
        :param keys:    posting list ids for those vectors (size n)
        :type x: float
        :param x:       vectors (size n * d)
        :type codes: uint8_t
        :param codes:   output codes (size n * code_size)
        :type compute_keys: boolean, optional
        :param compute_keys:  if false, assume keys are precomputed,
                                 otherwise compute them
        """
        ...
    
    def decode_multiple(self, n, keys, xcodes, x):
        r"""inverse of encode_multiple"""
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def precompute_table(self):
        r"""build precomputed table"""
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


def initialize_IVFPQ_precomputed_table(use_precomputed_table, quantizer, pq, precomputed_table, by_residual, verbose):
    r"""
     Pre-compute distance tables for IVFPQ with by-residual and METRIC_L2

    :type use_precomputed_table: int
    :param use_precomputed_table: (I/O)
               =-1: force disable
               =0: decide heuristically (default: use tables only if they are
                   < precomputed_tables_max_bytes), set use_precomputed_table on
        output =1: tables that work for all quantizers (size 256 * nlist * M) =2:
        specific version for MultiIndexQuantizer (much more compact)
    :type precomputed_table: faiss::AlignedTable< float,32 >
    :param precomputed_table: precomputed table to initialize
    """
    ...

class IndexIVFPQStats:
    r"""
    statistics are robust to internal threading, but not if
    IndexIVFPQ::search_preassigned is called by multiple threads
    """
    thisown = ...
    __repr__ = ...
    nrefine = ...
    n_hamming_pass = ...
    search_cycles = ...
    refine_cycles = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexIVFPQR(IndexIVFPQ):
    r"""Index with an additional level of PQ refinement"""
    thisown = ...
    __repr__ = ...
    refine_pq = ...
    refine_codes = ...
    k_factor = ...
    def reset(self):
        ...
    
    def remove_ids(self, sel):
        ...
    
    def train_encoder(self, n, x, assign):
        r"""trains the two product quantizers"""
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        r"""same as add_with_ids, but optionally use the precomputed list ids"""
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def merge_from(self, otherIndex, add_id):
        ...
    
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=..., stats=...):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class Index2Layer(IndexFlatCodes):
    r"""
     Same as an IndexIVFPQ without the inverted lists: codes are stored
    sequentially

    The class is mainly inteded to store encoded vectors that can be
    accessed randomly, the search function is not implemented.
    """
    thisown = ...
    __repr__ = ...
    q1 = ...
    pq = ...
    code_size_1 = ...
    code_size_2 = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""not implemented"""
        ...
    
    def get_distance_computer(self):
        ...
    
    def transfer_to_IVFPQ(self, other):
        r"""transfer the flat codes to an IVFPQ index"""
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    


class IndexFastScan(Index):
    r"""
     Fast scan version of IndexPQ and IndexAQ. Works for 4-bit PQ and AQ for now.

    The codes are not stored sequentially but grouped in blocks of size bbs.
    This makes it possible to compute distances quickly with SIMD instructions.
    The trailing codes (padding codes that are added to complete the last code)
    are garbage.

    Implementations:
    12: blocked loop with internal loop on Q with qbs
    13: same with reservoir accumulator to store results
    14: no qbs with heap accumulator
    15: no qbs with reservoir accumulator
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    implem = ...
    skip = ...
    bbs = ...
    qbs = ...
    M = ...
    nbits = ...
    ksub = ...
    code_size = ...
    ntotal2 = ...
    M2 = ...
    codes = ...
    orig_codes = ...
    def init_fastscan(self, d, M, nbits, metric, bbs):
        ...
    
    def reset(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def add(self, n, x):
        ...
    
    def compute_codes(self, codes, n, x):
        ...
    
    def compute_float_LUT(self, lut, n, x):
        ...
    
    def compute_quantized_LUT(self, n, x, lut, normalizers):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def remove_ids(self, sel):
        ...
    
    def get_CodePacker(self):
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def sa_code_size(self):
        r"""standalone codes interface (but the codes are flattened)"""
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    __swig_destroy__ = ...


class FastScanStats:
    thisown = ...
    __repr__ = ...
    t0 = ...
    t1 = ...
    t2 = ...
    t3 = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexAdditiveQuantizerFastScan(IndexFastScan):
    r"""
     Fast scan version of IndexAQ. Works for 4-bit AQ for now.

    The codes are not stored sequentially but grouped in blocks of size bbs.
    This makes it possible to compute distances quickly with SIMD instructions.

    Implementations:
    12: blocked loop with internal loop on Q with qbs
    13: same with reservoir accumulator to store results
    14: no qbs with heap accumulator
    15: no qbs with reservoir accumulator
    """
    thisown = ...
    __repr__ = ...
    aq = ...
    rescale_norm = ...
    norm_scale = ...
    max_train_points = ...
    def init(self, *args):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        build from an existing IndexAQ

        |

        *Overload 2:*
        build from an existing IndexAQ
        """
        ...
    
    def train(self, n, x):
        ...
    
    def estimate_norm_scale(self, n, x):
        ...
    
    def compute_codes(self, codes, n, x):
        ...
    
    def compute_float_LUT(self, lut, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def sa_decode(self, n, bytes, x):
        r"""
         Decode a set of vectors.

         NOTE: The codes in the IndexAdditiveQuantizerFastScan object are non-
               contiguous. But this method requires a contiguous representation.

        :type n: int
        :param n:       number of vectors
        :type bytes: uint8_t
        :param bytes:   input encoded vectors, size n * code_size
        :type x: float
        :param x:       output vectors, size n * d
        """
        ...
    


class IndexResidualQuantizerFastScan(IndexAdditiveQuantizerFastScan):
    r"""
     Index based on a residual quantizer. Stored vectors are
    approximated by residual quantization codes.
    Can also be used as a codec
    """
    thisown = ...
    __repr__ = ...
    rq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index
        :type metric: int, optional
        :param metric:  metric type
        :type search_type: int, optional
        :param search_type: AQ search type

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexLocalSearchQuantizerFastScan(IndexAdditiveQuantizerFastScan):
    r"""
     Index based on a local search quantizer. Stored vectors are
    approximated by local search quantization codes.
    Can also be used as a codec
    """
    thisown = ...
    __repr__ = ...
    lsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type M: int
        :param M:      number of subquantizers
        :type nbits: int
        :param nbits:  number of bit per subvector index
        :type metric: int, optional
        :param metric:  metric type
        :type search_type: int, optional
        :param search_type: AQ search type

        :type d: int
        :param d: dimensionality of the input vectors
        :type M: int
        :param M: number of subquantizers
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexProductResidualQuantizerFastScan(IndexAdditiveQuantizerFastScan):
    r"""
     Index based on a product residual quantizer. Stored vectors are
    approximated by product residual quantization codes.
    Can also be used as a codec
    """
    thisown = ...
    __repr__ = ...
    prq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of residual quantizers
        :type Msub: int
        :param Msub:     number of subquantizers per RQ
        :type nbits: int
        :param nbits:  number of bit per subvector index
        :type metric: int, optional
        :param metric:  metric type
        :type search_type: int, optional
        :param search_type: AQ search type

        :type d: int
        :param d: dimensionality of the input vectors
        :type nsplits: int
        :param nsplits: number of residual quantizers
        :type Msub: int
        :param Msub: number of subquantizers per RQ
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexProductLocalSearchQuantizerFastScan(IndexAdditiveQuantizerFastScan):
    r"""
     Index based on a product local search quantizer. Stored vectors are
    approximated by product local search quantization codes.
    Can also be used as a codec
    """
    thisown = ...
    __repr__ = ...
    plsq = ...
    def __init__(self, *args) -> None:
        r"""
         Constructor.

        :type d: int
        :param d:      dimensionality of the input vectors
        :type nsplits: int
        :param nsplits:  number of local search quantizers
        :type Msub: int
        :param Msub:     number of subquantizers per LSQ
        :type nbits: int
        :param nbits:  number of bit per subvector index
        :type metric: int, optional
        :param metric:  metric type
        :type search_type: int, optional
        :param search_type: AQ search type

        :type d: int
        :param d: dimensionality of the input vectors
        :type nsplits: int
        :param nsplits: number of local search quantizers
        :type Msub: int
        :param Msub: number of subquantizers per LSQ
        :type nbits: int
        :param nbits: number of bit per subvector index
        """
        ...
    
    __swig_destroy__ = ...


class IndexPQFastScan(IndexFastScan):
    r"""
     Fast scan version of IndexPQ. Works for 4-bit PQ for now.

    The codes are not stored sequentially but grouped in blocks of size bbs.
    This makes it possible to compute distances quickly with SIMD instructions.

    Implementations:
    12: blocked loop with internal loop on Q with qbs
    13: same with reservoir accumulator to store results
    14: no qbs with heap accumulator
    15: no qbs with reservoir accumulator
    """
    thisown = ...
    __repr__ = ...
    pq = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        build from an existing IndexPQ

        |

        *Overload 2:*
        build from an existing IndexPQ
        """
        ...
    
    def train(self, n, x):
        ...
    
    def compute_codes(self, codes, n, x):
        ...
    
    def compute_float_LUT(self, lut, n, x):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class simd16uint16:
    thisown = ...
    __repr__ = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class SIMDResultHandler:
    r"""This file contains callbacks for kernels that compute distances."""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    is_CMax = ...
    sizeof_ids = ...
    with_fields = ...
    def handle(self, q, b, d0, d1):
        r"""
        called when 32 distances are computed and provided in two
        simd16uint16. (q, b) indicate which entry it is in the block.
        """
        ...
    
    def set_block_origin(self, i0, j0):
        r"""set the sub-matrix that is being computed"""
        ...
    
    __swig_destroy__ = ...


class SIMDResultHandlerToFloat(SIMDResultHandler):
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    nq = ...
    ntotal = ...
    id_map = ...
    q_map = ...
    dbias = ...
    normalizers = ...
    def begin(self, norms):
        ...
    
    def end(self):
        ...
    
    __swig_destroy__ = ...


class DummyResultHandler(SIMDResultHandler):
    r"""
     Dummy structure that just computes a chqecksum on results
    (to avoid the computation to be optimized away)
    """
    thisown = ...
    __repr__ = ...
    cs = ...
    def handle(self, q, b, d0, d1):
        ...
    
    def set_block_origin(self, arg2, arg3):
        ...
    
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class StoreResultHandler(SIMDResultHandler):
    r"""
     memorize results in a nq-by-nb matrix.

    j0 is the current upper-left block of the matrix
    """
    thisown = ...
    __repr__ = ...
    data = ...
    ld = ...
    i0 = ...
    j0 = ...
    def __init__(self, data, ld) -> None:
        ...
    
    def handle(self, q, b, d0, d1):
        ...
    
    def set_block_origin(self, i0_in, j0_in):
        ...
    
    __swig_destroy__ = ...


class IndexIVFFastScan(IndexIVF):
    r"""
     Fast scan version of IVFPQ and IVFAQ. Works for 4-bit PQ/AQ for now.

    The codes in the inverted lists are not stored sequentially but
    grouped in blocks of size bbs. This makes it possible to very quickly
    compute distances with SIMD instructions.

    Implementations (implem):
    0: auto-select implementation (default)
    1: orig's search, re-implemented
    2: orig's search, re-ordered by invlist
    10: optimizer int16 search, collect results in heap, no qbs
    11: idem, collect results in reservoir
    12: optimizer int16 search, collect results in heap, uses qbs
    13: idem, collect results in reservoir
    14: internally multithreaded implem over nq * nprobe
    15: same with reservoir

    For range search, only 10 and 12 are supported.
    add 100 to the implem to force single-thread scanning (the coarse quantizer
    may still use multiple threads).
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    bbs = ...
    M = ...
    nbits = ...
    ksub = ...
    M2 = ...
    implem = ...
    skip = ...
    qbs = ...
    qbs2 = ...
    fine_quantizer = ...
    def init_fastscan(self, fine_quantizer, M, nbits, nlist, metric, bbs):
        r"""called by implementations"""
        ...
    
    def init_code_packer(self):
        ...
    
    __swig_destroy__ = ...
    orig_invlists = ...
    def add_with_ids(self, n, x, xids):
        ...
    
    def lookup_table_is_3d(self):
        ...
    
    def compute_LUT(self, n, x, cq, dis_tables, biases):
        ...
    
    def compute_LUT_uint8(self, n, x, cq, dis_tables, biases, normalizers):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=..., stats=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def search_dispatch_implem(self, n, x, k, distances, labels, cq, scaler, params=...):
        ...
    
    def range_search_dispatch_implem(self, n, x, radius, rres, cq_in, scaler, params=...):
        ...
    
    def search_implem_10(self, n, x, handler, cq, ndis_out, nlist_out, scaler, params=...):
        ...
    
    def search_implem_12(self, n, x, handler, cq, ndis_out, nlist_out, scaler, params=...):
        ...
    
    def search_implem_14(self, n, x, k, distances, labels, cq, impl, scaler, params=...):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def get_CodePacker(self):
        ...
    
    def reconstruct_orig_invlists(self):
        ...
    
    def sa_decode(self, n, bytes, x):
        r"""
         Decode a set of vectors.

         NOTE: The codes in the IndexFastScan object are non-contiguous.
               But this method requires a contiguous representation.

        :type n: int
        :param n:       number of vectors
        :type bytes: uint8_t
        :param bytes:   input encoded vectors, size n * code_size
        :type x: float
        :param x:       output vectors, size n * d
        """
        ...
    


class IVFFastScanStats:
    thisown = ...
    __repr__ = ...
    times = ...
    t_compute_distance_tables = ...
    t_round = ...
    t_copy_pack = ...
    t_scan = ...
    t_to_flat = ...
    reservoir_times = ...
    t_aq_encode = ...
    t_aq_norm_encode = ...
    def Mcy_at(self, i):
        ...
    
    def Mcy_reservoir_at(self, i):
        ...
    
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexIVFAdditiveQuantizerFastScan(IndexIVFFastScan):
    r"""
     Fast scan version of IVFAQ. Works for 4-bit AQ for now.

    The codes in the inverted lists are not stored sequentially but
    grouped in blocks of size bbs. This makes it possible to very quickly
    compute distances with SIMD instructions.

    Implementations (implem):
    0: auto-select implementation (default)
    1: orig's search, re-implemented
    2: orig's search, re-ordered by invlist
    10: optimizer int16 search, collect results in heap, no qbs
    11: idem, collect results in reservoir
    12: optimizer int16 search, collect results in heap, uses qbs
    13: idem, collect results in reservoir
    """
    thisown = ...
    __repr__ = ...
    aq = ...
    rescale_norm = ...
    norm_scale = ...
    max_train_points = ...
    def init(self, aq, nlist, metric, bbs):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def estimate_norm_scale(self, n, x):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listno=...):
        r"""
        same as the regular IVFAQ encoder. The codes are not reorganized by
        blocks a that point
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def lookup_table_is_3d(self):
        ...
    
    def compute_LUT(self, n, x, cq, dis_tables, biases):
        ...
    


class IndexIVFLocalSearchQuantizerFastScan(IndexIVFAdditiveQuantizerFastScan):
    thisown = ...
    __repr__ = ...
    lsq = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFResidualQuantizerFastScan(IndexIVFAdditiveQuantizerFastScan):
    thisown = ...
    __repr__ = ...
    rq = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFProductLocalSearchQuantizerFastScan(IndexIVFAdditiveQuantizerFastScan):
    thisown = ...
    __repr__ = ...
    plsq = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFProductResidualQuantizerFastScan(IndexIVFAdditiveQuantizerFastScan):
    thisown = ...
    __repr__ = ...
    prq = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFIndependentQuantizer(Index):
    r"""
     An IVF index with a quantizer that has a different input dimension from the
    payload size. The vectors to encode are obtained from the input vectors by a
    VectorTransform.
    """
    thisown = ...
    __repr__ = ...
    quantizer = ...
    vt = ...
    index_ivf = ...
    own_fields = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def add(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexIVFPQFastScan(IndexIVFFastScan):
    r"""
     Fast scan version of IVFPQ. Works for 4-bit PQ for now.

    The codes in the inverted lists are not stored sequentially but
    grouped in blocks of size bbs. This makes it possible to very quickly
    compute distances with SIMD instructions.

    Implementations (implem):
    0: auto-select implementation (default)
    1: orig's search, re-implemented
    2: orig's search, re-ordered by invlist
    10: optimizer int16 search, collect results in heap, no qbs
    11: idem, collect results in reservoir
    12: optimizer int16 search, collect results in heap, uses qbs
    13: idem, collect results in reservoir
    """
    thisown = ...
    __repr__ = ...
    pq = ...
    use_precomputed_table = ...
    precomputed_table = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def train_encoder_num_vectors(self):
        ...
    
    def precompute_table(self):
        r"""build precomputed table, possibly updating use_precomputed_table"""
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listno=...):
        r"""
        same as the regular IVFPQ encoder. The codes are not reorganized by
        blocks a that point
        """
        ...
    
    def lookup_table_is_3d(self):
        ...
    
    def compute_LUT(self, n, x, cq, dis_tables, biases):
        ...
    
    __swig_destroy__ = ...


def round_uint8_per_column(tab, n, d, a_out=..., b_out=...):
    r"""
     Functions to quantize PQ floating-point Look Up Tables (LUT) to uint8, and
    biases to uint16. The accumulation is supposed to take place in uint16.
    The quantization coefficients are float (a, b) such that

         original_value = quantized_value * a / b

    The hardest part of the quantization is with multiple LUTs that need to be
    added up together. In that case, coefficient a has to be chosen so that
    the sum fits in a uint16 accumulator.
    """
    ...

def round_uint8_per_column_multi(tab, m, n, d, a_out=..., b_out=...):
    ...

def quantize_LUT_and_bias(nprobe, M, ksub, lut_is_3d, LUT, bias, LUTq, M2, biasq, a_out=..., b_out=...):
    r"""
     LUT quantization to uint8 and bias to uint16.

    (nprobe, M, ksub, lut_is_3d) determine the size of the the LUT

     LUT input:
     - 2D size (M, ksub): single matrix per probe (lut_is_3d=false)
     - 3D size (nprobe, M, ksub): separate LUT per probe (lut_is_3d=true)
     bias input:
     - nullptr: bias is 0
     - size (nprobe): one bias per probe
     Output:
     - LUTq uint8 version of the LUT (M size is rounded up to M2)
     - biasq (or nullptr): uint16 version of the LUT
     - a, b: scalars to approximate the true distance
    """
    ...

def aq_quantize_LUT_and_bias(nprobe, M, ksub, LUT, bias, M_norm, norm_scale, LUTq, M2, biasq, a_out, b_out):
    ...

def aq_estimate_norm_scale(M, ksub, M_norm, LUT):
    ...

class IndexBinary:
    r"""
     Abstract structure for a binary index.

    Supports adding vertices and searching them.

    All queries are symmetric because there is no distinction between codes and
    vectors.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    d = ...
    code_size = ...
    ntotal = ...
    verbose = ...
    is_trained = ...
    metric_type = ...
    __swig_destroy__ = ...
    def train(self, n, x):
        r"""
         Perform training on a representative set of vectors.

        :type n: int
        :param n:      nb of training vectors
        :type x: uint8_t
        :param x:      training vecors, size n * d / 8
        """
        ...
    
    def add(self, n, x):
        r"""
         Add n vectors of dimension d to the index.

        Vectors are implicitly assigned labels ntotal .. ntotal + n - 1
        :type x: uint8_t
        :param x:      input matrix, size n * d / 8
        """
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""
         Same as add, but stores xids instead of sequential ids.

        The default implementation fails with an assertion, as it is
        not supported by all indexes.

        :type xids: int
        :param xids: if non-null, ids to store for the vectors (size n)
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""
         Query n vectors of dimension d to the index.

        return at most k vectors. If there are not enough results for a
        query, the result array is padded with -1s.

        :type x: uint8_t
        :param x:           input vectors to search, size n * d / 8
        :type labels: int
        :param labels:      output labels of the NNs, size n*k
        :type distances: int
        :param distances:   output pairwise distances, size n*k
        """
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        r"""
         Query n vectors of dimension d to the index.

        return all vectors with distance < radius. Note that many indexes
        do not implement the range_search (only the k-NN search is
        mandatory). The distances are converted to float to reuse the
        RangeSearchResult structure, but they are integer. By convention,
        only distances < radius (strict comparison) are returned,
        ie. radius = 0 does not return any result and 1 returns only
        exact same vectors.

        :type x: uint8_t
        :param x:           input vectors to search, size n * d / 8
        :type radius: int
        :param radius:      search radius
        :type result: :py:class:`RangeSearchResult`
        :param result:      result table
        """
        ...
    
    def assign(self, n, x, labels, k=...):
        r"""
         Return the indexes of the k vectors closest to the query x.

        This function is identical to search but only returns labels of
        neighbors.
        :type x: uint8_t
        :param x:           input vectors to search, size n * d / 8
        :type labels: int
        :param labels:      output labels of the NNs, size n*k
        """
        ...
    
    def reset(self):
        r"""Removes all elements from the database."""
        ...
    
    def remove_ids(self, sel):
        r"""Removes IDs from the index. Not supported by all indexes."""
        ...
    
    def reconstruct(self, key, recons):
        r"""
         Reconstruct a stored vector.

        This function may not be defined for some indexes.
        :type key: int
        :param key:         id of the vector to reconstruct
        :type recons: uint8_t
        :param recons:      reconstucted vector (size d / 8)
        """
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        r"""
         Reconstruct vectors i0 to i0 + ni - 1.

        This function may not be defined for some indexes.
        :type recons: uint8_t
        :param recons:      reconstucted vectors (size ni * d / 8)
        """
        ...
    
    def search_and_reconstruct(self, n, x, k, distances, labels, recons, params=...):
        r"""
         Similar to search, but also reconstructs the stored vectors (or an
        approximation in the case of lossy coding) for the search results.

        If there are not enough results for a query, the resulting array
        is padded with -1s.

        :type recons: uint8_t
        :param recons:      reconstructed vectors size (n, k, d)
        """
        ...
    
    def display(self):
        r"""Display the actual class name and some more info."""
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        r"""
         moves the entries from another dataset to self.
        On output, other is empty.
        add_id is added to all moved ids
        (for sequential ids, this would be this->ntotal)
        """
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        r"""
         check that the two indexes are compatible (ie, they are
        trained in the same way and have the same
        parameters). Otherwise throw.
        """
        ...
    
    def sa_code_size(self):
        r"""size of the produced codes in bytes"""
        ...
    
    def add_sa_codes(self, n, codes, xids):
        r"""Same as add_with_ids for IndexBinary."""
        ...
    


class IndexBinaryFlat(IndexBinary):
    r"""Index that stores the full vectors and performs exhaustive search."""
    thisown = ...
    __repr__ = ...
    xb = ...
    use_heap = ...
    query_batch_size = ...
    approx_topk_mode = ...
    def add(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def remove_ids(self, sel):
        r"""
         Remove some ids. Note that because of the indexing structure,
        the semantics of this operation are different from the usual ones:
        the new ids are shifted.
        """
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexBinaryIVF(IndexBinary):
    r"""
     Index based on a inverted file (IVF)

    In the inverted file, the quantizer (an IndexBinary instance) provides a
    quantization index for each vector to be added. The quantization
    index maps to a list (aka inverted list or posting list), where the
    id of the vector is stored.

    Otherwise the object is similar to the IndexIVF
    """
    thisown = ...
    __repr__ = ...
    invlists = ...
    own_invlists = ...
    nprobe = ...
    max_codes = ...
    use_heap = ...
    per_invlist_search = ...
    direct_map = ...
    quantizer = ...
    nlist = ...
    own_fields = ...
    cp = ...
    clustering_index = ...
    def __init__(self, *args) -> None:
        r"""
         The Inverted file takes a quantizer (an IndexBinary) on input,
        which implements the function mapping a vector to a list
        identifier. The pointer is borrowed: the quantizer should not
        be deleted while the IndexBinaryIVF is in use.
        """
        ...
    
    __swig_destroy__ = ...
    def reset(self):
        ...
    
    def train(self, n, x):
        r"""Trains the quantizer"""
        ...
    
    def add(self, n, x):
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def add_core(self, n, x, xids, precomputed_idx):
        r"""
         Implementation of vector addition where the vector assignments are
        predefined.

        :type precomputed_idx: int
        :param precomputed_idx:    quantization indices for the input vectors
            (size n)
        """
        ...
    
    def search_preassigned(self, n, x, k, assign, centroid_dis, distances, labels, store_pairs, params=...):
        r"""
         Search a set of vectors, that are pre-quantized by the IVF
         quantizer. Fill in the corresponding heaps with the query
         results. search() calls this.

        :type n: int
        :param n:      nb of vectors to query
        :type x: uint8_t
        :param x:      query vectors, size nx * d
        :type assign: int
        :param assign: coarse quantization indices, size nx * nprobe
        :type centroid_dis: int
        :param centroid_dis:
                          distances to coarse centroids, size nx * nprobe
        :param distance:
                          output distances, size n * k
        :type labels: int
        :param labels: output labels, size n * k
        :type store_pairs: boolean
        :param store_pairs: store inv list index + inv list offset
                                instead in upper/lower 32 bit of result,
                                instead of ids (used for reranking).
        :type params: :py:class:`IVFSearchParameters`, optional
        :param params: used to override the object's search parameters
        """
        ...
    
    def get_InvertedListScanner(self, store_pairs=...):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""assign the vectors, then call search_preassign"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def range_search_preassigned(self, n, x, radius, assign, centroid_dis, result):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reconstruct_n(self, i0, ni, recons):
        r"""
         Reconstruct a subset of the indexed vectors.

        Overrides default implementation to bypass reconstruct() which requires
        direct_map to be maintained.

        :type i0: int
        :param i0:     first vector to reconstruct
        :type ni: int
        :param ni:     nb of vectors to reconstruct
        :type recons: uint8_t
        :param recons: output array of reconstructed vectors, size ni * d / 8
        """
        ...
    
    def search_and_reconstruct(self, n, x, k, distances, labels, recons, params=...):
        r"""
         Similar to search, but also reconstructs the stored vectors (or an
        approximation in the case of lossy coding) for the search results.

        Overrides default implementation to avoid having to maintain direct_map
        and instead fetch the code offsets through the `store_pairs` flag in
        search_preassigned().

        :type recons: uint8_t
        :param recons:      reconstructed vectors size (n, k, d / 8)
        """
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        r"""
         Reconstruct a vector given the location in terms of (inv list index +
        inv list offset) instead of the id.

        Useful for reconstructing when the direct_map is not maintained and
        the inv list offset is computed by search_preassigned() with
        `store_pairs` set.
        """
        ...
    
    def remove_ids(self, sel):
        r"""Dataset manipulation functions"""
        ...
    
    def merge_from(self, other, add_id):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def get_list_size(self, list_no):
        ...
    
    def make_direct_map(self, new_maintain_direct_map=...):
        r"""
         initialize a direct map

        :type new_maintain_direct_map: boolean, optional
        :param new_maintain_direct_map:    if true, create a direct map,
                                              else clear it
        """
        ...
    
    def set_direct_map_type(self, type):
        ...
    
    def replace_invlists(self, il, own=...):
        ...
    


class BinaryInvertedListScanner:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def set_query(self, query_vector):
        r"""from now on we handle this query."""
        ...
    
    def set_list(self, list_no, coarse_dis):
        r"""following codes come from this inverted list"""
        ...
    
    def distance_to_code(self, code):
        r"""compute a single query-to-code distance"""
        ...
    
    def scan_codes(self, n, codes, ids, distances, labels, k):
        r"""
         compute the distances to codes. (distances, labels) should be
        organized as a min- or max-heap

        :type n: int
        :param n:      number of codes to scan
        :type codes: uint8_t
        :param codes:  codes to scan (n * code_size)
        :type ids: int
        :param ids:        corresponding ids (ignored if store_pairs)
        :type distances: int
        :param distances:  heap distances (size k)
        :type labels: int
        :param labels:     heap labels (size k)
        :type k: int
        :param k:          heap size
        """
        ...
    
    def scan_codes_range(self, n, codes, ids, radius, result):
        ...
    
    __swig_destroy__ = ...


class IndexBinaryFromFloat(IndexBinary):
    r"""
     IndexBinary backed by a float Index.

    Supports adding vertices and searching them.

    All queries are symmetric because there is no distinction between codes and
    vectors.
    """
    thisown = ...
    __repr__ = ...
    index = ...
    own_fields = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def add(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    


class IndexBinaryHNSW(IndexBinary):
    r"""
     The HNSW index is a normal random-access index with a HNSW
    link structure built on top
    """
    thisown = ...
    __repr__ = ...
    hnsw = ...
    own_fields = ...
    storage = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def get_distance_computer(self):
        ...
    
    def add(self, n, x):
        ...
    
    def train(self, n, x):
        r"""Trains the storage if needed"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""entry point for search"""
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reset(self):
        ...
    


class IndexBinaryHash(IndexBinary):
    r"""just uses the b first bits as a hash value"""
    thisown = ...
    __repr__ = ...
    invlists = ...
    b = ...
    nflip = ...
    def __init__(self, *args) -> None:
        ...
    
    def reset(self):
        ...
    
    def add(self, n, x):
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def display(self):
        ...
    
    def hashtable_size(self):
        ...
    
    __swig_destroy__ = ...


class IndexBinaryHashStats:
    thisown = ...
    __repr__ = ...
    nq = ...
    n0 = ...
    nlist = ...
    ndis = ...
    def __init__(self) -> None:
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexBinaryMultiHash(IndexBinary):
    r"""just uses the b first bits as a hash value"""
    thisown = ...
    __repr__ = ...
    storage = ...
    own_fields = ...
    maps = ...
    nhash = ...
    b = ...
    nflip = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...
    def reset(self):
        ...
    
    def add(self, n, x):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def hashtable_size(self):
        ...
    


class ThreadedIndexBase(Index):
    r"""
    A holder of indices in a collection of threads
    The interface to this class itself is not thread safe
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    __swig_destroy__ = ...
    def addIndex(self, index):
        r"""
        override an index that is managed by ourselves.
        WARNING: once an index is added, it becomes unsafe to touch it from any
        other thread than that on which is managing it, until we are shut
        down. Use runOnIndex to perform work on it instead.
        """
        ...
    
    def removeIndex(self, index):
        r"""
        Remove an index that is managed by ourselves.
        This will flush all pending work on that index, and then shut
        down its managing thread, and will remove the index.
        """
        ...
    
    def runOnIndex(self, *args):
        r"""
        Run a function on all indices, in the thread that the index is
        managed in.
        Function arguments are (index in collection, index pointer)
        """
        ...
    
    def reset(self):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def count(self):
        r"""Returns the number of sub-indices"""
        ...
    
    def at(self, *args):
        r"""
        *Overload 1:*
        Returns the i-th sub-index

        |

        *Overload 2:*
        Returns the i-th sub-index (const version)
        """
        ...
    
    own_indices = ...


class ThreadedIndexBaseBinary(IndexBinary):
    r"""
    A holder of indices in a collection of threads
    The interface to this class itself is not thread safe
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    __swig_destroy__ = ...
    def addIndex(self, index):
        r"""
        override an index that is managed by ourselves.
        WARNING: once an index is added, it becomes unsafe to touch it from any
        other thread than that on which is managing it, until we are shut
        down. Use runOnIndex to perform work on it instead.
        """
        ...
    
    def removeIndex(self, index):
        r"""
        Remove an index that is managed by ourselves.
        This will flush all pending work on that index, and then shut
        down its managing thread, and will remove the index.
        """
        ...
    
    def runOnIndex(self, *args):
        r"""
        Run a function on all indices, in the thread that the index is
        managed in.
        Function arguments are (index in collection, index pointer)
        """
        ...
    
    def reset(self):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def count(self):
        r"""Returns the number of sub-indices"""
        ...
    
    def at(self, *args):
        r"""
        *Overload 1:*
        Returns the i-th sub-index

        |

        *Overload 2:*
        Returns the i-th sub-index (const version)
        """
        ...
    
    own_indices = ...


class IndexShards(ThreadedIndexBase):
    r"""Index that concatenates the results from several sub-indexes"""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*

        The dimension that all sub-indices must share will be the dimension of
        the first sub-index added

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :type successive_ids: boolean, optional
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 2:*

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :type successive_ids: boolean, optional
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 3:*

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 4:*

        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 5:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension

        |

        *Overload 6:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension

        |

        *Overload 7:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension
        """
        ...
    
    def add_shard(self, index):
        r"""Alias for addIndex()"""
        ...
    
    def remove_shard(self, index):
        r"""Alias for removeIndex()"""
        ...
    
    def add(self, n, x):
        r"""supported only for sub-indices that implement add_with_ids"""
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""
        Cases (successive_ids, xids):
        - true, non-NULL       ERROR: it makes no sense to pass in ids and
                               request them to be shifted
        - true, NULL           OK: but should be called only once (calls add()
                               on sub-indexes).
        - false, non-NULL      OK: will call add_with_ids with passed in xids
                               distributed evenly over shards
        - false, NULL          OK: will call add_with_ids on each sub-index,
                               starting at ntotal
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    
    successive_ids = ...
    def syncWithSubIndexes(self):
        r"""
        Synchronize the top-level index (IndexShards) with data in the
        sub-indices
        """
        ...
    
    __swig_destroy__ = ...


class IndexBinaryShards(ThreadedIndexBaseBinary):
    r"""Index that concatenates the results from several sub-indexes"""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*

        The dimension that all sub-indices must share will be the dimension of
        the first sub-index added

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :type successive_ids: boolean, optional
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 2:*

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :type successive_ids: boolean, optional
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 3:*

        :type threaded: boolean, optional
        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 4:*

        :param threaded:     do we use one thread per sub_index or do
                                queries sequentially?
        :param successive_ids: should we shift the returned ids by
                                the size of each sub-index or return them
                                as they are?

        |

        *Overload 5:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension

        |

        *Overload 6:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension

        |

        *Overload 7:*
         int version due to the implicit bool conversion ambiguity of int as
         dimension
        """
        ...
    
    def add_shard(self, index):
        r"""Alias for addIndex()"""
        ...
    
    def remove_shard(self, index):
        r"""Alias for removeIndex()"""
        ...
    
    def add(self, n, x):
        r"""supported only for sub-indices that implement add_with_ids"""
        ...
    
    def add_with_ids(self, n, x, xids):
        r"""
        Cases (successive_ids, xids):
        - true, non-NULL       ERROR: it makes no sense to pass in ids and
                               request them to be shifted
        - true, NULL           OK: but should be called only once (calls add()
                               on sub-indexes).
        - false, non-NULL      OK: will call add_with_ids with passed in xids
                               distributed evenly over shards
        - false, NULL          OK: will call add_with_ids on each sub-index,
                               starting at ntotal
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    
    successive_ids = ...
    def syncWithSubIndexes(self):
        r"""
        Synchronize the top-level index (IndexShards) with data in the
        sub-indices
        """
        ...
    
    __swig_destroy__ = ...


class IndexShardsIVF(IndexShards, Level1Quantizer):
    r"""
    IndexShards with a common coarse quantizer. All the indexes added should be
    IndexIVFInterface indexes so that the search_precomputed can be called.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, quantizer, nlist, threaded=..., successive_ids=...) -> None:
        ...
    
    def addIndex(self, index):
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def train(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    __swig_destroy__ = ...


class IndexReplicas(ThreadedIndexBase):
    r"""
    Takes individual faiss::Index instances, and splits queries for
    sending to each Index instance, and joins the results together
    when done.
    Each index is managed by a separate CPU thread.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        The dimension that all sub-indices must share will be the dimension of
        the first sub-index added
        :type threaded: boolean, optional
        :param threaded: do we use one thread per sub-index or do queries
            sequentially?

        |

        *Overload 2:*
        :type d: int
        :param d: the dimension that all sub-indices must share
        :type threaded: boolean, optional
        :param threaded: do we use one thread per sub index or do queries
            sequentially?

        |

        *Overload 3:*
        :type d: int
        :param d: the dimension that all sub-indices must share
        :param threaded: do we use one thread per sub index or do queries
            sequentially?

        |

        *Overload 4:*
        int version due to the implicit bool conversion ambiguity of int as
        dimension

        |

        *Overload 5:*
        int version due to the implicit bool conversion ambiguity of int as
        dimension
        """
        ...
    
    def add_replica(self, index):
        r"""Alias for addIndex()"""
        ...
    
    def remove_replica(self, index):
        r"""Alias for removeIndex()"""
        ...
    
    def train(self, n, x):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def add(self, n, x):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""
        faiss::Index API
        Query is partitioned into a slice for each sub-index
        split by ceil(n / #indices) for our sub-indices
        """
        ...
    
    def reconstruct(self, arg2, v):
        r"""reconstructs from the first index"""
        ...
    
    def syncWithSubIndexes(self):
        r"""
        Synchronize the top-level index (IndexShards) with data in the
        sub-indices
        """
        ...
    
    __swig_destroy__ = ...


class IndexBinaryReplicas(ThreadedIndexBaseBinary):
    r"""
    Takes individual faiss::Index instances, and splits queries for
    sending to each Index instance, and joins the results together
    when done.
    Each index is managed by a separate CPU thread.
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        r"""
        *Overload 1:*
        The dimension that all sub-indices must share will be the dimension of
        the first sub-index added
        :type threaded: boolean, optional
        :param threaded: do we use one thread per sub-index or do queries
            sequentially?

        |

        *Overload 2:*
        :type d: int
        :param d: the dimension that all sub-indices must share
        :type threaded: boolean, optional
        :param threaded: do we use one thread per sub index or do queries
            sequentially?

        |

        *Overload 3:*
        :type d: int
        :param d: the dimension that all sub-indices must share
        :param threaded: do we use one thread per sub index or do queries
            sequentially?

        |

        *Overload 4:*
        int version due to the implicit bool conversion ambiguity of int as
        dimension

        |

        *Overload 5:*
        int version due to the implicit bool conversion ambiguity of int as
        dimension
        """
        ...
    
    def add_replica(self, index):
        r"""Alias for addIndex()"""
        ...
    
    def remove_replica(self, index):
        r"""Alias for removeIndex()"""
        ...
    
    def train(self, n, x):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def add(self, n, x):
        r"""
        faiss::Index API
        All indices receive the same call
        """
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        r"""
        faiss::Index API
        Query is partitioned into a slice for each sub-index
        split by ceil(n / #indices) for our sub-indices
        """
        ...
    
    def reconstruct(self, arg2, v):
        r"""reconstructs from the first index"""
        ...
    
    def syncWithSubIndexes(self):
        r"""
        Synchronize the top-level index (IndexShards) with data in the
        sub-indices
        """
        ...
    
    __swig_destroy__ = ...


class IndexSplitVectors(Index):
    r"""
     splits input vectors in segments and assigns each segment to a sub-index
    used to distribute a MultiIndexQuantizer
    """
    thisown = ...
    __repr__ = ...
    own_fields = ...
    threaded = ...
    sub_indexes = ...
    sum_d = ...
    def __init__(self, d, threaded=...) -> None:
        r"""sum of dimensions seen so far"""
        ...
    
    def add_sub_index(self, arg2):
        ...
    
    def sync_with_sub_indexes(self):
        ...
    
    def add(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexRandom(Index):
    r"""
     index that returns random results.
    used mainly for time benchmarks
    """
    thisown = ...
    __repr__ = ...
    seed = ...
    def __init__(self, *args) -> None:
        ...
    
    def add(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def reset(self):
        ...
    
    __swig_destroy__ = ...


class IndexRowwiseMinMaxBase(Index):
    r"""
     Index wrapper that performs rowwise normalization to [0,1], preserving
     the coefficients. This is a vector codec index only.

     Basically, this index performs a rowwise scaling to [0,1] of every row
     in an input dataset before calling subindex::train() and
     subindex::sa_encode(). sa_encode() call stores the scaling coefficients
      (scaler and minv) in the very beginning of every output code. The format:
         [scaler][minv][subindex::sa_encode() output]
     The de-scaling in sa_decode() is done using:
         output_rescaled = scaler * output + minv

     An additional ::train_inplace() function is provided in order to do
     an inplace scaling before calling subindex::train() and, thus, avoiding
     the cloning of the input dataset, but modifying the input dataset because
     of the scaling and the scaling back. It is up to user to call
     this function instead of ::train()

     Derived classes provide different data types for scaling coefficients.
     Currently, versions with fp16 and fp32 scaling coefficients are available.
    fp16 version adds 4 extra bytes per encoded vector
    fp32 version adds 8 extra bytes per encoded vector
     Provides base functions for rowwise normalizing indices.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    index = ...
    own_fields = ...
    __swig_destroy__ = ...
    def add(self, n, x):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def reset(self):
        ...
    
    def train_inplace(self, n, x):
        ...
    


class IndexRowwiseMinMaxFP16(IndexRowwiseMinMaxBase):
    r"""Stores scaling coefficients as fp16 values."""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def train_inplace(self, n, x):
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class IndexRowwiseMinMax(IndexRowwiseMinMaxBase):
    r"""Stores scaling coefficients as fp32 values."""
    thisown = ...
    __repr__ = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def train_inplace(self, n, x):
        ...
    
    def sa_code_size(self):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    __swig_destroy__ = ...


class Linear:
    r"""minimal translation of nn.Linear"""
    thisown = ...
    __repr__ = ...
    in_features = ...
    out_features = ...
    weight = ...
    bias = ...
    def __init__(self, in_features, out_features, bias=...) -> None:
        ...
    
    def __call__(self, x):
        ...
    
    __swig_destroy__ = ...


class Embedding:
    r"""minimal translation of nn.Embedding"""
    thisown = ...
    __repr__ = ...
    num_embeddings = ...
    embedding_dim = ...
    weight = ...
    def __init__(self, num_embeddings, embedding_dim) -> None:
        ...
    
    def __call__(self, arg2):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class FFN:
    r"""
    Feed forward layer that expands to a hidden dimension, applies a ReLU non
    linearity and maps back to the orignal dimension
    """
    thisown = ...
    __repr__ = ...
    linear1 = ...
    linear2 = ...
    def __init__(self, d, h) -> None:
        ...
    
    def __call__(self, x):
        ...
    
    __swig_destroy__ = ...


class QINCoStep:
    thisown = ...
    __repr__ = ...
    d = ...
    K = ...
    L = ...
    h = ...
    def __init__(self, d, K, L, h) -> None:
        ...
    
    codebook = ...
    MLPconcat = ...
    residual_blocks = ...
    def get_residual_block(self, i):
        ...
    
    def encode(self, xhat, x, residuals=...):
        r"""
         encode a set of vectors x with intial estimate xhat. Optionally return
        the delta to be added to xhat to form the new xhat
        """
        ...
    
    def decode(self, xhat, codes):
        ...
    
    __swig_destroy__ = ...


class NeuralNetCodec:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    d = ...
    M = ...
    def decode(self, codes):
        ...
    
    def encode(self, x):
        ...
    
    __swig_destroy__ = ...


class QINCo(NeuralNetCodec):
    thisown = ...
    __repr__ = ...
    K = ...
    L = ...
    h = ...
    codebook0 = ...
    steps = ...
    def __init__(self, d, K, L, M, h) -> None:
        ...
    
    def get_step(self, i):
        ...
    
    def decode(self, codes):
        ...
    
    def encode(self, x):
        ...
    
    __swig_destroy__ = ...


class Tensor2D:
    r"""Implements a few neural net layers, mainly to support QINCo"""
    thisown = ...
    __repr__ = ...
    shape = ...
    v = ...
    def __init__(self, n0, n1, data=...) -> None:
        ...
    
    def __iadd__(self, arg2):
        ...
    
    def column(self, j):
        r"""get column #j as a 1-column Tensor2D"""
        ...
    
    def numel(self):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class Int32Tensor2D:
    r"""Implements a few neural net layers, mainly to support QINCo"""
    thisown = ...
    __repr__ = ...
    shape = ...
    v = ...
    def __init__(self, n0, n1, data=...) -> None:
        ...
    
    def __iadd__(self, arg2):
        ...
    
    def column(self, j):
        r"""get column #j as a 1-column Tensor2D"""
        ...
    
    def numel(self):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class IndexNeuralNetCodec(IndexFlatCodes):
    thisown = ...
    __repr__ = ...
    net = ...
    M = ...
    nbits = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def sa_encode(self, n, x, codes):
        ...
    
    def sa_decode(self, n, codes, x):
        ...
    
    __swig_destroy__ = ...


class IndexQINCo(IndexNeuralNetCodec):
    thisown = ...
    __repr__ = ...
    qinco = ...
    def __init__(self, *args) -> None:
        ...
    
    __swig_destroy__ = ...


class RaBitQuantizer(Quantizer):
    thisown = ...
    __repr__ = ...
    centroid = ...
    metric_type = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def compute_codes(self, x, codes, n):
        ...
    
    def compute_codes_core(self, x, codes, n, centroid_in):
        ...
    
    def decode(self, codes, x, n):
        ...
    
    def decode_core(self, codes, x, n, centroid_in):
        ...
    
    def get_distance_computer(self, qb, centroid_in=...):
        ...
    
    __swig_destroy__ = ...


class RaBitQSearchParameters(SearchParameters):
    thisown = ...
    __repr__ = ...
    qb = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexRaBitQ(IndexFlatCodes):
    thisown = ...
    __repr__ = ...
    rabitq = ...
    center = ...
    qb = ...
    def __init__(self, *args) -> None:
        ...
    
    def train(self, n, x):
        ...
    
    def sa_encode(self, n, x, bytes):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def get_FlatCodesDistanceComputer(self):
        ...
    
    def get_quantized_distance_computer(self, qb_in):
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    __swig_destroy__ = ...


class IVFRaBitQSearchParameters(SearchParametersIVF):
    thisown = ...
    __repr__ = ...
    qb = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class IndexIVFRaBitQ(IndexIVF):
    thisown = ...
    __repr__ = ...
    rabitq = ...
    qb = ...
    def __init__(self, *args) -> None:
        ...
    
    def train_encoder(self, n, x, assign):
        ...
    
    def encode_vectors(self, n, x, list_nos, codes, include_listnos=...):
        ...
    
    def add_core(self, n, x, xids, precomputed_idx, inverted_list_context=...):
        ...
    
    def get_InvertedListScanner(self, store_pairs, sel, params):
        ...
    
    def reconstruct_from_offset(self, list_no, offset, recons):
        ...
    
    def sa_decode(self, n, bytes, x):
        ...
    
    def get_distance_computer(self):
        ...
    
    __swig_destroy__ = ...


class RangeSearchResult:
    r"""
    The objective is to have a simple result structure while
    minimizing the number of mem copies in the result. The method
    do_allocation can be overloaded to allocate the result tables in
    the matrix type of a scripting language like Lua or Python.
    """
    thisown = ...
    __repr__ = ...
    nq = ...
    lims = ...
    labels = ...
    distances = ...
    buffer_size = ...
    def __init__(self, nq, alloc_lims=...) -> None:
        r"""lims must be allocated on input to range_search."""
        ...
    
    def do_allocation(self):
        r"""
        called when lims contains the nb of elements result entries
        for each query
        """
        ...
    
    __swig_destroy__ = ...


class BufferList:
    r"""
    List of temporary buffers used to store results before they are
    copied to the RangeSearchResult object.
    """
    thisown = ...
    __repr__ = ...
    buffer_size = ...
    buffers = ...
    wp = ...
    def __init__(self, buffer_size) -> None:
        ...
    
    __swig_destroy__ = ...
    def append_buffer(self):
        r"""create a new buffer"""
        ...
    
    def add(self, id, dis):
        r"""add one result, possibly appending a new buffer if needed"""
        ...
    
    def copy_range(self, ofs, n, dest_ids, dest_dis):
        r"""
        copy elemnts ofs:ofs+n-1 seen as linear data in the buffers to
        tables dest_ids, dest_dis
        """
        ...
    


class RangeQueryResult:
    r"""result structure for a single query"""
    thisown = ...
    __repr__ = ...
    qno = ...
    nres = ...
    pres = ...
    def add(self, dis, id):
        r"""called by search function to report a new result"""
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class RangeSearchPartialResult(BufferList):
    r"""the entries in the buffers are split per query"""
    thisown = ...
    __repr__ = ...
    res = ...
    def __init__(self, res_in) -> None:
        r"""eventually the result will be stored in res_in"""
        ...
    
    queries = ...
    def new_result(self, qno):
        r"""begin a new result"""
        ...
    
    def finalize(self):
        ...
    
    def set_lims(self):
        r"""called by range_search before do_allocation"""
        ...
    
    def copy_result(self, incremental=...):
        r"""called by range_search after do_allocation"""
        ...
    
    @staticmethod
    def merge(partial_results, do_delete=...):
        r"""
        merge a set of PartialResult's into one RangeSearchResult
        on output the partialresults are empty!
        """
        ...
    
    __swig_destroy__ = ...


class InterruptCallback:
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def want_interrupt(self):
        ...
    
    __swig_destroy__ = ...
    @staticmethod
    def clear_instance():
        ...
    
    @staticmethod
    def check():
        r"""
         check if:
        - an interrupt callback is set
        - the callback returns true
        if this is the case, then throw an exception. Should not be called
        from multiple threads.
        """
        ...
    
    @staticmethod
    def is_interrupted():
        r"""
        same as check() but return true if is interrupted instead of
        throwing. Can be called from multiple threads.
        """
        ...
    
    @staticmethod
    def get_period_hint(flops):
        r"""
         assuming each iteration takes a certain number of flops, what
        is a reasonable interval to check for interrupts?
        """
        ...
    


class TimeoutCallback(InterruptCallback):
    thisown = ...
    __repr__ = ...
    start = ...
    timeout = ...
    def want_interrupt(self):
        ...
    
    def set_timeout(self, timeout_in_seconds):
        ...
    
    @staticmethod
    def reset(timeout_in_seconds):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class VisitedTable:
    r"""set implementation optimized for fast access."""
    thisown = ...
    __repr__ = ...
    visited = ...
    visno = ...
    def __init__(self, size) -> None:
        ...
    
    def set(self, no):
        r"""set flag #no to true"""
        ...
    
    def get(self, no):
        r"""get flag #no"""
        ...
    
    def advance(self):
        r"""reset all flags to false"""
        ...
    
    __swig_destroy__ = ...


class IDSelector:
    r"""Encapsulates a set of ids to handle."""
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorRange(IDSelector):
    r"""ids between [imin, imax)"""
    thisown = ...
    __repr__ = ...
    imin = ...
    imax = ...
    assume_sorted = ...
    def __init__(self, imin, imax, assume_sorted=...) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    def find_sorted_ids_bounds(self, list_size, ids, jmin, jmax):
        r"""
        for sorted ids, find the range of list indices where the valid ids are
        stored
        """
        ...
    
    __swig_destroy__ = ...


class IDSelectorArray(IDSelector):
    r"""
     Simple array of elements

    is_member calls are very inefficient, but some operations can use the ids
    directly.
    """
    thisown = ...
    __repr__ = ...
    n = ...
    ids = ...
    def __init__(self, n, ids) -> None:
        r"""
         Construct with an array of ids to process

        :type n: int
        :param n: number of ids to store
        :type ids: int
        :param ids: elements to store. The pointer should remain valid during
                       IDSelectorArray's lifetime
        """
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorBatch(IDSelector):
    r"""
     Ids from a set.

    Repetitions of ids in the indices set passed to the constructor does not hurt
    performance.

    The hash function used for the bloom filter and GCC's implementation of
    unordered_set are just the least significant bits of the id. This works fine
    for random ids or ids in sequences but will produce many hash collisions if
    lsb's are always the same
    """
    thisown = ...
    __repr__ = ...
    nbits = ...
    mask = ...
    def __init__(self, n, indices) -> None:
        r"""
         Construct with an array of ids to process

        :type n: int
        :param n: number of ids to store
        :param ids: elements to store. The pointer can be released after
                       construction
        """
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorBitmap(IDSelector):
    r"""One bit per element. Constructed with a bitmap, size ceil(n / 8)."""
    thisown = ...
    __repr__ = ...
    n = ...
    bitmap = ...
    def __init__(self, n, bitmap) -> None:
        r"""
         Construct with a binary mask

        :type n: int
        :param n: size of the bitmap array
        :type bitmap: uint8_t
        :param bitmap: id will be selected iff id / 8 < n and bit number
                          (i%8) of bitmap[floor(i / 8)] is 1.
        """
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorNot(IDSelector):
    r"""reverts the membership test of another selector"""
    thisown = ...
    __repr__ = ...
    sel = ...
    def __init__(self, sel) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorAll(IDSelector):
    r"""selects all entries (useful for benchmarking)"""
    thisown = ...
    __repr__ = ...
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


class IDSelectorAnd(IDSelector):
    r"""
    does an AND operation on the the two given IDSelector's is_membership
    results.
    """
    thisown = ...
    __repr__ = ...
    lhs = ...
    rhs = ...
    def __init__(self, lhs, rhs) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorOr(IDSelector):
    r"""
    does an OR operation on the the two given IDSelector's is_membership
    results.
    """
    thisown = ...
    __repr__ = ...
    lhs = ...
    rhs = ...
    def __init__(self, lhs, rhs) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorXOr(IDSelector):
    r"""
    does an XOR operation on the the two given IDSelector's is_membership
    results.
    """
    thisown = ...
    __repr__ = ...
    lhs = ...
    rhs = ...
    def __init__(self, lhs, rhs) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IDSelectorTranslated(IDSelector):
    thisown = ...
    __repr__ = ...
    id_map = ...
    sel = ...
    def __init__(self, *args) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class IndexIDMap(Index):
    r"""Index that translates search results to ids"""
    thisown = ...
    __repr__ = ...
    index = ...
    own_fields = ...
    id_map = ...
    def add_with_ids(self, n, x, xids):
        r"""
        :type xids: int
        :param xids: if non-null, ids to store for the vectors (size n)
        """
        ...
    
    def add(self, n, x):
        r"""this will fail. Use add_with_ids"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def remove_ids(self, sel):
        r"""remove ids adapted to IndexFlat"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def sa_code_size(self):
        ...
    
    def add_sa_codes(self, n, x, xids):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    


class IndexBinaryIDMap(IndexBinary):
    r"""Index that translates search results to ids"""
    thisown = ...
    __repr__ = ...
    index = ...
    own_fields = ...
    id_map = ...
    def add_with_ids(self, n, x, xids):
        r"""
        :type xids: int
        :param xids: if non-null, ids to store for the vectors (size n)
        """
        ...
    
    def add(self, n, x):
        r"""this will fail. Use add_with_ids"""
        ...
    
    def search(self, n, x, k, distances, labels, params=...):
        ...
    
    def train(self, n, x):
        ...
    
    def reset(self):
        ...
    
    def remove_ids(self, sel):
        r"""remove ids adapted to IndexFlat"""
        ...
    
    def range_search(self, n, x, radius, result, params=...):
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    def check_compatible_for_merge(self, otherIndex):
        ...
    
    def sa_code_size(self):
        ...
    
    def add_sa_codes(self, n, x, xids):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    


class IndexIDMap2(IndexIDMap):
    r"""
    same as IndexIDMap but also provides an efficient reconstruction
    implementation via a 2-way index
    """
    thisown = ...
    __repr__ = ...
    rev_map = ...
    def construct_rev_map(self):
        r"""make the rev_map from scratch"""
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def remove_ids(self, sel):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def check_consistency(self):
        r"""check that the rev_map and the id_map are in sync"""
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    


class IndexBinaryIDMap2(IndexBinaryIDMap):
    r"""
    same as IndexIDMap but also provides an efficient reconstruction
    implementation via a 2-way index
    """
    thisown = ...
    __repr__ = ...
    rev_map = ...
    def construct_rev_map(self):
        r"""make the rev_map from scratch"""
        ...
    
    def add_with_ids(self, n, x, xids):
        ...
    
    def remove_ids(self, sel):
        ...
    
    def reconstruct(self, key, recons):
        ...
    
    def check_consistency(self):
        r"""check that the rev_map and the id_map are in sync"""
        ...
    
    def merge_from(self, otherIndex, add_id=...):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    


EXACT_TOPK = ...
APPROX_TOPK_BUCKETS_B32_D2 = ...
APPROX_TOPK_BUCKETS_B8_D3 = ...
APPROX_TOPK_BUCKETS_B16_D2 = ...
APPROX_TOPK_BUCKETS_B8_D2 = ...
def downcast_index(index):
    ...

def downcast_VectorTransform(vt):
    ...

def downcast_IndexBinary(index):
    ...

def downcast_InvertedLists(il):
    ...

def downcast_AdditiveQuantizer(aq):
    ...

def downcast_Quantizer(aq):
    ...

def write_index(*args):
    ...

def write_index_binary(*args):
    ...

def read_index(*args):
    ...

def read_index_binary(*args):
    ...

def write_VectorTransform(*args):
    ...

def read_VectorTransform(*args):
    ...

def read_ProductQuantizer(*args):
    ...

def write_ProductQuantizer(*args):
    ...

def write_InvertedLists(ils, f):
    ...

def read_InvertedLists(reader, io_flags=...):
    ...

def clone_index(arg1):
    ...

class Cloner:
    r"""
     Cloner class, useful to override classes with other cloning
    functions. The cloning function above just calls
    Cloner::clone_Index.
    """
    thisown = ...
    __repr__ = ...
    def clone_VectorTransform(self, arg2):
        ...
    
    def clone_Index(self, arg2):
        ...
    
    def clone_IndexIVF(self, arg2):
        ...
    
    __swig_destroy__ = ...
    def __init__(self) -> None:
        ...
    


IO_FLAG_SKIP_STORAGE = ...
IO_FLAG_READ_ONLY = ...
IO_FLAG_ONDISK_SAME_DIR = ...
IO_FLAG_SKIP_IVF_DATA = ...
IO_FLAG_SKIP_PRECOMPUTE_TABLE = ...
IO_FLAG_PQ_SKIP_SDC_TABLE = ...
IO_FLAG_MMAP = ...
IO_FLAG_MMAP_IFC = ...
def clone_Quantizer(quant):
    ...

def clone_binary_index(index):
    ...

class AutoTuneCriterion:
    r"""
    Evaluation criterion. Returns a performance measure in [0,1],
    higher is better.
    """
    thisown = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    __repr__ = ...
    nq = ...
    nnn = ...
    gt_nnn = ...
    gt_D = ...
    gt_I = ...
    def set_groundtruth(self, gt_nnn, gt_D_in, gt_I_in):
        r"""
         Intitializes the gt_D and gt_I vectors. Must be called before evaluating

        :type gt_D_in: float
        :param gt_D_in:  size nq * gt_nnn
        :type gt_I_in: int
        :param gt_I_in:  size nq * gt_nnn
        """
        ...
    
    def evaluate(self, D, I):
        r"""
         Evaluate the criterion.

        :type D: float
        :param D:  size nq * nnn
        :type I: int
        :param I:  size nq * nnn
        :rtype: float
        :return: the criterion, between 0 and 1. Larger is better.
        """
        ...
    
    __swig_destroy__ = ...


class OneRecallAtRCriterion(AutoTuneCriterion):
    thisown = ...
    __repr__ = ...
    R = ...
    def __init__(self, nq, R) -> None:
        ...
    
    def evaluate(self, D, I):
        ...
    
    __swig_destroy__ = ...


class IntersectionCriterion(AutoTuneCriterion):
    thisown = ...
    __repr__ = ...
    R = ...
    def __init__(self, nq, R) -> None:
        ...
    
    def evaluate(self, D, I):
        ...
    
    __swig_destroy__ = ...


class OperatingPoint:
    r"""
    Maintains a list of experimental results. Each operating point is a
    (perf, t, key) triplet, where higher perf and lower t is
    better. The key field is an arbitrary identifier for the operating point.

    Includes primitives to extract the Pareto-optimal operating points in the
    (perf, t) space.
    """
    thisown = ...
    __repr__ = ...
    perf = ...
    t = ...
    key = ...
    cno = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class OperatingPoints:
    thisown = ...
    __repr__ = ...
    all_pts = ...
    optimal_pts = ...
    def __init__(self) -> None:
        ...
    
    def merge_with(self, *args):
        r"""add operating points from other to this, with a prefix to the keys"""
        ...
    
    def clear(self):
        ...
    
    def add(self, perf, t, key, cno=...):
        r"""add a performance measure. Return whether it is an optimal point"""
        ...
    
    def t_for_perf(self, perf):
        r"""get time required to obtain a given performance measure"""
        ...
    
    def display(self, only_optimal=...):
        r"""easy-to-read output"""
        ...
    
    def all_to_gnuplot(self, fname):
        r"""output to a format easy to digest by gnuplot"""
        ...
    
    def optimal_to_gnuplot(self, fname):
        ...
    
    __swig_destroy__ = ...


class ParameterRange:
    r"""possible values of a parameter, sorted from least to most expensive/accurate"""
    thisown = ...
    __repr__ = ...
    name = ...
    values = ...
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class ParameterSpace:
    r"""Uses a-priori knowledge on the Faiss indexes to extract tunable parameters."""
    thisown = ...
    __repr__ = ...
    parameter_ranges = ...
    verbose = ...
    n_experiments = ...
    batchsize = ...
    thread_over_batches = ...
    min_test_duration = ...
    def __init__(self) -> None:
        ...
    
    def n_combinations(self):
        r"""nb of combinations, = product of values sizes"""
        ...
    
    def combination_ge(self, c1, c2):
        r"""returns whether combinations c1 >= c2 in the tuple sense"""
        ...
    
    def combination_name(self, cno):
        r"""get string representation of the combination"""
        ...
    
    def display(self):
        r"""print a description on stdout"""
        ...
    
    def add_range(self, name):
        r"""add a new parameter (or return it if it exists)"""
        ...
    
    def initialize(self, index):
        r"""initialize with reasonable parameters for the index"""
        ...
    
    def set_index_parameters(self, *args):
        r"""
        *Overload 1:*
        set a combination of parameters on an index

        |

        *Overload 2:*
        set a combination of parameters described by a string
        """
        ...
    
    def set_index_parameter(self, index, name, val):
        r"""set one of the parameters, returns whether setting was successful"""
        ...
    
    def update_bounds(self, cno, op, upper_bound_perf, lower_bound_t):
        r"""
         find an upper bound on the performance and a lower bound on t
        for configuration cno given another operating point op
        """
        ...
    
    def explore(self, index, nq, xq, crit, ops):
        r"""
         explore operating points
        :type index: :py:class:`Index`
        :param index:   index to run on
        :type xq: float
        :param xq:      query vectors (size nq * index.d)
        :type crit: :py:class:`AutoTuneCriterion`
        :param crit:    selection criterion
        :type ops: :py:class:`OperatingPoints`
        :param ops:     resulting operating points
        """
        ...
    
    __swig_destroy__ = ...


def index_factory(*args):
    r"""
    Build and index with the sequence of processing steps described in
    the string.
    """
    ...

def index_binary_factory(d, description):
    ...

class MatrixStats:
    r"""
     Reports some statistics on a dataset and comments on them.

    It is a class rather than a function so that all stats can also be
    accessed from code
    """
    thisown = ...
    __repr__ = ...
    def __init__(self, n, d, x) -> None:
        ...
    
    comments = ...
    n = ...
    d = ...
    n_collision = ...
    n_valid = ...
    n0 = ...
    min_norm2 = ...
    max_norm2 = ...
    hash_value = ...
    per_dim_stats = ...
    occurrences = ...
    buf = ...
    nbuf = ...
    def do_comment(self, fmt):
        ...
    
    __swig_destroy__ = ...


class PyCallbackIOWriter(IOWriter):
    thisown = ...
    __repr__ = ...
    callback = ...
    bs = ...
    def __init__(self, *args) -> None:
        r"""
        Callback: Python function that takes a bytes object and
        returns the number of bytes successfully written.
        """
        ...
    
    def __call__(self, ptrv, size, nitems):
        ...
    
    __swig_destroy__ = ...


class PyCallbackIOReader(IOReader):
    thisown = ...
    __repr__ = ...
    callback = ...
    bs = ...
    def __init__(self, *args) -> None:
        r"""
         Callback: Python function that takes a size and returns a
        bytes object with the resulting read
        """
        ...
    
    def __call__(self, ptrv, size, nitems):
        ...
    
    __swig_destroy__ = ...


class PyCallbackIDSelector(IDSelector):
    thisown = ...
    __repr__ = ...
    callback = ...
    def __init__(self, callback) -> None:
        ...
    
    def is_member(self, id):
        ...
    
    __swig_destroy__ = ...


class PyCallbackShardingFunction(ShardingFunction):
    thisown = ...
    __repr__ = ...
    callback = ...
    def __call__(self, i, shard_count):
        ...
    
    __swig_destroy__ = ...
    def __init__(self, *args) -> None:
        ...
    


class float_minheap_array_t:
    r"""
     a template structure for a set of [min|max]-heaps it is tailored
    so that the actual data of the heaps can just live in compact
    arrays.
    """
    thisown = ...
    __repr__ = ...
    nh = ...
    k = ...
    ids = ...
    val = ...
    def get_val(self, key):
        r"""Return the list of values for a heap"""
        ...
    
    def get_ids(self, key):
        r"""Correspponding identifiers"""
        ...
    
    def heapify(self):
        r"""prepare all the heaps before adding"""
        ...
    
    def addn(self, nj, vin, j0=..., i0=..., ni=...):
        r"""
         add nj elements to heaps i0:i0+ni, with sequential ids

        :type nj: int
        :param nj:    nb of elements to add to each heap
        :type vin: float
        :param vin:   elements to add, size ni * nj
        :type j0: int, optional
        :param j0:    add this to the ids that are added
        :type i0: int, optional
        :param i0:    first heap to update
        :type ni: int, optional
        :param ni:    nb of elements to update (-1 = use nh)
        """
        ...
    
    def addn_with_ids(self, nj, vin, id_in=..., id_stride=..., i0=..., ni=...):
        r"""
         same as addn

        :type id_in: int, optional
        :param id_in:     ids of the elements to add, size ni * nj
        :type id_stride: int, optional
        :param id_stride: stride for id_in
        """
        ...
    
    def addn_query_subset_with_ids(self, nsubset, subset, nj, vin, id_in=..., id_stride=...):
        r"""
         same as addn_with_ids, but for just a subset of queries

        :type nsubset: int
        :param nsubset:  number of query entries to update
        :type subset: int
        :param subset:   indexes of queries to update, in 0..nh-1, size nsubset
        """
        ...
    
    def reorder(self):
        r"""reorder all the heaps"""
        ...
    
    def per_line_extrema(self, vals_out, idx_out):
        r"""
         this is not really a heap function. It just finds the per-line
          extrema of each line of array D
        :type vals_out: float
        :param vals_out:    extreme value of each line (size nh, or NULL)
        :type idx_out: int
        :param idx_out:     index of extreme value (size nh or NULL)
        """
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class int_minheap_array_t:
    r"""
     a template structure for a set of [min|max]-heaps it is tailored
    so that the actual data of the heaps can just live in compact
    arrays.
    """
    thisown = ...
    __repr__ = ...
    nh = ...
    k = ...
    ids = ...
    val = ...
    def get_val(self, key):
        r"""Return the list of values for a heap"""
        ...
    
    def get_ids(self, key):
        r"""Correspponding identifiers"""
        ...
    
    def heapify(self):
        r"""prepare all the heaps before adding"""
        ...
    
    def addn(self, nj, vin, j0=..., i0=..., ni=...):
        r"""
         add nj elements to heaps i0:i0+ni, with sequential ids

        :type nj: int
        :param nj:    nb of elements to add to each heap
        :type vin: int
        :param vin:   elements to add, size ni * nj
        :type j0: int, optional
        :param j0:    add this to the ids that are added
        :type i0: int, optional
        :param i0:    first heap to update
        :type ni: int, optional
        :param ni:    nb of elements to update (-1 = use nh)
        """
        ...
    
    def addn_with_ids(self, nj, vin, id_in=..., id_stride=..., i0=..., ni=...):
        r"""
         same as addn

        :type id_in: int, optional
        :param id_in:     ids of the elements to add, size ni * nj
        :type id_stride: int, optional
        :param id_stride: stride for id_in
        """
        ...
    
    def addn_query_subset_with_ids(self, nsubset, subset, nj, vin, id_in=..., id_stride=...):
        r"""
         same as addn_with_ids, but for just a subset of queries

        :type nsubset: int
        :param nsubset:  number of query entries to update
        :type subset: int
        :param subset:   indexes of queries to update, in 0..nh-1, size nsubset
        """
        ...
    
    def reorder(self):
        r"""reorder all the heaps"""
        ...
    
    def per_line_extrema(self, vals_out, idx_out):
        r"""
         this is not really a heap function. It just finds the per-line
          extrema of each line of array D
        :type vals_out: int
        :param vals_out:    extreme value of each line (size nh, or NULL)
        :type idx_out: int
        :param idx_out:     index of extreme value (size nh or NULL)
        """
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class float_maxheap_array_t:
    r"""
     a template structure for a set of [min|max]-heaps it is tailored
    so that the actual data of the heaps can just live in compact
    arrays.
    """
    thisown = ...
    __repr__ = ...
    nh = ...
    k = ...
    ids = ...
    val = ...
    def get_val(self, key):
        r"""Return the list of values for a heap"""
        ...
    
    def get_ids(self, key):
        r"""Correspponding identifiers"""
        ...
    
    def heapify(self):
        r"""prepare all the heaps before adding"""
        ...
    
    def addn(self, nj, vin, j0=..., i0=..., ni=...):
        r"""
         add nj elements to heaps i0:i0+ni, with sequential ids

        :type nj: int
        :param nj:    nb of elements to add to each heap
        :type vin: float
        :param vin:   elements to add, size ni * nj
        :type j0: int, optional
        :param j0:    add this to the ids that are added
        :type i0: int, optional
        :param i0:    first heap to update
        :type ni: int, optional
        :param ni:    nb of elements to update (-1 = use nh)
        """
        ...
    
    def addn_with_ids(self, nj, vin, id_in=..., id_stride=..., i0=..., ni=...):
        r"""
         same as addn

        :type id_in: int, optional
        :param id_in:     ids of the elements to add, size ni * nj
        :type id_stride: int, optional
        :param id_stride: stride for id_in
        """
        ...
    
    def addn_query_subset_with_ids(self, nsubset, subset, nj, vin, id_in=..., id_stride=...):
        r"""
         same as addn_with_ids, but for just a subset of queries

        :type nsubset: int
        :param nsubset:  number of query entries to update
        :type subset: int
        :param subset:   indexes of queries to update, in 0..nh-1, size nsubset
        """
        ...
    
    def reorder(self):
        r"""reorder all the heaps"""
        ...
    
    def per_line_extrema(self, vals_out, idx_out):
        r"""
         this is not really a heap function. It just finds the per-line
          extrema of each line of array D
        :type vals_out: float
        :param vals_out:    extreme value of each line (size nh, or NULL)
        :type idx_out: int
        :param idx_out:     index of extreme value (size nh or NULL)
        """
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


class int_maxheap_array_t:
    r"""
     a template structure for a set of [min|max]-heaps it is tailored
    so that the actual data of the heaps can just live in compact
    arrays.
    """
    thisown = ...
    __repr__ = ...
    nh = ...
    k = ...
    ids = ...
    val = ...
    def get_val(self, key):
        r"""Return the list of values for a heap"""
        ...
    
    def get_ids(self, key):
        r"""Correspponding identifiers"""
        ...
    
    def heapify(self):
        r"""prepare all the heaps before adding"""
        ...
    
    def addn(self, nj, vin, j0=..., i0=..., ni=...):
        r"""
         add nj elements to heaps i0:i0+ni, with sequential ids

        :type nj: int
        :param nj:    nb of elements to add to each heap
        :type vin: int
        :param vin:   elements to add, size ni * nj
        :type j0: int, optional
        :param j0:    add this to the ids that are added
        :type i0: int, optional
        :param i0:    first heap to update
        :type ni: int, optional
        :param ni:    nb of elements to update (-1 = use nh)
        """
        ...
    
    def addn_with_ids(self, nj, vin, id_in=..., id_stride=..., i0=..., ni=...):
        r"""
         same as addn

        :type id_in: int, optional
        :param id_in:     ids of the elements to add, size ni * nj
        :type id_stride: int, optional
        :param id_stride: stride for id_in
        """
        ...
    
    def addn_query_subset_with_ids(self, nsubset, subset, nj, vin, id_in=..., id_stride=...):
        r"""
         same as addn_with_ids, but for just a subset of queries

        :type nsubset: int
        :param nsubset:  number of query entries to update
        :type subset: int
        :param subset:   indexes of queries to update, in 0..nh-1, size nsubset
        """
        ...
    
    def reorder(self):
        r"""reorder all the heaps"""
        ...
    
    def per_line_extrema(self, vals_out, idx_out):
        r"""
         this is not really a heap function. It just finds the per-line
          extrema of each line of array D
        :type vals_out: int
        :param vals_out:    extreme value of each line (size nh, or NULL)
        :type idx_out: int
        :param idx_out:     index of extreme value (size nh or NULL)
        """
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def CMin_float_partition_fuzzy(vals, ids, n, q_min, q_max, q_out):
    r"""
     partitions the table into 0:q and q:n where all elements above q are >= all
    elements below q (for C = CMax, for CMin comparisons are reversed)

    Returns the partition threshold. The elements q:n are destroyed on output.
    """
    ...

def CMax_float_partition_fuzzy(vals, ids, n, q_min, q_max, q_out):
    r"""
     partitions the table into 0:q and q:n where all elements above q are >= all
    elements below q (for C = CMax, for CMin comparisons are reversed)

    Returns the partition threshold. The elements q:n are destroyed on output.
    """
    ...

class AlignedTableUint8:
    thisown = ...
    __repr__ = ...
    tab = ...
    numel = ...
    @staticmethod
    def round_capacity(n):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    def itemsize(self):
        ...
    
    def resize(self, n):
        ...
    
    def clear(self):
        ...
    
    def size(self):
        ...
    
    def nbytes(self):
        ...
    
    def get(self, *args):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class AlignedTableUint16:
    thisown = ...
    __repr__ = ...
    tab = ...
    numel = ...
    @staticmethod
    def round_capacity(n):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    def itemsize(self):
        ...
    
    def resize(self, n):
        ...
    
    def clear(self):
        ...
    
    def size(self):
        ...
    
    def nbytes(self):
        ...
    
    def get(self, *args):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class AlignedTableFloat32:
    thisown = ...
    __repr__ = ...
    tab = ...
    numel = ...
    @staticmethod
    def round_capacity(n):
        ...
    
    def __init__(self, *args) -> None:
        ...
    
    def itemsize(self):
        ...
    
    def resize(self, n):
        ...
    
    def clear(self):
        ...
    
    def size(self):
        ...
    
    def nbytes(self):
        ...
    
    def get(self, *args):
        ...
    
    def data(self, *args):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorUInt8:
    thisown = ...
    __repr__ = ...
    is_owned = ...
    owned_data = ...
    view_data = ...
    view_size = ...
    owner = ...
    c_ptr = ...
    c_size = ...
    def __init__(self, *args) -> None:
        ...
    
    @staticmethod
    def create_view(address, n_elements, owner):
        ...
    
    def data(self, *args):
        ...
    
    def size(self):
        ...
    
    def byte_size(self):
        ...
    
    def at(self, *args):
        ...
    
    def begin(self, *args):
        ...
    
    def end(self, *args):
        ...
    
    def erase(self, begin, end):
        ...
    
    def clear(self):
        ...
    
    def resize(self, *args):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorInt32:
    thisown = ...
    __repr__ = ...
    is_owned = ...
    owned_data = ...
    view_data = ...
    view_size = ...
    owner = ...
    c_ptr = ...
    c_size = ...
    def __init__(self, *args) -> None:
        ...
    
    @staticmethod
    def create_view(address, n_elements, owner):
        ...
    
    def data(self, *args):
        ...
    
    def size(self):
        ...
    
    def byte_size(self):
        ...
    
    def at(self, *args):
        ...
    
    def begin(self, *args):
        ...
    
    def end(self, *args):
        ...
    
    def erase(self, begin, end):
        ...
    
    def clear(self):
        ...
    
    def resize(self, *args):
        ...
    
    __swig_destroy__ = ...


class MaybeOwnedVectorFloat32:
    thisown = ...
    __repr__ = ...
    is_owned = ...
    owned_data = ...
    view_data = ...
    view_size = ...
    owner = ...
    c_ptr = ...
    c_size = ...
    def __init__(self, *args) -> None:
        ...
    
    @staticmethod
    def create_view(address, n_elements, owner):
        ...
    
    def data(self, *args):
        ...
    
    def size(self):
        ...
    
    def byte_size(self):
        ...
    
    def at(self, *args):
        ...
    
    def begin(self, *args):
        ...
    
    def end(self, *args):
        ...
    
    def erase(self, begin, end):
        ...
    
    def clear(self):
        ...
    
    def resize(self, *args):
        ...
    
    __swig_destroy__ = ...


def CMin_uint16_partition_fuzzy(*args):
    ...

def CMax_uint16_partition_fuzzy(*args):
    ...

def merge_knn_results_CMin(*args):
    ...

def merge_knn_results_CMax(*args):
    ...

class MapLong2Long:
    thisown = ...
    __repr__ = ...
    map = ...
    def add(self, n, keys, vals):
        ...
    
    def search(self, key):
        ...
    
    def search_multiple(self, n, keys, vals):
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def omp_set_num_threads(num_threads):
    ...

def omp_get_max_threads():
    ...

def memcpy(dest, src, n):
    ...

class PythonInterruptCallback(InterruptCallback):
    thisown = ...
    __repr__ = ...
    def want_interrupt(self):
        ...
    
    @staticmethod
    def reset():
        ...
    
    def __init__(self) -> None:
        ...
    
    __swig_destroy__ = ...


def swig_ptr(a):
    ...

def rev_swig_ptr(*args):
    ...

def cast_integer_to_uint8_ptr(x):
    ...

def cast_integer_to_float_ptr(x):
    ...

def cast_integer_to_idx_t_ptr(x):
    ...

def cast_integer_to_int_ptr(x):
    ...

def cast_integer_to_void_ptr(x):
    ...

def swig_version():
    ...

