"""
This type stub file was generated by pyright.
"""

from faiss.loader import *

def kmin(array, k): # -> tuple[_Array[tuple[int, int], Any], _Array[tuple[int, int], Any]]:
    """return k smallest values (and their indices) of the lines of a
    float32 array"""
    ...

def kmax(array, k): # -> tuple[_Array[tuple[int, int], Any], _Array[tuple[int, int], Any]]:
    """return k largest values (and their indices) of the lines of a
    float32 array"""
    ...

def pairwise_distances(xq, xb, metric=..., metric_arg=...): # -> _Array[tuple[int, int], Any]:
    """compute the whole pairwise distance matrix between two sets of
    vectors"""
    ...

def rand(n, seed=...): # -> _Array1D[Any]:
    ...

def randint(n, seed=..., vmax=...): # -> _Array1D[Any]:
    ...

lrand = ...
def randn(n, seed=...): # -> _Array1D[Any]:
    ...

def checksum(a): # -> _Array1D[Any]:
    """ compute a checksum for quick-and-dirty comparisons of arrays """
    ...

rand_smooth_vectors_c = ...
def rand_smooth_vectors(n, d, seed=...): # -> _Array[tuple[int, int], Any]:
    ...

def eval_intersection(I1, I2): # -> Literal[0]:
    """ size of intersection between each line of two result tables"""
    ...

def normalize_L2(x): # -> None:
    ...

bucket_sort_c = ...
def bucket_sort(tab, nbucket=..., nt=...): # -> tuple[_Array1D[Any], _Array1D[Any]]:
    """Perform a bucket sort on a table of integers.

    Parameters
    ----------
    tab : array_like
        elements to sort, max value nbucket - 1
    nbucket : integer
        number of buckets, None if unknown
    nt : integer
        number of threads to use (0 = use unthreaded codepath)

    Returns
    -------
    lims : array_like
        cumulative sum of bucket sizes (size vmax + 1)
    perm : array_like
        perm[lims[i] : lims[i + 1]] contains the indices of bucket #i (size tab.size)
    """
    ...

matrix_bucket_sort_inplace_c = ...
def matrix_bucket_sort_inplace(tab, nbucket=..., nt=...): # -> _Array1D[Any]:
    """Perform a bucket sort on a matrix, recording the original
    row of each element.

    Parameters
    ----------
    tab : array_like
        array of size (N, ncol) that contains the bucket ids, maximum
        value nbucket - 1.
        On output, it the elements are shuffled such that the flat array
        tab.ravel()[lims[i] : lims[i + 1]] contains the row numbers
        of each bucket entry.
    nbucket : integer
        number of buckets (the maximum value in tab should be nbucket - 1)
    nt : integer
        number of threads to use (0 = use unthreaded codepath)

    Returns
    -------
    lims : array_like
        cumulative sum of bucket sizes (size vmax + 1)
    """
    ...

class ResultHeap:
    """Accumulate query results from a sliced dataset. The final result will
    be in self.D, self.I."""
    def __init__(self, nq, k, keep_max=...) -> None:
        """
        nq: number of query vectors,
        k: number of results per query
        keep_max: keep the top-k maximum values instead of the minima
        """
        ...
    
    def add_result(self, D, I): # -> None:
        """
        Add results for all heaps
        D, I should be of size (nh, nres)
        D, I do not need to be in a particular order (heap or sorted)
        """
        ...
    
    def add_result_subset(self, subset, D, I): # -> None:
        """
        Add results for a subset of heaps.
        D, I should hold resutls for all the subset
        as a special case, if I is 1D, then all ids are assumed to be the same
        """
        ...
    
    def finalize(self): # -> None:
        ...
    


def merge_knn_results(Dall, Iall, keep_max=...): # -> tuple[_Array[tuple[int, int], float64], _Array[tuple[int, int], float64]]:
    """
    Merge a set of sorted knn-results obtained from different shards in a dataset
    Dall and Iall are of size (nshard, nq, k) each D[i, j] should be sorted
    returns D, I of size (nq, k) as the merged result set
    """
    ...

class MapInt64ToInt64:
    def __init__(self, capacity) -> None:
        ...
    
    def add(self, keys, vals): # -> None:
        ...
    
    def lookup(self, keys): # -> _Array[tuple[int], Any]:
        ...
    


def knn(xq, xb, k, metric=..., metric_arg=...): # -> tuple[_Array[tuple[int, int], Any], _Array[tuple[int, int], Any]]:
    """
    Compute the k nearest neighbors of a vector without constructing an index


    Parameters
    ----------
    xq : array_like
        Query vectors, shape (nq, d) where the dimension d is that same as xb
        `dtype` must be float32.
    xb : array_like
        Database vectors, shape (nb, d) where dimension d is the same as xq
        `dtype` must be float32.
    k : int
        Number of nearest neighbors.
    metric : MetricType, optional
        distance measure to use (either METRIC_L2 or METRIC_INNER_PRODUCT)

    Returns
    -------
    D : array_like
        Distances of the nearest neighbors, shape (nq, k)
    I : array_like
        Labels of the nearest neighbors, shape (nq, k)
    """
    ...

def knn_hamming(xq, xb, k, variant=...): # -> tuple[_Array[tuple[int, int], Any], _Array[tuple[int, int], Any]]:
    """
    Compute the k nearest neighbors of a set of vectors without constructing an index.

    Parameters
    ----------
    xq : array_like
        Query vectors, shape (nq, d) where d is the number of bits / 8
        `dtype` must be uint8.
    xb : array_like
        Database vectors, shape (nb, d) where d is the number of bits / 8
        `dtype` must be uint8.
    k : int
        Number of nearest neighbors.
    variant : string
        Function variant to use, either "mc" (counter) or "hc" (heap)

    Returns
    -------
    D : array_like
        Distances of the nearest neighbors, shape (nq, k)
    I : array_like
        Labels of the nearest neighbors, shape (nq, k)
    """
    ...

class Kmeans:
    """Object that performs k-means clustering and manages the centroids.
    The `Kmeans` class is essentially a wrapper around the C++ `Clustering` object.

    Parameters
    ----------
    d : int
       dimension of the vectors to cluster
    k : int
       number of clusters
    gpu: bool or int, optional
       False: don't use GPU
       True: use all GPUs
       number: use this many GPUs
    progressive_dim_steps:
        use a progressive dimension clustering (with that number of steps)

    Subsequent parameters are fields of the Clustring object. The most important are:

    niter: int, optional
       clustering iterations
    nredo: int, optional
       redo clustering this many times and keep best
    verbose: bool, optional
    spherical: bool, optional
       do we want normalized centroids?
    int_centroids: bool, optional
       round centroids coordinates to integer
    seed: int, optional
       seed for the random number generator

    """
    def __init__(self, d, k, **kwargs) -> None:
        """d: input dimension, k: nb of centroids. Additional
         parameters are passed on the ClusteringParameters object,
         including niter=25, verbose=False, spherical = False
        """
        ...
    
    def set_index(self): # -> None:
        ...
    
    def reset(self, k=...): # -> None:
        """ prepare k-means object to perform a new clustering, possibly
        with another number of centroids """
        ...
    
    def train(self, x, weights=..., init_centroids=...): # -> Any | float:
        """ Perform k-means clustering.
        On output of the function call:

        - the centroids are in the centroids field of size (`k`, `d`).

        - the objective value at each iteration is in the array obj (size `niter`)

        - detailed optimization statistics are in the array iteration_stats.

        Parameters
        ----------
        x : array_like
            Training vectors, shape (n, d), `dtype` must be float32 and n should
            be larger than the number of clusters `k`.
        weights : array_like
            weight associated to each vector, shape `n`
        init_centroids : array_like
            initial set of centroids, shape (n, d)

        Returns
        -------
        final_obj: float
            final optimization objective

        """
        ...
    
    def assign(self, x): # -> tuple[Any, Any]:
        ...
    


def is_sequence(x): # -> bool:
    ...

pack_bitstrings_c = ...
def pack_bitstrings(a, nbit): # -> _Array[tuple[int, int], Any]:
    """
    Pack a set integers (i, j) where i=0:n and j=0:M into
    n bitstrings.
    Output is an uint8 array of size (n, code_size), where code_size is
    such that at most 7 bits per code are wasted.

    If nbit is an integer: all entries takes nbit bits.
    If nbit is an array: entry (i, j) takes nbit[j] bits.
    """
    ...

unpack_bitstrings_c = ...
def unpack_bitstrings(b, M_or_nbits, nbit=...): # -> _Array[tuple[int, int], Any]:
    """
    Unpack a set integers (i, j) where i=0:n and j=0:M from
    n bitstrings (encoded as uint8s).
    Input is an uint8 array of size (n, code_size), where code_size is
    such that at most 7 bits per code are wasted.

    Two forms:
    - when called with (array, M, nbit): there are M entries of size
      nbit per row
    - when called with (array, nbits): element (i, j) is encoded in
      nbits[j] bits
    """
    ...

